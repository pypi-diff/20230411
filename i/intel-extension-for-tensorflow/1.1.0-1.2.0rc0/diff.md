# Comparing `tmp/intel_extension_for_tensorflow-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip` & `tmp/intel_extension_for_tensorflow-1.2.0rc0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,30 +1,34 @@
-Zip file size: 121271 bytes, number of entries: 28
--rw-r--r--  2.0 unx     1594 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/__init__.py
--rwxr-xr-x  2.0 unx        0 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/core/utils/protobuf/__init__.py
--rwxr-xr-x  2.0 unx    16884 b- defN 22-Dec-26 13:09 intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py
--rwxr-xr-x  2.0 unx        0 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/__init__.py
--rw-r--r--  2.0 unx     1593 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/device.py
--rw-r--r--  2.0 unx    29820 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/launch.py
--rw-r--r--  2.0 unx     1091 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/version.py
--rw-r--r--  2.0 unx     1169 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/ops/__init__.py
--rw-r--r--  2.0 unx     2526 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/ops/activations.py
--rw-r--r--  2.0 unx    17836 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/ops/layer_norm.py
--rw-r--r--  2.0 unx     2668 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/ops/load_ops_library.py
--rw-r--r--  2.0 unx     2593 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/ops/ops_grad.py
--rw-r--r--  2.0 unx    10620 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/ops/optimizers.py
--rw-r--r--  2.0 unx    28127 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/ops/recurrent.py
--rw-r--r--  2.0 unx     1017 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/test_func/__init__.py
--rw-r--r--  2.0 unx     1117 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/test_func/eager_test.py
--rw-r--r--  2.0 unx    17631 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/test_func/keras_parameterized.py
--rw-r--r--  2.0 unx    37515 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/test_func/keras_testing_utils.py
--rw-r--r--  2.0 unx     6545 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/test_func/test.py
--rw-r--r--  2.0 unx   134568 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/python/test_func/test_util.py
--rw-r--r--  2.0 unx    81897 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/third-party-programs/THIRD-PARTY-PROGRAMS
--rw-r--r--  2.0 unx    28478 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onednn.txt
--rw-r--r--  2.0 unx    21801 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onevpl.txt
--rw-r--r--  2.0 unx    10775 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow-1.1.0.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     3447 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow-1.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx      103 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow-1.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       31 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow-1.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3189 b- defN 22-Dec-26 13:34 intel_extension_for_tensorflow-1.1.0.dist-info/RECORD
-28 files, 464635 bytes uncompressed, 115835 bytes compressed:  75.1%
+Zip file size: 130634 bytes, number of entries: 32
+-rw-rw-r--  2.0 unx     1898 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/__init__.py
+-rw-rw-r--  2.0 unx      243 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/__main__.py
+-rwxr-xr-x  2.0 unx        0 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/core/utils/protobuf/__init__.py
+-rwxr-xr-x  2.0 unx     3575 b- defN 23-Apr-11 10:58 intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py
+-rwxr-xr-x  2.0 unx        0 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/__init__.py
+-rw-rw-r--  2.0 unx     1403 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/config.py
+-rw-rw-r--  2.0 unx     1214 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/device.py
+-rw-rw-r--  2.0 unx    26593 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/experimental_ops_override.py
+-rw-rw-r--  2.0 unx     2190 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/gen_itex_version.py
+-rw-rw-r--  2.0 unx    31025 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/launch.py
+-rw-rw-r--  2.0 unx     1028 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/version.py
+-rw-rw-r--  2.0 unx     1169 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/ops/__init__.py
+-rw-rw-r--  2.0 unx     2531 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/ops/activations.py
+-rw-rw-r--  2.0 unx    17867 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/ops/layer_norm.py
+-rw-rw-r--  2.0 unx     2668 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/ops/load_ops_library.py
+-rw-rw-r--  2.0 unx     3451 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/ops/ops_grad.py
+-rw-rw-r--  2.0 unx    10707 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/ops/optimizers.py
+-rw-rw-r--  2.0 unx    28487 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/ops/recurrent.py
+-rw-rw-r--  2.0 unx     1038 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/test_func/__init__.py
+-rw-rw-r--  2.0 unx     1116 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/test_func/eager_test.py
+-rw-rw-r--  2.0 unx    17632 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/test_func/keras_parameterized.py
+-rw-rw-r--  2.0 unx    38058 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/test_func/keras_testing_utils.py
+-rw-rw-r--  2.0 unx     6345 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/test_func/test.py
+-rw-rw-r--  2.0 unx   135335 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/python/test_func/test_util.py
+-rw-rw-r--  2.0 unx    81897 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/third-party-programs/THIRD-PARTY-PROGRAMS
+-rw-rw-r--  2.0 unx    28478 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onednn.txt
+-rw-rw-r--  2.0 unx    21801 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onevpl.txt
+-rw-rw-r--  2.0 unx    10775 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow-1.2.0rc0.dist-info/LICENSE.txt
+-rw-rw-r--  2.0 unx     3391 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow-1.2.0rc0.dist-info/METADATA
+-rw-rw-r--  2.0 unx      103 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow-1.2.0rc0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       31 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow-1.2.0rc0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3643 b- defN 23-Apr-11 11:00 intel_extension_for_tensorflow-1.2.0rc0.dist-info/RECORD
+32 files, 485692 bytes uncompressed, 124440 bytes compressed:  74.4%
```

## zipnote {}

```diff
@@ -1,22 +1,34 @@
 Filename: intel_extension_for_tensorflow/__init__.py
 Comment: 
 
+Filename: intel_extension_for_tensorflow/__main__.py
+Comment: 
+
 Filename: intel_extension_for_tensorflow/core/utils/protobuf/__init__.py
 Comment: 
 
 Filename: intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py
 Comment: 
 
 Filename: intel_extension_for_tensorflow/python/__init__.py
 Comment: 
 
+Filename: intel_extension_for_tensorflow/python/config.py
+Comment: 
+
 Filename: intel_extension_for_tensorflow/python/device.py
 Comment: 
 
+Filename: intel_extension_for_tensorflow/python/experimental_ops_override.py
+Comment: 
+
+Filename: intel_extension_for_tensorflow/python/gen_itex_version.py
+Comment: 
+
 Filename: intel_extension_for_tensorflow/python/launch.py
 Comment: 
 
 Filename: intel_extension_for_tensorflow/python/version.py
 Comment: 
 
 Filename: intel_extension_for_tensorflow/python/ops/__init__.py
@@ -63,23 +75,23 @@
 
 Filename: intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onednn.txt
 Comment: 
 
 Filename: intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onevpl.txt
 Comment: 
 
-Filename: intel_extension_for_tensorflow-1.1.0.dist-info/LICENSE.txt
+Filename: intel_extension_for_tensorflow-1.2.0rc0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: intel_extension_for_tensorflow-1.1.0.dist-info/METADATA
+Filename: intel_extension_for_tensorflow-1.2.0rc0.dist-info/METADATA
 Comment: 
 
-Filename: intel_extension_for_tensorflow-1.1.0.dist-info/WHEEL
+Filename: intel_extension_for_tensorflow-1.2.0rc0.dist-info/WHEEL
 Comment: 
 
-Filename: intel_extension_for_tensorflow-1.1.0.dist-info/top_level.txt
+Filename: intel_extension_for_tensorflow-1.2.0rc0.dist-info/top_level.txt
 Comment: 
 
-Filename: intel_extension_for_tensorflow-1.1.0.dist-info/RECORD
+Filename: intel_extension_for_tensorflow-1.2.0rc0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## intel_extension_for_tensorflow/__init__.py

```diff
@@ -12,15 +12,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 '''Init file for graph optimizer config, custom ops'''
 
 import tensorflow  # pylint: disable=unused-import
 import intel_extension_for_tensorflow_lib  # pylint: disable=unused-import
+from intel_extension_for_tensorflow.python.config import set_config  # pylint: disable=unused-import
+from intel_extension_for_tensorflow.python.config import get_config  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python.device import set_backend  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python.device import get_backend  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python import ops  # pylint: disable=unused-import,line-too-long
 from intel_extension_for_tensorflow.python.version import __version__  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python import version  # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python import test_func  # pylint: disable=unused-import
 
 from intel_extension_for_tensorflow.core.utils.protobuf.config_pb2 import *  # pylint: disable=unused-import,wildcard-import,unused-wildcard-import
+from intel_extension_for_tensorflow.python.experimental_ops_override import experimental_ops_override
```

## intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py

```diff
@@ -1,412 +1,41 @@
 # -*- coding: utf-8 -*-
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: itex/core/utils/protobuf/config.proto
-
-import sys
-_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))
-from google.protobuf.internal import enum_type_wrapper
+"""Generated protocol buffer code."""
+from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
-from google.protobuf import message as _message
-from google.protobuf import reflection as _reflection
+from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 
 
-DESCRIPTOR = _descriptor.FileDescriptor(
-  name='itex/core/utils/protobuf/config.proto',
-  package='itex',
-  syntax='proto3',
-  serialized_options=None,
-  serialized_pb=_b('\n%itex/core/utils/protobuf/config.proto\x12\x04itex\"\x0c\n\nGPUOptions\"\x8c\x02\n\x0cGraphOptions\x12\"\n\x0conednn_graph\x18\x01 \x01(\x0e\x32\x0c.itex.Toggle\x12 \n\nlayout_opt\x18\x02 \x01(\x0e\x32\x0c.itex.Toggle\x12\x1e\n\x08remapper\x18\x03 \x01(\x0e\x32\x0c.itex.Toggle\x12*\n\x14\x61uto_mixed_precision\x18\x04 \x01(\x0e\x32\x0c.itex.Toggle\x12\x45\n\x1c\x61uto_mixed_precision_options\x18\x05 \x01(\x0b\x32\x1f.itex.AutoMixedPrecisionOptions\x12#\n\rnative_format\x18\x06 \x01(\x0e\x32\x0c.itex.Toggle\"\x8a\x01\n\x0b\x43onfigProto\x12%\n\x0bgpu_options\x18\x01 \x01(\x0b\x32\x10.itex.GPUOptions\x12)\n\rgraph_options\x18\x02 \x01(\x0b\x32\x12.itex.GraphOptions\x12)\n\rdebug_options\x18\x03 \x01(\x0b\x32\x12.itex.DebugOptions\"\x9e\x02\n\x19\x41utoMixedPrecisionOptions\x12\x15\n\rallowlist_add\x18\x01 \x01(\t\x12\x15\n\rinferlist_add\x18\x02 \x01(\t\x12\x15\n\rclearlist_add\x18\x03 \x01(\t\x12\x14\n\x0c\x64\x65nylist_add\x18\x04 \x01(\t\x12\x18\n\x10\x61llowlist_remove\x18\x05 \x01(\t\x12\x18\n\x10inferlist_remove\x18\x06 \x01(\t\x12\x18\n\x10\x63learlist_remove\x18\x07 \x01(\t\x12\x17\n\x0f\x64\x65nylist_remove\x18\x08 \x01(\t\x12\x18\n\x10unsafe_force_all\x18\t \x01(\x08\x12%\n\tdata_type\x18\n \x01(\x0e\x32\x12.itex.ITEXDataType\"[\n\x0c\x44\x65\x62ugOptions\x12%\n\x1d\x61uto_mixed_precision_log_path\x18\x01 \x01(\t\x12$\n\x0expu_force_sync\x18\x02 \x01(\x0e\x32\x0c.itex.Toggle*&\n\x06Toggle\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\x06\n\x02ON\x10\x01\x12\x07\n\x03OFF\x10\x02*@\n\x0cITEXDataType\x12\x15\n\x11\x44\x45\x46\x41ULT_DATA_TYPE\x10\x00\x12\x0b\n\x07\x46LOAT16\x10\x01\x12\x0c\n\x08\x42\x46LOAT16\x10\x02\x62\x06proto3')
-)
-
-_TOGGLE = _descriptor.EnumDescriptor(
-  name='Toggle',
-  full_name='itex.Toggle',
-  filename=None,
-  file=DESCRIPTOR,
-  values=[
-    _descriptor.EnumValueDescriptor(
-      name='DEFAULT', index=0, number=0,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='ON', index=1, number=1,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='OFF', index=2, number=2,
-      serialized_options=None,
-      type=None),
-  ],
-  containing_type=None,
-  serialized_options=None,
-  serialized_start=855,
-  serialized_end=893,
-)
-_sym_db.RegisterEnumDescriptor(_TOGGLE)
-
-Toggle = enum_type_wrapper.EnumTypeWrapper(_TOGGLE)
-_ITEXDATATYPE = _descriptor.EnumDescriptor(
-  name='ITEXDataType',
-  full_name='itex.ITEXDataType',
-  filename=None,
-  file=DESCRIPTOR,
-  values=[
-    _descriptor.EnumValueDescriptor(
-      name='DEFAULT_DATA_TYPE', index=0, number=0,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='FLOAT16', index=1, number=1,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='BFLOAT16', index=2, number=2,
-      serialized_options=None,
-      type=None),
-  ],
-  containing_type=None,
-  serialized_options=None,
-  serialized_start=895,
-  serialized_end=959,
-)
-_sym_db.RegisterEnumDescriptor(_ITEXDATATYPE)
-
-ITEXDataType = enum_type_wrapper.EnumTypeWrapper(_ITEXDATATYPE)
-DEFAULT = 0
-ON = 1
-OFF = 2
-DEFAULT_DATA_TYPE = 0
-FLOAT16 = 1
-BFLOAT16 = 2
-
-
-
-_GPUOPTIONS = _descriptor.Descriptor(
-  name='GPUOptions',
-  full_name='itex.GPUOptions',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=47,
-  serialized_end=59,
-)
-
-
-_GRAPHOPTIONS = _descriptor.Descriptor(
-  name='GraphOptions',
-  full_name='itex.GraphOptions',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='onednn_graph', full_name='itex.GraphOptions.onednn_graph', index=0,
-      number=1, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='layout_opt', full_name='itex.GraphOptions.layout_opt', index=1,
-      number=2, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='remapper', full_name='itex.GraphOptions.remapper', index=2,
-      number=3, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='auto_mixed_precision', full_name='itex.GraphOptions.auto_mixed_precision', index=3,
-      number=4, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='auto_mixed_precision_options', full_name='itex.GraphOptions.auto_mixed_precision_options', index=4,
-      number=5, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='native_format', full_name='itex.GraphOptions.native_format', index=5,
-      number=6, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=62,
-  serialized_end=330,
-)
-
-
-_CONFIGPROTO = _descriptor.Descriptor(
-  name='ConfigProto',
-  full_name='itex.ConfigProto',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='gpu_options', full_name='itex.ConfigProto.gpu_options', index=0,
-      number=1, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='graph_options', full_name='itex.ConfigProto.graph_options', index=1,
-      number=2, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='debug_options', full_name='itex.ConfigProto.debug_options', index=2,
-      number=3, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=333,
-  serialized_end=471,
-)
-
-
-_AUTOMIXEDPRECISIONOPTIONS = _descriptor.Descriptor(
-  name='AutoMixedPrecisionOptions',
-  full_name='itex.AutoMixedPrecisionOptions',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='allowlist_add', full_name='itex.AutoMixedPrecisionOptions.allowlist_add', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='inferlist_add', full_name='itex.AutoMixedPrecisionOptions.inferlist_add', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='clearlist_add', full_name='itex.AutoMixedPrecisionOptions.clearlist_add', index=2,
-      number=3, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='denylist_add', full_name='itex.AutoMixedPrecisionOptions.denylist_add', index=3,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='allowlist_remove', full_name='itex.AutoMixedPrecisionOptions.allowlist_remove', index=4,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='inferlist_remove', full_name='itex.AutoMixedPrecisionOptions.inferlist_remove', index=5,
-      number=6, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='clearlist_remove', full_name='itex.AutoMixedPrecisionOptions.clearlist_remove', index=6,
-      number=7, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='denylist_remove', full_name='itex.AutoMixedPrecisionOptions.denylist_remove', index=7,
-      number=8, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='unsafe_force_all', full_name='itex.AutoMixedPrecisionOptions.unsafe_force_all', index=8,
-      number=9, type=8, cpp_type=7, label=1,
-      has_default_value=False, default_value=False,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='data_type', full_name='itex.AutoMixedPrecisionOptions.data_type', index=9,
-      number=10, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=474,
-  serialized_end=760,
-)
-
-
-_DEBUGOPTIONS = _descriptor.Descriptor(
-  name='DebugOptions',
-  full_name='itex.DebugOptions',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='auto_mixed_precision_log_path', full_name='itex.DebugOptions.auto_mixed_precision_log_path', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='xpu_force_sync', full_name='itex.DebugOptions.xpu_force_sync', index=1,
-      number=2, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=762,
-  serialized_end=853,
-)
-
-_GRAPHOPTIONS.fields_by_name['onednn_graph'].enum_type = _TOGGLE
-_GRAPHOPTIONS.fields_by_name['layout_opt'].enum_type = _TOGGLE
-_GRAPHOPTIONS.fields_by_name['remapper'].enum_type = _TOGGLE
-_GRAPHOPTIONS.fields_by_name['auto_mixed_precision'].enum_type = _TOGGLE
-_GRAPHOPTIONS.fields_by_name['auto_mixed_precision_options'].message_type = _AUTOMIXEDPRECISIONOPTIONS
-_GRAPHOPTIONS.fields_by_name['native_format'].enum_type = _TOGGLE
-_CONFIGPROTO.fields_by_name['gpu_options'].message_type = _GPUOPTIONS
-_CONFIGPROTO.fields_by_name['graph_options'].message_type = _GRAPHOPTIONS
-_CONFIGPROTO.fields_by_name['debug_options'].message_type = _DEBUGOPTIONS
-_AUTOMIXEDPRECISIONOPTIONS.fields_by_name['data_type'].enum_type = _ITEXDATATYPE
-_DEBUGOPTIONS.fields_by_name['xpu_force_sync'].enum_type = _TOGGLE
-DESCRIPTOR.message_types_by_name['GPUOptions'] = _GPUOPTIONS
-DESCRIPTOR.message_types_by_name['GraphOptions'] = _GRAPHOPTIONS
-DESCRIPTOR.message_types_by_name['ConfigProto'] = _CONFIGPROTO
-DESCRIPTOR.message_types_by_name['AutoMixedPrecisionOptions'] = _AUTOMIXEDPRECISIONOPTIONS
-DESCRIPTOR.message_types_by_name['DebugOptions'] = _DEBUGOPTIONS
-DESCRIPTOR.enum_types_by_name['Toggle'] = _TOGGLE
-DESCRIPTOR.enum_types_by_name['ITEXDataType'] = _ITEXDATATYPE
-_sym_db.RegisterFileDescriptor(DESCRIPTOR)
-
-GPUOptions = _reflection.GeneratedProtocolMessageType('GPUOptions', (_message.Message,), {
-  'DESCRIPTOR' : _GPUOPTIONS,
-  '__module__' : 'itex.core.utils.protobuf.config_pb2'
-  # @@protoc_insertion_point(class_scope:itex.GPUOptions)
-  })
-_sym_db.RegisterMessage(GPUOptions)
-
-GraphOptions = _reflection.GeneratedProtocolMessageType('GraphOptions', (_message.Message,), {
-  'DESCRIPTOR' : _GRAPHOPTIONS,
-  '__module__' : 'itex.core.utils.protobuf.config_pb2'
-  # @@protoc_insertion_point(class_scope:itex.GraphOptions)
-  })
-_sym_db.RegisterMessage(GraphOptions)
-
-ConfigProto = _reflection.GeneratedProtocolMessageType('ConfigProto', (_message.Message,), {
-  'DESCRIPTOR' : _CONFIGPROTO,
-  '__module__' : 'itex.core.utils.protobuf.config_pb2'
-  # @@protoc_insertion_point(class_scope:itex.ConfigProto)
-  })
-_sym_db.RegisterMessage(ConfigProto)
-
-AutoMixedPrecisionOptions = _reflection.GeneratedProtocolMessageType('AutoMixedPrecisionOptions', (_message.Message,), {
-  'DESCRIPTOR' : _AUTOMIXEDPRECISIONOPTIONS,
-  '__module__' : 'itex.core.utils.protobuf.config_pb2'
-  # @@protoc_insertion_point(class_scope:itex.AutoMixedPrecisionOptions)
-  })
-_sym_db.RegisterMessage(AutoMixedPrecisionOptions)
-
-DebugOptions = _reflection.GeneratedProtocolMessageType('DebugOptions', (_message.Message,), {
-  'DESCRIPTOR' : _DEBUGOPTIONS,
-  '__module__' : 'itex.core.utils.protobuf.config_pb2'
-  # @@protoc_insertion_point(class_scope:itex.DebugOptions)
-  })
-_sym_db.RegisterMessage(DebugOptions)
-
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n%itex/core/utils/protobuf/config.proto\x12\x04itex\"\x0c\n\nGPUOptions\"\xb6\x02\n\x0cGraphOptions\x12\"\n\x0conednn_graph\x18\x01 \x01(\x0e\x32\x0c.itex.Toggle\x12 \n\nlayout_opt\x18\x02 \x01(\x0e\x32\x0c.itex.Toggle\x12\x1e\n\x08remapper\x18\x03 \x01(\x0e\x32\x0c.itex.Toggle\x12*\n\x14\x61uto_mixed_precision\x18\x04 \x01(\x0e\x32\x0c.itex.Toggle\x12\x45\n\x1c\x61uto_mixed_precision_options\x18\x05 \x01(\x0b\x32\x1f.itex.AutoMixedPrecisionOptions\x12\x1e\n\x08sharding\x18\x06 \x01(\x0e\x32\x0c.itex.Toggle\x12-\n\x0fsharding_config\x18\x07 \x01(\x0b\x32\x14.itex.ShardingConfig\"\x8a\x01\n\x0b\x43onfigProto\x12%\n\x0bgpu_options\x18\x01 \x01(\x0b\x32\x10.itex.GPUOptions\x12)\n\rgraph_options\x18\x02 \x01(\x0b\x32\x12.itex.GraphOptions\x12)\n\rdebug_options\x18\x03 \x01(\x0b\x32\x12.itex.DebugOptions\"\x9e\x02\n\x19\x41utoMixedPrecisionOptions\x12\x15\n\rallowlist_add\x18\x01 \x01(\t\x12\x15\n\rinferlist_add\x18\x02 \x01(\t\x12\x15\n\rclearlist_add\x18\x03 \x01(\t\x12\x14\n\x0c\x64\x65nylist_add\x18\x04 \x01(\t\x12\x18\n\x10\x61llowlist_remove\x18\x05 \x01(\t\x12\x18\n\x10inferlist_remove\x18\x06 \x01(\t\x12\x18\n\x10\x63learlist_remove\x18\x07 \x01(\t\x12\x17\n\x0f\x64\x65nylist_remove\x18\x08 \x01(\t\x12\x18\n\x10unsafe_force_all\x18\t \x01(\x08\x12%\n\tdata_type\x18\n \x01(\x0e\x32\x12.itex.ITEXDataType\"[\n\x0c\x44\x65\x62ugOptions\x12%\n\x1d\x61uto_mixed_precision_log_path\x18\x01 \x01(\t\x12$\n\x0expu_force_sync\x18\x02 \x01(\x0e\x32\x0c.itex.Toggle\"K\n\x0eShardingConfig\x12\x11\n\tauto_mode\x18\x01 \x01(\x08\x12&\n\x07\x64\x65vices\x18\x02 \x03(\x0b\x32\x15.itex.ShardingDevices\"a\n\x0fShardingDevices\x12\x13\n\x0b\x64\x65vice_type\x18\x01 \x01(\t\x12\x12\n\ndevice_num\x18\x02 \x01(\x05\x12\x12\n\nbatch_size\x18\x03 \x01(\x05\x12\x11\n\tstage_num\x18\x04 \x01(\x05*&\n\x06Toggle\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\x06\n\x02ON\x10\x01\x12\x07\n\x03OFF\x10\x02*@\n\x0cITEXDataType\x12\x15\n\x11\x44\x45\x46\x41ULT_DATA_TYPE\x10\x00\x12\x0b\n\x07\x46LOAT16\x10\x01\x12\x0c\n\x08\x42\x46LOAT16\x10\x02\x62\x06proto3')
 
+_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'itex.core.utils.protobuf.config_pb2', globals())
+if _descriptor._USE_C_DESCRIPTORS == False:
+
+  DESCRIPTOR._options = None
+  _TOGGLE._serialized_start=1073
+  _TOGGLE._serialized_end=1111
+  _ITEXDATATYPE._serialized_start=1113
+  _ITEXDATATYPE._serialized_end=1177
+  _GPUOPTIONS._serialized_start=47
+  _GPUOPTIONS._serialized_end=59
+  _GRAPHOPTIONS._serialized_start=62
+  _GRAPHOPTIONS._serialized_end=372
+  _CONFIGPROTO._serialized_start=375
+  _CONFIGPROTO._serialized_end=513
+  _AUTOMIXEDPRECISIONOPTIONS._serialized_start=516
+  _AUTOMIXEDPRECISIONOPTIONS._serialized_end=802
+  _DEBUGOPTIONS._serialized_start=804
+  _DEBUGOPTIONS._serialized_end=895
+  _SHARDINGCONFIG._serialized_start=897
+  _SHARDINGCONFIG._serialized_end=972
+  _SHARDINGDEVICES._serialized_start=974
+  _SHARDINGDEVICES._serialized_end=1071
 # @@protoc_insertion_point(module_scope)
```

## intel_extension_for_tensorflow/python/device.py

```diff
@@ -15,29 +15,19 @@
 
 """device"""
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 from intel_extension_for_tensorflow.python._pywrap_itex import *
-from intel_extension_for_tensorflow.core.utils.protobuf import config_pb2
 
 _VALID_DEVICE_BACKENDS = frozenset({"CPU", "GPU", "AUTO"})
 
-
-def set_backend(backend, config=None):
-  """set backend"""
-  if config is None:
-    config = config_pb2.ConfigProto()
-  if not isinstance(config, config_pb2.ConfigProto):
-    raise TypeError('config must be a tf.ConfigProto, but got %s' %
-                    type(config))
+def set_backend(backend):
   if backend.upper() in _VALID_DEVICE_BACKENDS:
-    config_str = config.SerializeToString()
-    ITEX_SetBackend(backend.upper(), config_str)
+    ITEX_SetBackend(backend.upper())
   else:
     raise ValueError("Cannot specify %s as XPU backend, Only %s is VALID"
                      % (backend, _VALID_DEVICE_BACKENDS))
 
-
 def get_backend():
   return ITEX_GetBackend()
```

## intel_extension_for_tensorflow/python/launch.py

```diff
@@ -1,7 +1,8 @@
+"""System module."""
 #   Copyright (c) 2022 Intel Corporation
 #
 #   Licensed under the Apache License, Version 2.0 (the "License");
 #   you may not use this file except in compliance with the License.
 #   You may obtain a copy of the License at
 #
 #       http://www.apache.org/licenses/LICENSE-2.0
@@ -51,17 +52,18 @@
         regex_out = re.search(pattern, line)
         if regex_out:
           self.cpuinfo.append(regex_out.group(1).strip().split(","))
       assert len(self.cpuinfo) > 0, "cpuinfo is empty"
       self.get_socket_info()
     else:
       raise RuntimeError(
-        "{} platform is not supported!!!".format(platform.system()))
+          "{} platform is not supported!!!".format(platform.system()))
 
   def get_socket_info(self):
+    """A dummy docstring"""
     idx_active = 3
     if self.cpuinfo[0][idx_active] == '':
       idx_active = 2
     self.nodes = int(max([line[idx_active] for line in self.cpuinfo])) + 1
     self.node_physical_cores = []  # node_id is index
     self.node_logical_cores = []   # node_id is index
     self.physical_core_node_map = {}  # phyical core to numa node id
@@ -116,20 +118,20 @@
     cores_numa_map = self.logical_core_node_map
     numa_ids = []
     for core in core_list:
       numa_id = cores_numa_map[core]
       if not numa_id in numa_ids:
         numa_ids.append(numa_id)
     if len(numa_ids) > 1:
-      logger.warning("Numa Aware: cores:{} on different NUMA nodes:{}".format(
-          str(core_list), str(numa_ids)))
+      logger.warning("Numa Aware: cores:%s on different NUMA nodes:%s", \
+                      'core_list', 'numa_ids')
     if len(numa_ids) == 0:
       logger.error(
           "invalid number of NUMA nodes; please make sure numa_ids >= 1")
-      exit(-1)
+      sys.exit(-1)
     return numa_ids
 
 
 class Launcher():
   r"""
    Base class for launcher
   """
@@ -173,56 +175,57 @@
           lib_find = True
           break
     return lib_set or lib_find
 
   def is_numactl_available(self):
     numactl_available = False
     cmd = ["numactl", "-C", "0", "-m", "0", "ls"]
-    r = subprocess.run(cmd, env=os.environ, stdout=subprocess.DEVNULL)
+    r = subprocess.run(cmd, env=os.environ, stdout=subprocess.DEVNULL, \
+                       check=True)
     if r.returncode == 0:
       numactl_available = True
     return numactl_available
 
   def set_memory_allocator(self, enable_tcmalloc=True,
                            enable_jemalloc=False, use_default_allocator=False):
     '''
     Enable TCMalloc/JeMalloc with LD_PRELOAD and set configuration for JeMalloc.
     '''
     if enable_tcmalloc and enable_jemalloc:
       logger.error(
           "Unable to enable TCMalloc and JEMalloc at the same time")
-      exit(-1)
+      sys.exit(-1)
 
     if enable_tcmalloc:
       find_tc = self.add_lib_preload(lib_type="tcmalloc")
       if not find_tc:
-        logger.warning("Unable to find the {} library file lib{}.so"
+        logger.warning("Unable to find the %s library file lib%s.so"
                        " in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib"
                        " or /.local/lib/ or /usr/local/lib/ or"
                        " /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
-                       "{}/.local/lib/ so the LD_PRELOAD environment variable"
+                       "%s/.local/lib/ so the LD_PRELOAD environment variable"
                        " will not be set. You can use "
                        "'conda install -c conda-forge gperftools' "
-                       "to install tcmalloc"
-                       .format("TCmalloc", "tcmalloc", expanduser("~")))
+                       "to install tcmalloc", "TCmalloc", "tcmalloc",
+                       expanduser("~"))
       else:
         logger.info("Use TCMalloc memory allocator")
 
     elif enable_jemalloc:
       find_je = self.add_lib_preload(lib_type="jemalloc")
       if not find_je:
-        logger.warning("Unable to find the {} library file lib{}.so"
+        logger.warning("Unable to find the %s library file lib%s.so"
                        " in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib"
                        " or /.local/lib/ or /usr/local/lib/ or"
                        " /usr/local/lib64/ or /usr/lib or /usr/lib64 or "
-                       "{}/.local/lib/ so the LD_PRELOAD environment variable"
+                       "%s/.local/lib/ so the LD_PRELOAD environment variable"
                        " will not be set. You can use "
                        "'conda install -c conda-forge jemalloc'"
-                       " to install jemalloc"
-                       .format("JeMalloc", "jemalloc", expanduser("~")))
+                       " to install jemalloc", "JeMalloc", "jemalloc",
+                       expanduser("~"))
       else:
         logger.info("Use JeMalloc memory allocator")
         self.set_env(
             'MALLOC_CONF',
             "oversize_threshold:1,background_thread:true,metadata_thp:auto")
 
     elif use_default_allocator:
@@ -237,50 +240,51 @@
       if find_je:
         logger.info("Use JeMalloc memory allocator")
         return
       logger.warning("Neither TCMalloc nor JeMalloc is found in"
                      " $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib"
                      " or /.local/lib/ or /usr/local/lib/ or "
                      "/usr/local/lib64/ or /usr/lib or /usr/lib64 or "
-                     "{}/.local/lib/ so the LD_PRELOAD environment "
+                     "%s/.local/lib/ so the LD_PRELOAD environment "
                      "variable will not be set. This may drop the performance."
-                     .format(expanduser("~")))
+                     , expanduser("~"))
 
   def logger_env(self, env_name=""):
     if env_name in os.environ:
-      logger.info("{}={}".format(env_name, os.environ[env_name]))
+      logger.info("%s=%s", env_name, os.environ[env_name])
 
   def set_env(self, env_name, env_value=None):
     if not env_value:
-      logger.warning("{} is None".format(env_name))
+      logger.warning("%s is None", 'env_name')
     if env_name not in os.environ:
       os.environ[env_name] = env_value
     elif os.environ[env_name] != env_value:
-      logger.warning("{} in environment variable is {} "
-            "while the value you set is {}"
-            .format(env_name, os.environ[env_name], env_value))
+      logger.warning("%s in environment variable is %s "
+                     "while the value you set is %s", env_name,
+                     os.environ[env_name], env_value)
     self.logger_env(env_name)
 
   # set_kmp_affinity is used to control whether to set KMP_AFFINITY or not.
   # In scenario that use all cores on all nodes, including logical cores,
   # setting KMP_AFFINITY disables logical cores. In this case, KMP_AFFINITY
   # should not be set.
   def set_multi_thread_and_allocator(self, ncore_per_instance, num_inter,
-                                    num_intra, set_kmp_affinity=True,
-                                    enable_tcmalloc=True, enable_jemalloc=False,
-                                    use_default_allocator=False):
+                                     num_intra, set_kmp_affinity=True,
+                                     enable_tcmalloc=True,
+                                     enable_jemalloc=False,
+                                     use_default_allocator=False):
     '''
     Set multi-thread configuration and enable LLVM openMP and TCMalloc/JeMalloc.
     '''
     self.set_memory_allocator(
         enable_tcmalloc, enable_jemalloc, use_default_allocator)
     self.set_env("OMP_NUM_THREADS", str(ncore_per_instance))
     if set_kmp_affinity:
       if len(self.cpuinfo.get_node_logical_cores(0)) > len(
-              self.cpuinfo.get_node_physical_cores(0)):
+          self.cpuinfo.get_node_physical_cores(0)):
         # HT is on
         self.set_env("KMP_AFFINITY",
                      "granularity=fine,verbose,compact,1,0")
       else:
         # HT is off
         self.set_env("KMP_AFFINITY",
                      "granularity=fine,verbose,compact,")
@@ -288,31 +292,31 @@
     self.set_env("KMP_BLOCKTIME", "1")
     if num_inter is None:
       self.set_env("TF_NUM_INTEROP_THREADS", "1")
     else:
       try:
         num = int(num_inter)
         assert num >= -1
-      except BaseException:
+      except ValueError:
         logger.error(
             "tf_num_interop_threads should be an integer >= -1, "
-            "but input is {}.".format(num_inter))
-        exit(-1)
+            "but input is %s.", 'num_inter')
+        sys.exit(-1)
       self.set_env("TF_NUM_INTEROP_THREADS", num_inter)
     if num_intra is None:
       self.set_env("TF_NUM_INTRAOP_THREADS", str(ncore_per_instance))
     else:
       try:
         num = int(num_intra)
         assert num >= 0
-      except BaseException:
+      except ValueError:
         logger.error(
             "tf_num_intraop_threads should be an integer >= 0, "
-            "but input is {}.".format(num_intra))
-        exit(-1)
+            "but input is %s.", 'num_intra')
+        sys.exit(-1)
       self.set_env("TF_NUM_INTRAOP_THREADS", num_intra)
 
   def set_itex(self, amp=False, enable_layout=False):
     self.set_env("TF_ENABLE_ONEDNN_OPTS", "1")
     if amp:
       self.set_env("ITEX_AUTO_MIXED_PRECISION", "1")
     if enable_layout:
@@ -333,19 +337,21 @@
     enable_taskset = False
     if args.core_list:  # user specify what cores will be used by params
       cores = [int(x) for x in args.core_list.split(",")]
       if args.ncore_per_instance == -1:
         logger.error(
             "please specify the '--ncore_per_instance' "
             "if you have pass the --core_list params")
-        exit(-1)
-      elif args.ninstances > 1 and args.ncore_per_instance * args.ninstances < len(cores):
-        logger.warning("only first {} cores will be used, "
-        "but you specify {} cores in core_list".format(
-            args.ncore_per_instance * args.ninstances, len(cores)))
+        sys.exit(-1)
+      elif args.ninstances > 1 and args.ncore_per_instance \
+           * args.ninstances < len(cores):
+        logger.warning("only first %s cores will be used, "
+                       "but you specify %s cores in core_list",
+                       args.ncore_per_instance * args.ninstances, \
+                       len(cores))
       else:
         args.ninstances = len(cores) // args.ncore_per_instance
 
     else:
       if args.use_logical_core:
         if args.node_id != -1:
           cores = self.cpuinfo.get_node_logical_cores(args.node_id)
@@ -362,89 +368,97 @@
           cores = self.cpuinfo.get_all_physical_cores()
 
       def skip_cores(cores):
         ncore_per_node = len(self.cpuinfo.node_physical_cores[0])
         num_leftover_cores = ncore_per_node % args.ncore_per_instance
         if args.ncore_per_instance > ncore_per_node:
           # too many ncore_per_instance to skip cross-node cores
-          logger.warning("there are {} core(s) per socket, "
-                         "but you specify {} ncore_per_instance "
+          logger.warning("there are %s core(s) per socket, "
+                         "but you specify %s ncore_per_instance "
                          "and skip_cross_node_cores. "
                          "Please make sure --ncore_per_instance < core(s) "
-                         "per socket".format(
-              ncore_per_node, args.ncore_per_instance))
-          exit(-1)
+                         "per socket", ncore_per_node, args.ncore_per_instance)
+          sys.exit(-1)
         elif num_leftover_cores == 0:
           # aren't any cross-node cores
           logger.info(
-              '--skip_cross_node_cores is set, but there are no cross-node cores.')
+              '--skip_cross_node_cores is set, but there are no \
+               cross-node cores.')
           args.ninstances = len(cores) // args.ncore_per_instance
+          return cores
         else:
           # skip cross-node cores
           if args.ninstances != -1:
             logger.warning(
                 "--skip_cross_node_cores is exclusive to --ninstances. "
                 "--ninstances won\'t take effect even if it is set explicitly.")
           i = 1
           leftover_cores = set()
           while ncore_per_node * i <= len(cores):
             leftover_cores.update(
-                cores[ncore_per_node * i - num_leftover_cores: ncore_per_node * i])
+                cores[ncore_per_node * i - num_leftover_cores: \
+                ncore_per_node * i])
             i += 1
           cores = list(set(cores) - leftover_cores)
           assert len(cores) % args.ncore_per_instance == 0
           args.ninstances = len(cores) // args.ncore_per_instance
           return cores
+
       if not args.multi_instance and args.ninstances == - \
               1 and args.ncore_per_instance == -1:
         args.ninstances = 1
         args.ncore_per_instance = len(cores)
-      elif args.multi_instance and args.ninstances == -1 and args.ncore_per_instance == -1:
+      elif args.multi_instance and args.ninstances == -1 and \
+           args.ncore_per_instance == -1:
         args.throughput_mode = True
       elif args.ncore_per_instance == -1 and args.ninstances != -1:
         if args.ninstances > len(cores):
-          logger.error("there are {} total cores but you specify {} ninstances; "
-              "please make sure ninstances <= total_cores)".format(
-              len(cores), args.ninstances))
-          exit(-1)
+          logger.error("there are %s total cores but you specify \
+                        %s ninstances; "
+                       "please make sure ninstances <= total_cores)",
+                       len(cores), args.ninstances)
+          sys.exit(-1)
         else:
           args.ncore_per_instance = len(cores) // args.ninstances
       elif args.ncore_per_instance != -1 and args.ninstances == -1:
         if not args.skip_cross_node_cores:
           args.ninstances = len(cores) // args.ncore_per_instance
         else:
           cores = skip_cores(cores)
-      elif args.ncore_per_instance != -1 and args.ninstances != -1 and args.skip_cross_node_cores:
+      elif args.ncore_per_instance != -1 and args.ninstances != -1 \
+           and args.skip_cross_node_cores:
         cores = skip_cores(cores)
       else:
         if args.ninstances * args.ncore_per_instance > len(cores):
           logger.error(
               "Please make sure ninstances * ncore_per_instance <= total_cores")
-          exit(-1)
+          sys.exit(-1)
       if args.latency_mode:
         logger.warning(
-            "--latency_mode is exclusive to --ninstances, --ncore_per_instance, "
+            "--latency_mode is exclusive to --ninstances, \
+             --ncore_per_instance, "
             "--node_id and --use_logical_core. "
             "They won\'t take effect even if they are set explicitly.")
         args.ncore_per_instance = 4
         cores = self.cpuinfo.get_all_physical_cores()
         args.ninstances = len(cores) // args.ncore_per_instance
 
       if args.throughput_mode:
         logger.warning(
-            "--throughput_mode is exclusive to --ninstances, --ncore_per_instance, "
+            "--throughput_mode is exclusive to --ninstances, \
+             --ncore_per_instance, "
             "--node_id and --use_logical_core. "
             "They won\'t take effect even if they are set explicitly.")
         args.ninstances = self.cpuinfo.node_nums()
         cores = self.cpuinfo.get_all_physical_cores()
         args.ncore_per_instance = len(cores) // args.ninstances
 
     if args.ninstances > 1 and args.instance_idx != -1:
-      logger.info("assigning {} cores for instance {}".format(
-          args.ncore_per_instance, args.instance_idx))
+      logger.info("assigning %s cores for instance %s", args.ncore_per_instance,
+      args.instance_idx)
 
     if not args.disable_numactl:
       numactl_available = self.is_numactl_available()
       if not numactl_available:
         if not args.disable_taskset:
           logger.warning(
               "Core binding with numactl is not available. "
@@ -453,16 +467,17 @@
               "please use numactl if memory binding is needed.")
           args.disable_numactl = True
           enable_taskset = True
         else:
           logger.warning(
               "Core binding with numactl is not available, "
               "and --disable_taskset is set. "
-              "Please unset --disable_taskset to use taskset insetad of numactl.")
-          exit(-1)
+              "Please unset --disable_taskset to use taskset \
+               insetad of numactl.")
+          sys.exit(-1)
 
     if not args.disable_taskset:
       enable_taskset = True
 
     self.set_multi_thread_and_allocator(args.ncore_per_instance,
                                         args.tf_num_interop_threads,
                                         args.tf_num_intraop_threads,
@@ -547,97 +562,129 @@
 
 def add_itex_params(parser):
 
   group = parser.add_argument_group("ITEX Parameters")
   # ITEX control
   group.add_argument("--enable_itex_amp", action='store_true', default=False,
                      help="Enable ITEX AMP")
-  group.add_argument("--enable_itex_layout_opt", action='store_true', default=False,
+  group.add_argument("--enable_itex_layout_opt", action='store_true', \
+                      default=False,
                      help="Enable ITEX layout opt")
 
 
 def add_memory_allocator_params(parser):
 
   group = parser.add_argument_group("Memory Allocator Parameters")
   # allocator control
   group.add_argument("--enable_tcmalloc", action='store_true', default=False,
                      help="Enable tcmalloc allocator")
   group.add_argument("--enable_jemalloc", action='store_true', default=False,
                      help="Enable jemalloc allocator")
-  group.add_argument("--use_default_allocator", action='store_true', default=False,
+  group.add_argument("--use_default_allocator", action='store_true', \
+                      default=False,
                      help="Use default memory allocator")
 
 
 def add_multi_instance_params(parser):
-
+  """A dummy docstring"""
   group = parser.add_argument_group("Multi-instance Parameters")
   # multi-instance control
   group.add_argument("--ncore_per_instance", metavar='\b', default=-1, type=int,
                      help="Cores per instance")
-  group.add_argument("--skip_cross_node_cores", action='store_true', default=False,
-                     help="If specified --ncore_per_instance, skips cross-node cores.")
+  group.add_argument("--skip_cross_node_cores", action='store_true', \
+                      default=False,
+                     help="If specified --ncore_per_instance, \
+                           skips cross-node cores.")
   group.add_argument("--ninstances", metavar='\b', default=-1, type=int,
-                     help="For multi-instance, you should give the cores number you used for per instance.")
+                     help="For multi-instance, you should give the \
+                           cores number you used for per instance.")
   group.add_argument("--instance_idx", metavar='\b', default="-1", type=int,
                      help="Specify instance index to assign ncores_per_instance for instance_idx; otherwise ncore_per_instance will be assigned sequentially to ninstances. Please refer to https://github.com/intel-innersource/frameworks.ai.infrastructure.intel-extension-for-tensorflow.intel-extension-for-tensorflow/tree/master/docs/guide/launch.md")
   group.add_argument("--latency_mode", action='store_true', default=False,
-                     help="By detault 4 core per instance and use all physical cores")
-  group.add_argument("--throughput_mode", action='store_true', default=False,
-                     help="By default one instance per node and use all physical cores")
+                     help="By detault 4 core per instance and use all \
+                           physical cores")
+  group.add_argument("--throughput_mode", action='store_true', \
+                      default=False,
+                     help="By default one instance per node and use \
+                           all physical cores")
   group.add_argument("--node_id", metavar='\b', default=-1, type=int,
-                     help="node id for multi-instance, by default all nodes will be used")
+                     help="node id for multi-instance, by default all \
+                           nodes will be used")
   group.add_argument("--use_logical_core", action='store_true', default=False,
                      help="Whether only use physical cores")
   group.add_argument("--disable_numactl", action='store_true', default=False,
                      help="Disable numactl")
   group.add_argument("--disable_taskset", action='store_true', default=False,
                      help="Disable taskset")
   group.add_argument("--core_list", metavar='\b', default=None, type=str,
-                     help="Specify the core list as 'core_id, core_id, ....', otherwise, all the cores will be used.")
-  group.add_argument("--tf_num_interop_threads", metavar='\b', default=None, type=str,
+                     help="Specify the core list as 'core_id, core_id, \
+                           ....', otherwise, all the cores will be used.")
+  group.add_argument("--tf_num_interop_threads", metavar='\b', \
+                      default=None, type=str,
                      help="Set TF_NUM_INTEROP_THREADS, by default it is 1.")
-  group.add_argument("--tf_num_intraop_threads", metavar='\b', default=None, type=str,
-                     help="Set TF_NUM_INTRAOP_THREADS, by default it equals to number of cores per instance.")
+  group.add_argument("--tf_num_intraop_threads", metavar='\b', \
+                      default=None, type=str,
+                     help="Set TF_NUM_INTRAOP_THREADS, by default it \
+                           equals to number of cores per instance.")
   group.add_argument("--log_path", metavar='\b', default="", type=str,
-                     help="The log file directory. Default path is '', which means disable logging to files.")
+                     help="The log file directory. Default path is '', \
+                           which means disable logging to files.")
   group.add_argument("--log_file_prefix", metavar='\b', default="run", type=str,
                      help="log file prefix")
 
 
 def parse_args():
   """
   Helper function parsing the command line options
   @retval ArgumentParser
   """
-  parser = ArgumentParser(description="This is a script for launching Tensorflow training and inference on Intel Xeon CPU "
-                                      "with optimal configurations. Now, single instance inference/training, multi-instance "
+  parser = ArgumentParser(description="This is a script for launching \
+                          Tensorflow training and inference on Intel Xeon CPU "
+                                      "with optimal configurations. \
+                          Now, single instance inference/training, \
+                          multi-instance "
                                       "inference/training are enabled. "
-                                      "To get the peak performance on Intel Xeon CPU, the script optimizes the configuration "
-                                      "of thread and memory management. For thread management, the script configures thread "
-                                      "affinity and the preload of Intel OMP library. For memory management, it configures "
-                                      "NUMA binding and preload optimized memory allocation library (e.g. tcmalloc, jemalloc) "
+                                      "To get the peak performance on \
+                          Intel Xeon CPU, the script optimizes the \
+                          configuration "
+                                      "of thread and memory management. \
+                          For thread management, the script configures thread "
+                                      "affinity and the preload of Intel\
+                          OMP library. For memory management, it configures "
+                                      "NUMA binding and preload optimized\
+                          memory allocation library (e.g. tcmalloc, jemalloc) "
                                       "\n################################# Basic usage ############################# \n"
                                       "\n 1. single instance\n"
-                                      "\n   >>> python -m intel_extension_for_tensorflow.python.launch python_script args \n"
+                                      "\n   >>> python -m \
+                          intel_extension_for_tensorflow.python.launch \
+                          python_script args \n"
                                       "\n2. multi-instance \n"
-                                      "\n    >>> python -m intel_extension_for_tensorflow.python.launch --ninstances xxx --ncore_per_instance xx python_script args\n"
+                                      "\n    >>> python -m \
+                          intel_extension_for_tensorflow.python.launch \
+                          --ninstances xxx --ncore_per_instance xx \
+                          python_script args\n"
                                       "\n############################################################################# \n",
-                                      formatter_class=RawTextHelpFormatter)
+                          formatter_class=RawTextHelpFormatter)
 
   parser.add_argument("--multi_instance", action='store_true', default=False,
-                      help="Enable multi-instance, by default one instance per socket")
+                      help="Enable multi-instance, by default one \
+                            instance per socket")
 
   parser.add_argument("-m", "--module", default=False, action="store_true",
-                      help="Changes each process to interpret the launch script "
-                           "as a python module, executing with the same behavior as"
+                      help="Changes each process to interpret the \
+                            launch script "
+                           "as a python module, executing with the same \
+                            behavior as"
                            "'python -m'.")
 
   parser.add_argument("--no_python", default=False, action="store_true",
-                      help="Do not prepend the --program script with \"python\" - just exec "
-                           "it directly. Useful when the script is not a Python script.")
+                      help="Do not prepend the --program script \
+                            with \"python\" - just exec "
+                           "it directly. Useful when the script is \
+                            not a Python script.")
 
   add_memory_allocator_params(parser)
   add_itex_params(parser)
   add_multi_instance_params(parser)
   # positional
   parser.add_argument("program", type=str,
                       help="The full path to the proram/script to be launched. "
@@ -659,47 +706,47 @@
         '/') else args.log_path + '/')
     if not os.path.exists(path):
       os.makedirs(path)
     args.log_path = path
 
     args.log_file_prefix = '{}_{}'.format(
         args.log_file_prefix, datetime.now().strftime("%Y%m%d%H%M%S"))
-    fileHandler = logging.FileHandler(
+    file_handler = logging.FileHandler(
         "{0}/{1}_instances.log".format(args.log_path, args.log_file_prefix))
-    logFormatter = logging.Formatter(format_str)
-    fileHandler.setFormatter(logFormatter)
-    logger.addHandler(fileHandler)
+    log_formatter = logging.Formatter(format_str)
+    file_handler.setFormatter(log_formatter)
+    logger.addHandler(file_handler)
 
   if args.latency_mode and args.throughput_mode:
     raise RuntimeError(
         "Either args.latency_mode or args.throughput_mode should be set")
 
   if not args.no_python and not args.program.endswith(".py"):
     logger.error(
         "For non Python script, you should use '--no_python' parameter.")
-    exit()
+    sys.exit()
 
   # Verify LD_PRELOAD
   if "LD_PRELOAD" in os.environ:
     lst_valid = []
     tmp_ldpreload = os.environ["LD_PRELOAD"]
     for item in tmp_ldpreload.split(":"):
       if item != "":
         matches = glob.glob(item)
         if len(matches) > 0:
           lst_valid.append(item)
         else:
           logger.warning(
-              "{} doesn't exist. Removing it from LD_PRELOAD.".format(item))
+              "%s doesn't exist. Removing it from LD_PRELOAD.", 'item')
     if len(lst_valid) > 0:
       os.environ["LD_PRELOAD"] = ":".join(lst_valid)
     else:
       os.environ["LD_PRELOAD"] = ""
 
   launcher = MultiInstanceLauncher()
   launcher.launch(args)
   for x in sorted(set(os.environ.keys()) - env_before):
-    logger.debug('{0}={1}'.format(x, os.environ[x]))
+    logger.debug('%s=%s', x, os.environ[x])
 
 
 if __name__ == "__main__":
   main()
```

## intel_extension_for_tensorflow/python/version.py

```diff
@@ -12,14 +12,14 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 '''version information for Intel  Extension for TensorFlow*'''
 
-__version__ = '1.1.0'
-__git_desc__= 'd4f2f46e'
+__version__ = '1.2.0rc0'
+__git_desc__= '0b1ee1b4'
 VERSION = __version__
-GIT_VERSION = __git_desc__ if len(__git_desc__) > 8 else 'v' + __version__ + '-' + __git_desc__
-COMPILER_VERSION = 'gcc-, dpcpp-2023.0.0.20221201'
-ONEDNN_GIT_VERSION = 'v2.7.0-4ee69b1f'
+GIT_VERSION = 'v' + __version__ + '-' + __git_desc__
+COMPILER_VERSION = 'dpcpp-'
+ONEDNN_GIT_VERSION = 'v3.1.0-ad34c124'
 TF_COMPATIBLE_VERSION = '>= 2.8.0'
```

## intel_extension_for_tensorflow/python/ops/activations.py

```diff
@@ -58,8 +58,8 @@
       if `approximate` is `False`.
 
   Reference:
     - [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)
   """
   with ops.name_scope(name, "Gelu", [features]):
     features = ops.convert_to_tensor(features, name="features")
-    return load_ops_library.gelu(features, approximate)
+    return load_ops_library.itex_gelu(features, approximate)
```

## intel_extension_for_tensorflow/python/ops/layer_norm.py

```diff
@@ -51,15 +51,15 @@
   offset = ops.convert_to_tensor(offset, name="offset")
 
   # Set a minimum epsilon to 1.001e-5, which is a requirement by CUDNN to
   # prevent exception (see cudnn.h).
   min_epsilon = 1.001e-5
   epsilon = epsilon if epsilon > min_epsilon else min_epsilon
 
-  y, running_mean, running_var = load_ops_library.layer_norm(
+  y, running_mean, running_var = load_ops_library.itex_layer_norm(
       x,
       scale,
       offset,
       epsilon=epsilon,
       is_training=is_training,
       data_format=data_format)
   return y, running_mean, running_var
@@ -269,15 +269,16 @@
 
     if len(axis) == 1:
       self._is_one_axis_len = True
     else:
       self._is_one_axis_len = False
 
     if self.dtype == 'float64':
-      raise ValueError('Itex Layernorm only support float32, bfloat16 and float16.')
+      raise ValueError('Itex Layernorm only support float32, \
+                        bfloat16 and float16.')
 
     return can_use_onednn_layer_norm
 
   @property
   def _param_dtype(self):
     # Raise parameters of fp16 layer tch norm to fp32
     if self.dtype == dtypes.float16 or self.dtype == dtypes.bfloat16: # pylint: disable=no-else-return
```

## intel_extension_for_tensorflow/python/ops/ops_grad.py

```diff
@@ -19,29 +19,54 @@
 
 @ops.RegisterGradient("Gelu")
 def _gelu_grad(op, grad):
   return load_ops_library.gelu_grad(
       grad, op.inputs[0], op.get_attr("approximate")
   )
 
+@ops.RegisterGradient("ITEXGelu")
+def _itex_gelu_grad(op, grad):
+  return load_ops_library.itex_gelu_grad(
+      grad, op.inputs[0], op.get_attr("approximate")
+  )
+
 @ops.RegisterGradient("LayerNorm")
 def _layer_norm_grad(op, *grad):
+  """A dummy docstring."""
   x = op.inputs[0]
   grad_y = grad[0]
   scale = op.inputs[1]
   epsilon = op.get_attr("epsilon")
   is_training = op.get_attr("is_training")
   data_format = op.get_attr("data_format")
   grad_fun = load_ops_library.layer_norm_grad
   reserve_space_1 = op.outputs[1]
   reserve_space_2 = op.outputs[2]
   dx, dscale, doffset, _, _ = grad_fun(
-    y_backprop=grad_y, x=x, scale=scale, reserve_space_1=reserve_space_1,
-    reserve_space_2=reserve_space_2, epsilon=epsilon, is_training=is_training,
-    data_format=data_format)
+      y_backprop=grad_y, x=x, scale=scale, reserve_space_1=reserve_space_1,
+      reserve_space_2=reserve_space_2, epsilon=epsilon, is_training=is_training,
+      data_format=data_format)
+  return dx, dscale, doffset
+
+@ops.RegisterGradient("ITEXLayerNorm")
+def _itex_layer_norm_grad(op, *grad):
+  """A dummy docstring."""
+  x = op.inputs[0]
+  grad_y = grad[0]
+  scale = op.inputs[1]
+  epsilon = op.get_attr("epsilon")
+  is_training = op.get_attr("is_training")
+  data_format = op.get_attr("data_format")
+  grad_fun = load_ops_library.itex_layer_norm_grad
+  reserve_space_1 = op.outputs[1]
+  reserve_space_2 = op.outputs[2]
+  dx, dscale, doffset, _, _ = grad_fun(
+      y_backprop=grad_y, x=x, scale=scale, reserve_space_1=reserve_space_1,
+      reserve_space_2=reserve_space_2, epsilon=epsilon, is_training=is_training,
+      data_format=data_format)
   return dx, dscale, doffset
 
 @ops.RegisterGradient("ItexRnn")
 def _itex_rnn_grad(op, *grad):
   if not op.get_attr("is_training"):
     raise ValueError("To use RNN in gradients, is_training must be True.")
   return load_ops_library.itex_rnn_grad(
@@ -61,9 +86,8 @@
       output_c_backprop=grad[2],
       rnn_mode=op.get_attr("rnn_mode"),
       dropout=op.get_attr("dropout"),
       recurrent_dropout=op.get_attr("recurrent_dropout"),
       # seed=op.get_attr("seed"),
       # seed2=op.get_attr("seed2"),
       num_proj=op.get_attr("num_proj"),
-      var_seq_length=op.get_attr("var_seq_length")) + (None, None, None, )
-  
+      var_seq_length=op.get_attr("var_seq_length")) + (None, None, None,)
```

## intel_extension_for_tensorflow/python/ops/optimizers.py

```diff
@@ -61,14 +61,15 @@
         graph = None
       else:
         graph = ops.get_default_graph()
       return (self._get_non_slot_variable("beta_1_power", graph=graph),
               self._get_non_slot_variable("beta_2_power", graph=graph))
 
   def _create_slots(self, var_list):
+    """A dummy docstring."""
     # Create the beta_1 and beta_2 accumulators on the same device as the first
     # variable. Sort the var_list to make sure this device is consistent across
     # workers (these need to go on the same PS, otherwise some updates are
     # silently ignored).
     first_var = min(var_list, key=lambda x: x.name)
     self._create_non_slot_variable(
         initial_value=self._beta_1,
@@ -109,14 +110,15 @@
     if self.exclude_from_weight_decay:
       for r in self.exclude_from_weight_decay:
         if re.search(r, param_name) is not None:
           return False
     return True
 
   def _apply_dense(self, grad, var):
+    """A dummy docstring."""
     m = self.get_slot(var, "m")
     v = self.get_slot(var, "v")
     beta_1_power, beta_2_power = self._get_beta_accumulators()
     param_name = self._get_variable_name(var.name)
     if self._do_use_weight_decay(param_name): # pylint: disable=no-else-return
       return load_ops_library.apply_adam_with_weight_decay(
           var,
@@ -142,14 +144,15 @@
           math_ops.cast(self._beta_1_t, var.dtype.base_dtype),
           math_ops.cast(self._beta_2_t, var.dtype.base_dtype),
           math_ops.cast(self._epsilon_t, var.dtype.base_dtype),
           grad,
           use_locking=self._use_locking).op
 
   def _resource_apply_dense(self, grad, var): # pylint: disable=arguments-differ
+    """A dummy docstring."""
     m = self.get_slot(var, "m")
     v = self.get_slot(var, "v")
     beta_1_power, beta_2_power = self._get_beta_accumulators()
     param_name = self._get_variable_name(var.name)
     if self._do_use_weight_decay(param_name): # pylint: disable=no-else-return
       return load_ops_library.resource_apply_adam_with_weight_decay(
           var.handle,
```

## intel_extension_for_tensorflow/python/ops/recurrent.py

```diff
@@ -13,23 +13,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 # pylint: disable=g-classes-have-attributes
 """Recurrent layers for TF 2."""
 
-import uuid
+#import uuid
 
 from intel_extension_for_tensorflow.python.ops.load_ops_library import load_ops_library
-from tensorflow.python.eager import context
+#from tensorflow.python.eager import context
 from tensorflow.python.framework import config
 from tensorflow.python.framework import constant_op
-from tensorflow.python.framework import ops
+#from tensorflow.python.framework import ops
 from tensorflow.python.ops import array_ops
-from tensorflow.python.ops import control_flow_ops
+#from tensorflow.python.ops import control_flow_ops
 from tensorflow.python.ops import math_ops
 from tensorflow.python.ops import nn
 from tensorflow.python.ops import state_ops
 from tensorflow.python.ops import variables
 from tensorflow.python.platform import tf_logging as logging
 
 from tensorflow.python import keras
@@ -50,15 +50,15 @@
 def _canonical_to_params(weights, biases, shape, transpose_weights=False):
   """Utility function convert variable to Itex compatible parameter.
 
   Note that Keras weights for kernels are different from the Itex format. Eg.:
 
   ```
     Keras                                       Itex
-    kernel: (ic, 4 * hc)          <--------->  kernel: (4, hc, ic) 
+    kernel: (ic, 4 * hc)          <--------->  kernel: (4, hc, ic)
     recurrent_kernel: (hc, 4 * hc)             recurrent_kernel: (4, hc, hc)
   ```
 
   Args:
     weights: list of weights for the individual kernels and recurrent kernels.
     biases: list of biases for individual gate.
     shape: the shape for the converted variables that will be feed to Itex.
@@ -317,29 +317,31 @@
       # Only show the message when there is GPU available, itex LSTM only support GPU currently
       if self._could_use_itex_kernel:
         logging.debug(_ITEX_AVAILABLE_MSG % self.name)
       else:
         logging.warning(_ITEX_NOT_AVAILABLE_MSG % self.name)
 
   def call(self, inputs, mask=None, training=None, initial_state=None):
+    """A dummy docstring."""
     # The input should be dense, padded with zeros. If a ragged input is fed
     # into the layer, it is padded and the row lengths are used for masking.
     inputs, row_lengths = backend.convert_inputs_if_ragged(inputs)
     is_ragged_input = (row_lengths is not None)
     self._validate_args_if_ragged(is_ragged_input, mask)
-    
+
     # TODO: support ragged_input and mask in the future
-    self._could_use_itex_kernel = (self._could_use_itex_kernel and (not is_ragged_input) and (mask is None))
-    
+    self._could_use_itex_kernel = (self._could_use_itex_kernel and \
+       (not is_ragged_input) and (mask is None))
+
     # LSTM does not support constants. Ignore it during process.
     inputs, initial_state, _ = self._process_inputs(
         inputs, initial_state, None)
 
     self._maybe_reset_cell_dropout_mask(self.cell)
-    
+
     if isinstance(mask, list):
       mask = mask[0]
 
     gpu_lstm_kwargs = {
         'cell': self.cell,
         'inputs': inputs,
         'mask': mask,
@@ -353,15 +355,16 @@
 
     normal_lstm_kwargs.update({
         'unroll': self.unroll,
         'zero_output_for_mask': self.zero_output_for_mask,
     })
 
     can_use_gpu = ((config.list_logical_devices('XPU')) and
-            (mask is None or is_itex_supported_inputs(mask, self.time_major)))
+                   (mask is None or is_itex_supported_inputs\
+                   (mask, self.time_major)))
     if self._could_use_itex_kernel and can_use_gpu:
       last_output, outputs, new_h, new_c = gpu_lstm(
           **gpu_lstm_kwargs)
     else:
       # Fall back to use the normal LSTM.
       last_output, outputs, new_h, new_c = standard_lstm(
           **normal_lstm_kwargs)
@@ -381,19 +384,19 @@
       output = backend.maybe_convert_to_ragged(
           is_ragged_input, outputs, row_lengths, go_backwards=self.go_backwards)
     else:
       output = last_output
 
     if self.return_state:
       return [output] + list(states)
-    else:
-      return output
-      
+    return output
+
   def get_config(self):
-    config = {
+    """A dummy docstring."""
+    derive_config = {
         'units':
             self.units,
         'activation':
             activations.serialize(self.activation),
         'recurrent_activation':
             activations.serialize(self.recurrent_activation),
         'use_bias':
@@ -422,16 +425,16 @@
             constraints.serialize(self.bias_constraint),
         'dropout':
             self.dropout,
         'recurrent_dropout':
             self.recurrent_dropout
     }
     base_config = super(ItexLSTM, self).get_config()
-    return dict(list(base_config.items()) + list(config.items()))    
-    
+    return dict(list(base_config.items()) + list(derive_config.items()))
+
 def standard_lstm(cell, inputs, mask, training, initial_state, sequence_lengths,
                   go_backwards, time_major, unroll, zero_output_for_mask):
   """LSTM with standard kernel implementation.
 
   Args:
     cell: a LSTM cell instance.
     inputs: input tensor of LSTM layer.
@@ -477,15 +480,16 @@
   last_output, outputs, new_states = backend.rnn(
       step,
       inputs,
       initial_state,
       go_backwards=go_backwards,
       mask=mask,
       unroll=unroll,
-      input_length=sequence_lengths if sequence_lengths is not None else timesteps,
+      input_length=sequence_lengths if sequence_lengths \
+                   is not None else timesteps,
       time_major=time_major,
       zero_output_for_mask=zero_output_for_mask)
   return last_output, outputs, new_states[0], new_states[1]
 
 
 def gpu_lstm(cell, inputs, mask, training, initial_state, sequence_lengths,
              go_backwards, time_major):
@@ -523,43 +527,44 @@
     runtime: Constant string tensor which indicate real runtime hardware. This
       value is for testing purpose and should not be used by user.
   """
   #TODO: Below cast is caused by states has differnet datat type with input when set stateful in official tensorflow
   #Maybe remove this in the future
   init_h = math_ops.cast(_read_variable_value(initial_state[0]), inputs.dtype)
   init_c = math_ops.cast(_read_variable_value(initial_state[1]), inputs.dtype)
-  kernel = math_ops.cast(_read_variable_value(cell.kernel), inputs.dtype) 
-  recurrent_kernel = math_ops.cast(_read_variable_value(cell.recurrent_kernel), inputs.dtype) 
-  bias = math_ops.cast(_read_variable_value(cell.bias),inputs.dtype)  
+  kernel = math_ops.cast(_read_variable_value(cell.kernel), inputs.dtype)
+  recurrent_kernel = math_ops.cast(_read_variable_value(cell.recurrent_kernel),\
+                                   inputs.dtype)
+  bias = math_ops.cast(_read_variable_value(cell.bias), inputs.dtype)
 
   if not time_major:
     inputs = array_ops.transpose(inputs, perm=(1, 0, 2))
 
   weights = array_ops.split(kernel, 4, axis=1)
   weights += array_ops.split(recurrent_kernel, 4, axis=1)
 
   params = _canonical_to_params(
       weights=weights,
       biases=array_ops.split(bias, 4),
       shape=constant_op.constant([-1]),
       transpose_weights=True)
-  
+
   # TODO, generate mask in c++ side
   dropout = _read_variable_value(cell.dropout)
-  if dropout > 0 and dropout < 1.0:
+  if 0 < dropout < 1.0:
     dp_mask = cell.get_dropout_mask_for_cell(inputs[0], training, count=4)
     dp_mask = array_ops.concat(dp_mask, axis=0)
   else:
     dp_mask = 0
-    
+
   recurrent_dropout = _read_variable_value(cell.recurrent_dropout)
-  if recurrent_dropout > 0 and recurrent_dropout < 1.0:
+  if 0 < recurrent_dropout < 1.0:
     rec_dp_mask = cell.get_recurrent_dropout_mask_for_cell(
-            init_h, training, count=4)
-    rec_dp_mask = array_ops.concat(rec_dp_mask, axis=0)  
+        init_h, training, count=4)
+    rec_dp_mask = array_ops.concat(rec_dp_mask, axis=0)
   else:
     rec_dp_mask = 0
 
   if mask is not None:
     sequence_lengths = calculate_sequence_by_mask(
         mask, time_major)
 
@@ -581,19 +586,23 @@
         dropout_mask=dp_mask,
         recurrent_dropout=recurrent_dropout,
         recurrent_dropout_mask=rec_dp_mask,
         sequence_lengths=sequence_lengths,
         rnn_mode='lstm',
         var_seq_length=True,
         is_training=training)
-    # TODO: below reshape operation is added as tensorflow shape inference c api bug, maybe remove this once rebase tensorflow>=2.10.0
-    outputs = array_ops.reshape(outputs, 
-                [array_ops.shape(inputs)[0], array_ops.shape(inputs)[1], array_ops.shape(init_h)[1]])
-    h = array_ops.reshape(h, [array_ops.shape(init_h)[0], array_ops.shape(init_h)[1]])
-    c = array_ops.reshape(c, [array_ops.shape(init_c)[0], array_ops.shape(init_c)[1]])    
+    # TODO: below reshape operation is added as tensorflow shape inference c api bug, maybe remove this once rebase tensorflow==2.12.0
+    outputs = array_ops.reshape(outputs,
+                                [array_ops.shape(inputs)[0], \
+                                array_ops.shape(inputs)[1], \
+                                array_ops.shape(init_h)[1]])
+    h = array_ops.reshape(h, [array_ops.shape(init_h)[0], \
+                          array_ops.shape(init_h)[1]])
+    c = array_ops.reshape(c, [array_ops.shape(init_c)[0], \
+                          array_ops.shape(init_c)[1]])
     if go_backwards:
       outputs = array_ops.reverse_sequence_v2(
           outputs, sequence_lengths, seq_axis=0, batch_axis=1)
       outputs = array_ops.reverse(outputs, axis=[0])
   else:
     # # Fill the array with shape [batch] with value of max timesteps.
     # sequence_length = array_ops.fill([array_ops.shape(inputs)[1]],
@@ -609,19 +618,23 @@
         dropout=dropout,
         dropout_mask=dp_mask,
         recurrent_dropout=recurrent_dropout,
         recurrent_dropout_mask=rec_dp_mask,
         sequence_lengths=0,
         rnn_mode='lstm',
         is_training=training)
-    # TODO: below reshape operation is added as tensorflow shape inference c api bug, maybe remove this once rebase tensorflow>=2.10.0
-    outputs = array_ops.reshape(outputs, 
-            [array_ops.shape(inputs)[0], array_ops.shape(inputs)[1], array_ops.shape(init_h)[1]])
-    h = array_ops.reshape(h, [array_ops.shape(init_h)[0], array_ops.shape(init_h)[1]])
-    c = array_ops.reshape(c, [array_ops.shape(init_c)[0], array_ops.shape(init_c)[1]])
+    # TODO: below reshape operation is added as tensorflow shape inference c api bug, maybe remove this once rebase tensorflow==2.12.0
+    outputs = array_ops.reshape(outputs,
+                                [array_ops.shape(inputs)[0], \
+                                array_ops.shape(inputs)[1], \
+                                array_ops.shape(init_h)[1]])
+    h = array_ops.reshape(h, [array_ops.shape(init_h)[0], \
+                          array_ops.shape(init_h)[1]])
+    c = array_ops.reshape(c, [array_ops.shape(init_c)[0], \
+                          array_ops.shape(init_c)[1]])
 
   last_output = outputs[-1]
   if not time_major:
     outputs = array_ops.transpose(outputs, perm=[1, 0, 2])
 
   # In the case of variable length input, the ITEX kernel will fill zeros for
   # the output, whereas the default keras behavior is to bring over the previous
```

## intel_extension_for_tensorflow/python/test_func/__init__.py

```diff
@@ -1,7 +1,8 @@
+"""System module."""
 # Copyright (c) 2022 Intel Corporation
 #
 # Copyright 2015 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
```

## intel_extension_for_tensorflow/python/test_func/eager_test.py

```diff
@@ -13,15 +13,14 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 """Utilities for testing tfe code."""
 
 from tensorflow.python.framework import ops as _ops
-
 from intel_extension_for_tensorflow.python.test_func import test as _test
 from intel_extension_for_tensorflow.python.test_func.test import *
 
 
 # TODO(akshayka): Do away with this file.
 def main(argv=None):  # pylint: disable=function-redefined
   _ops.enable_eager_execution()
```

## intel_extension_for_tensorflow/python/test_func/keras_parameterized.py

```diff
@@ -12,35 +12,35 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 """Utilities for unit-testing Keras."""
 
-import tensorflow.compat.v2 as tf
 
 import collections
 import functools
 import itertools
 import unittest
 
+import tensorflow.compat.v2 as tf
 from absl.testing import parameterized
 
 import keras
 import intel_extension_for_tensorflow.python.test_func.keras_testing_utils as testing_utils
 
 try:
   import h5py  # pylint:disable=g-import-not-at-top
 except ImportError:
   h5py = None
 
 
 class TestCase(tf.test.TestCase, parameterized.TestCase):
 
-  def tearDown(self):
+  def tear_down(self):
     keras.backend.clear_session()
     super(TestCase, self).tearDown()
 
 
 def run_with_all_saved_model_formats(
     test_or_class=None,
     exclude_formats=None):
```

## intel_extension_for_tensorflow/python/test_func/keras_testing_utils.py

```diff
@@ -12,22 +12,22 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 """Utilities for unit-testing Keras."""
 
-import tensorflow.compat.v2 as tf
-
 import collections
 import contextlib
 import functools
 import itertools
 import threading
 
+import tensorflow.compat.v2 as tf
+
 import numpy as np
 from intel_extension_for_tensorflow.python.test_func import test_util
 from keras import backend
 from keras import layers
 from keras import models
 from keras.engine import base_layer_utils
 try:
@@ -35,21 +35,30 @@
   from keras.optimizers.optimizer_v2 import adagrad as adagrad_v2
   from keras.optimizers.optimizer_v2 import adam as adam_v2
   from keras.optimizers.optimizer_v2 import adamax as adamax_v2
   from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_v2
   from keras.optimizers.optimizer_v2 import nadam as nadam_v2
   from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_v2
 except ImportError:
-  from keras.optimizer_v2 import adadelta as adadelta_v2
-  from keras.optimizer_v2 import adagrad as adagrad_v2
-  from keras.optimizer_v2 import adam as adam_v2
-  from keras.optimizer_v2 import adamax as adamax_v2
-  from keras.optimizer_v2 import gradient_descent as gradient_descent_v2
-  from keras.optimizer_v2 import nadam as nadam_v2
-  from keras.optimizer_v2 import rmsprop as rmsprop_v2
+  try:
+    from keras.optimizer_v2 import adadelta as adadelta_v2
+    from keras.optimizer_v2 import adagrad as adagrad_v2
+    from keras.optimizer_v2 import adam as adam_v2
+    from keras.optimizer_v2 import adamax as adamax_v2
+    from keras.optimizer_v2 import gradient_descent as gradient_descent_v2
+    from keras.optimizer_v2 import nadam as nadam_v2
+    from keras.optimizer_v2 import rmsprop as rmsprop_v2
+  except ImportError:
+    from keras.optimizers.legacy import adadelta as adadelta_v2
+    from keras.optimizers.legacy import adagrad as adagrad_v2
+    from keras.optimizers.legacy import adam as adam_v2
+    from keras.optimizers.legacy import adamax as adamax_v2
+    from keras.optimizers.legacy import gradient_descent as gradient_descent_v2
+    from keras.optimizers.legacy import nadam as nadam_v2
+    from keras.optimizers.legacy import rmsprop as rmsprop_v2
 from keras.utils import tf_contextlib
 from keras.utils import tf_inspect
 
 
 def string_test(actual, expected):
   np.testing.assert_array_equal(actual, expected)
 
@@ -656,14 +665,15 @@
     super(_MultiIOSubclassModel, self).__init__(name=name)
     self._shared_input_branch = shared_input_branch
     self._branch_a = branch_a
     self._branch_b = branch_b
     self._shared_output_branch = shared_output_branch
 
   def call(self, inputs, **kwargs):
+    """A dummy docstring."""
     if self._shared_input_branch:
       for layer in self._shared_input_branch:
         inputs = layer(inputs)
       a = inputs
       b = inputs
     elif isinstance(inputs, dict):
       a = inputs['input_1']
@@ -707,14 +717,15 @@
     self._branch_a = self._branch_a_func()
     self._branch_b = self._branch_b_func()
 
     if self._shared_output_branch_func():
       self._shared_output_branch = self._shared_output_branch_func()
 
   def call(self, inputs, **kwargs):
+    """A dummy docstring."""
     if self._shared_input_branch:
       for layer in self._shared_input_branch:
         inputs = layer(inputs)
       a = inputs
       b = inputs
     else:
       a, b = inputs
```

## intel_extension_for_tensorflow/python/test_func/test.py

```diff
@@ -16,34 +16,30 @@
 # ==============================================================================
 
 """Testing."""
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
+import functools
+import sys
 
 # pylint: disable=g-bad-import-order
 import intel_extension_for_tensorflow.python.test_func.test_util as _test_util
 from tensorflow.python.platform import googletest as _googletest
+from tensorflow.python.util.tf_export import tf_export
 
 # pylint: disable=unused-import
 from intel_extension_for_tensorflow.python.test_func.test_util import assert_equal_graph_def
 from intel_extension_for_tensorflow.python.test_func.test_util import create_local_cluster
 from intel_extension_for_tensorflow.python.test_func.test_util import TensorFlowTestCase as TestCase
 from intel_extension_for_tensorflow.python.test_func.test_util import gpu_device_name
 from intel_extension_for_tensorflow.python.test_func.test_util import is_gpu_available
 
-from tensorflow.python.ops.gradient_checker import compute_gradient_error
-from tensorflow.python.ops.gradient_checker import compute_gradient
-# pylint: enable=unused-import,g-bad-import-order
 
-import functools
-
-import sys
-from tensorflow.python.util.tf_export import tf_export
 if sys.version_info.major == 2:
   import mock                # pylint: disable=g-import-not-at-top,unused-import
 else:
   from unittest import mock  # pylint: disable=g-import-not-at-top,g-importing-member
 
 # TODO(itex): Check the effect in commenting this instruction
 # tf_export(v1=['test.mock'])(mock)
@@ -144,15 +140,15 @@
   def decorator_disable_with_predicate(func):
 
     @functools.wraps(func)
     def wrapper_disable_with_predicate(self, *args, **kwargs):
       if pred():
         self.skipTest(skip_message)
       else:
-        return func(self, *args, **kwargs)
+        func(self, *args, **kwargs)
 
     return wrapper_disable_with_predicate
 
   return decorator_disable_with_predicate
 
 
 @tf_export('test.is_built_with_gpu_support')
```

## intel_extension_for_tensorflow/python/test_func/test_util.py

```diff
@@ -14,22 +14,22 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 ################################################################################
 # Device placement API summary:
 # Usually there are 3 ways to set device placment
-# 1) 
+# 1)
 # with self.cached_session(use_gpu=use_gpu, force_gpu=force_gpu) as sess:
 # """ default: use_gpu = True, force_gpu = False
 # force_gpu = True                       -> with ops.device("/device:XPU:0") + allow_soft_placement = False
 # force_gpu = False and use_gpu = True   -> with ops.device("/device:XPU:0") + allow_soft_placement = True
 # force_gpu = False and use_gpu = False  -> with ops.device("/device:CPU:0") + allow_soft_placement = True
-# 
-# 
+#
+#
 # 2) with test_util.device(use_gpu=use_gpu) / with test_util.force_gpu()
 # test_util.device(use_gpu=True) or test_util.force_gpu()   -> with ops.device("/device:XPU:0")
 # test_util.device(use_gpu=False) or test_util.force_cpu()  -> with ops.device("/device:CPU:0")
 #
 # 3) if test.is_gpu_available(): then do some stuff
 # is_gpu_available() = True  <- XPU device + GPU backend
 ################################################################################
@@ -50,18 +50,18 @@
 import os
 import random
 import re
 import tempfile
 import threading
 import time
 import unittest
+import six
 
 from absl.testing import parameterized
 import numpy as np
-import six
 
 from intel_extension_for_tensorflow.python.device import get_backend
 
 from google.protobuf import descriptor_pool
 from google.protobuf import text_format
 
 from tensorflow.core.framework import graph_pb2
@@ -264,14 +264,15 @@
   """
   assert_equal_graph_def(actual, expected, checkpoint_v2,
                          hash_table_shared_name)
 
 
 def assert_equal_graph_def(actual, expected, checkpoint_v2=False,
                            hash_table_shared_name=False):
+  """A dummy docstring."""
   if not isinstance(actual, graph_pb2.GraphDef):
     raise TypeError("Expected tf.GraphDef for actual, got %s" %
                     type(actual).__name__)
   if not isinstance(expected, graph_pb2.GraphDef):
     raise TypeError("Expected tf.GraphDef for expected, got %s" %
                     type(expected).__name__)
 
@@ -330,14 +331,15 @@
 
 # Matches attributes named via _SHARDED_SUFFIX in
 # tensorflow/python/training/saver.py
 _SHARDED_SAVE_OP_PATTERN = "_temp_[0-9a-z]{32}/part"
 
 
 def _strip_checkpoint_v2_randomized(graph_def):
+  """A dummy docstring."""
   for node in graph_def.node:
     delete_keys = []
     for attr_key in node.attr:
       attr_tensor_value = node.attr[attr_key].tensor
       if attr_tensor_value and len(attr_tensor_value.string_val) == 1:
         attr_tensor_string_value = attr_tensor_value.string_val[0]
         if (attr_tensor_string_value and
@@ -401,17 +403,16 @@
     converted tensor or shape array
   """
   # tensor dim -> new axis order
   new_axes = {3: [0, 2, 1], 4: [0, 3, 1, 2], 5: [0, 4, 1, 2, 3]}
   if isinstance(input_tensor, ops.Tensor):
     ndims = input_tensor.shape.ndims
     return array_ops.transpose(input_tensor, new_axes[ndims])
-  else:
-    ndims = len(input_tensor)
-    return [input_tensor[a] for a in new_axes[ndims]]
+  ndims = len(input_tensor)
+  return [input_tensor[a] for a in new_axes[ndims]]
 
 
 def NHWCToNCHW_VECT_C(input_shape_or_tensor):
   """Transforms the input from the NHWC layout to NCHW_VECT_C layout.
 
   Note: Does not include quantization or type conversion steps, which should
   be applied afterwards.
@@ -437,16 +438,15 @@
         "NCHW_VECT_C.")
   temp_shape[-1] //= 4
   temp_shape.append(4)
   permutation = permutations[len(temp_shape)]
   if is_tensor:
     t = array_ops.reshape(input_shape_or_tensor, temp_shape)
     return array_ops.transpose(t, permutation)
-  else:
-    return [temp_shape[a] for a in permutation]
+  return [temp_shape[a] for a in permutation]
 
 
 def NCHW_VECT_CToNHWC(input_shape_or_tensor):
   """Transforms the input from the NCHW_VECT_C layout to NHWC layout.
 
   Note: Does not include de-quantization or type conversion steps, which should
   be applied beforehand.
@@ -469,16 +469,15 @@
     raise ValueError("Last dimension of NCHW_VECT_C must be 4.")
   permutation = permutations[len(input_shape)]
   nhwc_shape = [input_shape[a] for a in permutation[:-1]]
   nhwc_shape[-1] *= input_shape[-1]
   if is_tensor:
     t = array_ops.transpose(input_shape_or_tensor, permutation)
     return array_ops.reshape(t, nhwc_shape)
-  else:
-    return nhwc_shape
+  return nhwc_shape
 
 
 def NCHWToNHWC(input_tensor):
   """Converts the input from the NCHW format to NHWC.
 
   Args:
     input_tensor: a 4- or 5-D tensor, or an array representing shape
@@ -487,17 +486,16 @@
     converted tensor or shape array
   """
   # tensor dim -> new axis order
   new_axes = {4: [0, 2, 3, 1], 5: [0, 2, 3, 4, 1]}
   if isinstance(input_tensor, ops.Tensor):
     ndims = input_tensor.shape.ndims
     return array_ops.transpose(input_tensor, new_axes[ndims])
-  else:
-    ndims = len(input_tensor)
-    return [input_tensor[a] for a in new_axes[ndims]]
+  ndims = len(input_tensor)
+  return [input_tensor[a] for a in new_axes[ndims]]
 
 
 def skip_if(condition):
   """Skips the decorated function if condition is or evaluates to True.
 
   Args:
     condition: Either an expression that can be used in "if not condition"
@@ -707,14 +705,25 @@
   """
 
   def wrap_f(f):
     def decorator(self, *args, **kwargs):
       """Warms up, gets object counts, runs the test, checks for new objects."""
       with context.eager_mode():
         gc.disable()
+        # Python 3.11 removed "errors" and "skipped" as members of
+        # unittest.case._Outcome so get them from the test result object
+        # instead.
+        test_errors = None
+        test_skipped = None
+        if hasattr(self._outcome, "errors"):
+          test_errors = self._outcome.errors
+          test_skipped = self._outcome.skipped
+        else:
+          test_errors = self._outcome.result.errors
+          test_skipped = self._outcome.result.skipped
         # Run the test 2 times as warmup, in an attempt to fill up caches, which
         # should not grow as the test is run repeatedly below.
         #
         # TODO(b/117156879): Running warmup twice is black magic; we have seen
         # tests that fail with 1 warmup run, and pass with 2, on various
         # versions of python2.7.x.
         for _ in range(warmup_iters):
@@ -731,16 +740,15 @@
         # Make sure any registered functions are cleaned up in the C++ runtime.
         registered_function_names = context.context().list_function_names()
 
         # unittest.doCleanups adds to self._outcome with each unwound call.
         # These objects are retained across gc collections so we exclude them
         # from the object count calculation.
         obj_count_by_type = _get_object_count_by_type(
-            exclude=gc.get_referents(self._outcome.errors,
-                                     self._outcome.skipped))
+            exclude=gc.get_referents(test_errors, test_skipped))
 
         if ops.has_default_graph():
           collection_sizes_before = {
               collection: len(ops.get_collection(collection))
               for collection in ops.get_default_graph().collections
           }
         for _ in range(3):
@@ -767,16 +775,15 @@
             del size_before
           del collection_sizes_before
         gc.collect()
 
         # There should be no new Python objects hanging around.
         obj_count_by_type = (
             _get_object_count_by_type(
-                exclude=gc.get_referents(self._outcome.errors,
-                                         self._outcome.skipped)) -
+                exclude=gc.get_referents(test_errors, test_skipped)) -
             obj_count_by_type)
 
         # There should be no newly registered functions hanging around.
         leftover_functions = (
             context.context().list_function_names() - registered_function_names)
         assert not leftover_functions, (
             "The following functions were newly created: %s" %
@@ -790,16 +797,15 @@
             "The following objects were newly created: %s" %
             str(obj_count_by_type))
         gc.enable()
     return decorator
 
   if func is None:
     return wrap_f
-  else:
-    return wrap_f(func)
+  return wrap_f(func)
 
 
 def assert_no_new_tensors(f):
   """Decorator for asserting that no new Tensors persist after a test.
 
   Mainly useful for checking that code using the Python C API has correctly
   manipulated reference counts.
@@ -857,15 +863,15 @@
       )))
     return result
 
   return decorator
 
 
 def _find_reference_cycle(objects, idx):
-
+  """A dummy docstring."""
   def get_ignore_reason(obj, denylist):
     """Tests whether an object should be omitted from the dependency graph."""
     if len(denylist) > 100:
       return "<depth limit>"
     if tf_inspect.isframe(obj):
       if "test_util.py" in tf_inspect.getframeinfo(obj)[0]:
         return "<test code>"
@@ -1396,15 +1402,15 @@
   if func is not None:
     return decorator(func)
 
   return decorator
 
 
 def py_func_if_in_function(f):
-
+  """A dummy docstring."""
   def decorated(*args, **kwds):
     if not ops.inside_function():
       return f(*args, **kwds)
 
     tensor_args = []
     tensor_indices = []
     for i, arg in enumerate(args):
@@ -1828,36 +1834,36 @@
   """
 
   # This was needed earlier when we had support for SYCL in TensorFlow.
   del cuda_only
 
   try:
     for local_device in device_lib.list_local_devices():
-      if local_device.device_type == "XPU" and get_backend() == b"GPU": 
+      if local_device.device_type == "XPU" and get_backend() == b"GPU":
         return True
       if local_device.device_type == "GPU":
         gpu_info = gpu_util.compute_capability_from_device_desc(local_device)
         cc = gpu_info.compute_capability or (0, 0)
         if not min_cuda_compute_capability or cc >= min_cuda_compute_capability:
           return True
     return False
   except errors_impl.NotFoundError as e:
     if not all(x in str(e) for x in ["CUDA", "not find"]):
       raise e
-    else:
-      logging.error(str(e))
-      return False
+    logging.error(str(e))
+    return False
 
 # use_gpu = True will not check is_gpu_available()
 @contextlib.contextmanager
 def device(use_gpu):
   """Uses gpu when requested and available."""
   if use_gpu:
     if get_backend() != b"GPU":
-      logging.info("The test is set use_gpu = True, but you are using INTEL_XPU device with CPU backend")
+      logging.info("The test is set use_gpu = True, but you are using \
+                    INTEL_XPU device with CPU backend")
     dev = "/device:XPU:0"
   else:
     dev = "/device:CPU:0"
   with ops.device(dev):
     yield
 
 
@@ -1867,15 +1873,16 @@
   with device(use_gpu=True):
     yield
 
 
 @contextlib.contextmanager
 def force_gpu():
   if get_backend() != b"GPU":
-    logging.info("The test is set force_gpu = True, but you are using INTEL_XPU device with CPU backend")
+    logging.info("The test is set force_gpu = True, but you are using \
+                  INTEL_XPU device with CPU backend")
   with ops.device("/device:XPU:0"):
     yield
 
 
 @contextlib.contextmanager
 def force_cpu():
   """Force the cpu to be used."""
@@ -1889,28 +1896,28 @@
   try:
     config.enable_op_determinism()
     yield
   finally:
     config.disable_op_determinism()
 
 
-class CapturedWrites(object):
+class CapturedWrites:
   """A utility class to load the captured writes made to a stream."""
 
   def __init__(self, capture_location):
     self.capture_location = capture_location
 
   def contents(self):
     """Get the captured writes as a single string."""
     with open(self.capture_location) as tmp_file:
       output_data = "".join(tmp_file.readlines())
     return output_data
 
 
-class FakeEagerSession(object):
+class FakeEagerSession:
   """Fake session so tests that conditionally use placeholders can use eager.
 
   There are a number of tests that conditionally use placeholders for shape
   inference. The pattern is demonstrated here:
 
   ```python
   with self.cached_session() as sess:
@@ -2000,19 +2007,19 @@
       new_xla_flags = "--xla_gpu_autotune_level=0"
       if original_xla_flags:
         new_xla_flags = original_xla_flags + " " + new_xla_flags
       os.environ["XLA_FLAGS"] = new_xla_flags
 
       result = f(self, *args, **kwargs)
 
-      if (original_tf_cudnn_use_autotune is None):
+      if original_tf_cudnn_use_autotune is None:
         del os.environ["TF_CUDNN_USE_AUTOTUNE"]
       else:
         os.environ["TF_CUDNN_USE_AUTOTUNE"] = original_tf_cudnn_use_autotune
-      if (original_xla_flags is None):
+      if original_xla_flags is None:
         del os.environ["XLA_FLAGS"]
       else:
         os.environ["XLA_FLAGS"] = original_xla_flags
 
       return result
 
     return decorated
@@ -2021,15 +2028,15 @@
     return decorator(func)
 
   return decorator
 
 
 # The description is just for documentation purposes.
 def enable_tf_xla_constant_folding(description):
-
+  """A dummy docstring."""
   if not isinstance(description, str):
     raise ValueError("'description' should be string, got {}".format(
         type(description)))
 
   def enable_tf_xla_constant_folding_impl(func):
     """Enable constant folding during the call to this function.
 
@@ -2059,20 +2066,21 @@
     return decorator
 
   return enable_tf_xla_constant_folding_impl
 
 
 # Updates test function by selectively disabling it.
 def _disable_test(execute_func):
-
+  """A dummy docstring."""
   def disable_test_impl(func):
 
     def decorator(func):
 
       def decorated(self, *args, **kwargs):
+        """A dummy docstring."""
         if execute_func:
           return func(self, *args, **kwargs)
 
       return tf_decorator.make_decorator(func, decorated)
 
     if func is not None:
       return decorator(func)
@@ -2122,31 +2130,29 @@
   """Execute the test method only if UBSAN is not enabled."""
   execute_func = not is_ubsan_enabled()
   return _disable_test(execute_func)
 
 
 # The description is just for documentation purposes.
 def disable_tfrt(unused_description):
-
+  """A dummy docstring."""
   def disable_tfrt_impl(cls_or_func):
     """Execute the test only if tfrt is not enabled."""
 
     if tf_inspect.isclass(cls_or_func):
       if tfrt_utils.enabled():
         return None
-      else:
-        return cls_or_func
+      return cls_or_func
     else:
       def decorator(func):
 
         def decorated(self, *args, **kwargs):
           if tfrt_utils.enabled():
             return
-          else:
-            return func(self, *args, **kwargs)
+          return func(self, *args, **kwargs)
 
         return decorated
 
       if cls_or_func is not None:
         return decorator(cls_or_func)
 
       return decorator
@@ -2186,31 +2192,30 @@
   """This test is not intended to be run with XLA auto jit enabled."""
   execute_func = not is_xla_enabled()
   return _disable_test(execute_func)
 
 
 # The description is just for documentation purposes.
 def xla_allow_fallback(description):  # pylint: disable=unused-argument
-
+  """A dummy docstring."""
   def xla_allow_fallback_impl(func):
     """Allow fallback to TF even though testing xla."""
 
     def decorator(func):
 
       def decorated(self, *args, **kwargs):
         if is_xla_enabled():
           # Update the global XLABuildOpsPassFlags to enable lazy compilation,
           # which allows the compiler to fall back to TF classic. Remember the
           # old value so that we can reset it.
           old_value = pywrap_tf_session.TF_SetXlaEnableLazyCompilation(True)
           result = func(self, *args, **kwargs)
           pywrap_tf_session.TF_SetXlaEnableLazyCompilation(old_value)
           return result
-        else:
-          return func(self, *args, **kwargs)
+        return func(self, *args, **kwargs)
 
       return decorated
 
     if func is not None:
       return decorator(func)
 
     return decorator
@@ -2287,19 +2292,18 @@
     ret = math_ops.matmul(a, b, *args, **kwargs)
     return math_ops.cast(ret, a.dtype)
   elif config.tensor_float_32_execution_enabled() and a.dtype == "complex64":
     a = math_ops.cast(a, "complex128")
     b = math_ops.cast(b, "complex128")
     ret = math_ops.matmul(a, b, *args, **kwargs)
     return math_ops.cast(ret, a.dtype)
-  else:
-    return math_ops.matmul(a, b, *args, **kwargs)
+  return math_ops.matmul(a, b, *args, **kwargs)
 
 
-class EagerSessionWarner(object):
+class EagerSessionWarner:
 
   def __getattr__(self, attr):
     raise AttributeError(
         "Trying to access properties or call methods on the result of "
         "self.session(), self.cached_session(), etc while eager execution "
         "is enabled. If you're porting this test case to TF 2.0, either "
         "adapt the test to work with eager execution or insert a call to "
@@ -2338,14 +2342,15 @@
     self._test_start_time = None
     # This flag provides the ability to control whether the graph mode gets
     # initialized for TF1 or not. Initializing for TF1, which is what was
     # happening earlier, was preventing enablement of 'eager mode' in the test.
     self._set_default_seed = True
 
   def setUp(self):
+    """A dummy docstring."""
     super(TensorFlowTestCase, self).setUp()
     self._ClearCachedSession()
     random.seed(random_seed.DEFAULT_GRAPH_SEED)
     np.random.seed(random_seed.DEFAULT_GRAPH_SEED)
     # Note: The following line is necessary because some test methods may error
     # out from within nested graph contexts (e.g., via assertRaises and
     # assertRaisesRegexp), which may leave ops._default_graph_stack non-empty
@@ -2364,14 +2369,15 @@
     # Avoiding calling setUp() for the poorly named test_session method.
     if self.id().endswith(".test_session"):
       self.skipTest("Not a test.")
 
     self._test_start_time = time.time()
 
   def tearDown(self):
+    """A dummy docstring."""
     # If a subclass overrides setUp and doesn't call the parent class's setUp,
     # then we may not have set the start time.
     if self._test_start_time is not None:
       logging.info("time(%s): %ss", self.id(),
                    round(time.time() - self._test_start_time, 2))
 
     for thread in self._threads:
@@ -2460,15 +2466,16 @@
       a: a proto.
       b: another proto.
       msg: Optional message to report on failure.
     """
     if not compare.ProtoEq(a, b):
       compare.assertProtoEqual(self, a, b, normalize_numbers=True, msg=msg)
 
-  def assertProtoEquals(self, expected_message_maybe_ascii, message, msg=None):
+  def assertProtoEquals(self,
+                          expected_message_maybe_ascii, message, msg=None):
     """Asserts that message is same as parsed expected_message_ascii.
 
     Creates another prototype of message, reads the ascii message into it and
     then compares them using self._AssertProtoEqual().
 
     Args:
       expected_message_maybe_ascii: proto message in original or ascii form.
@@ -2510,14 +2517,15 @@
     """
     if not actual.startswith(expected_start):
       fail_msg = "%r does not start with %r" % (actual, expected_start)
       fail_msg += " : %r" % (msg) if msg else ""
       self.fail(fail_msg)
 
   def _eval_tensor(self, tensor):
+    """A dummy docstring."""
     if tensor is None:
       return None
     elif callable(tensor):
       return self._eval_helper(tensor())
     else:
       try:
         if sparse_tensor.is_sparse(tensor):
@@ -2556,16 +2564,15 @@
     if context.executing_eagerly():
       return self._eval_helper(tensors)
     else:
       sess = ops.get_default_session()
       if sess is None:
         with self.test_session() as sess:
           return sess.run(tensors)
-      else:
-        return sess.run(tensors)
+      return sess.run(tensors)
 
   # pylint: disable=g-doc-return-or-yield
   @contextlib.contextmanager
   def session(self, graph=None, config=None, use_gpu=True, force_gpu=False):
     """A context manager for a TensorFlow Session for use in executing tests.
 
     Note that this will set this session and the graph as global defaults.
@@ -2686,15 +2693,15 @@
           yield cached
       else:
         with self.session(graph, config, use_gpu, force_gpu) as sess:
           yield sess
 
   # pylint: enable=g-doc-return-or-yield
 
-  class _CheckedThread(object):
+  class _CheckedThread:
     """A wrapper class for Thread that asserts successful completion.
 
     This class should be created using the TensorFlowTestCase.checkedThread()
     method.
     """
 
     def __init__(self, testcase, target, args=None, kwargs=None):
@@ -2859,18 +2866,18 @@
     return a
 
   def evaluate_if_both_tensors(self, a, b):
     if (tensor_util.is_tf_type(a) and tensor_util.is_tf_type(b) and
         not isinstance(a, ops._EagerTensorBase) and
         not isinstance(b, ops._EagerTensorBase)):
       return self.evaluate((a, b))
-    else:
-      return (a, b)
+    return (a, b)
 
   def _assertArrayLikeAllClose(self, a, b, rtol=1e-6, atol=1e-6, msg=None):
+    """A dummy docstring."""
     (a, b) = self.evaluate_if_both_tensors(a, b)
     a = self._GetNdArray(a)
     b = self._GetNdArray(b)
     # When the array rank is small, print its contents. Numpy array printing is
     # implemented using inefficient recursion so prints can cause tests to
     # time out.
     if a.shape != b.shape and (b.ndim <= 3 or b.size < 500):
@@ -2915,20 +2922,21 @@
       # TODO(xpan): There seems to be a bug:
       # tensorflow/compiler/tests:binary_ops_test pass with float32
       # nan even though the equal_nan is False by default internally.
       np.testing.assert_allclose(
           a, b, rtol=rtol, atol=atol, err_msg="\n".join(msgs), equal_nan=True)
 
   def _assertAllCloseRecursive(self,
-                               a,
-                               b,
-                               rtol=1e-6,
-                               atol=1e-6,
-                               path=None,
-                               msg=None):
+                                  a,
+                                  b,
+                                  rtol=1e-6,
+                                  atol=1e-6,
+                                  path=None,
+                                  msg=None):
+    """A dummy docstring."""
     path = path or []
     path_str = (("[" + "][".join(str(p) for p in path) + "]") if path else "")
     msg = msg if msg else ""
 
     # Check if a and/or b are namedtuples.
     if hasattr(a, "_asdict"):
       a = a._asdict()
@@ -3022,25 +3030,25 @@
     """
     if ragged_tensor.is_ragged(a) or ragged_tensor.is_ragged(b):
       return self._assertRaggedClose(a, b, rtol, atol, msg)
     self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
 
   @py_func_if_in_function
   def assertAllCloseAccordingToType(self,
-                                    a,
-                                    b,
-                                    rtol=1e-6,
-                                    atol=1e-6,
-                                    float_rtol=1e-6,
-                                    float_atol=1e-6,
-                                    half_rtol=1e-3,
-                                    half_atol=1e-3,
-                                    bfloat16_rtol=1e-2,
-                                    bfloat16_atol=1e-2,
-                                    msg=None):
+                                         a,
+                                         b,
+                                         rtol=1e-6,
+                                         atol=1e-6,
+                                         float_rtol=1e-6,
+                                         float_atol=1e-6,
+                                         half_rtol=1e-3,
+                                         half_atol=1e-3,
+                                         bfloat16_rtol=1e-2,
+                                         bfloat16_atol=1e-2,
+                                         msg=None):
     """Like assertAllClose, but also suitable for comparing fp16 arrays.
 
     In particular, the tolerance is reduced to 1e-3 if at least
     one of the arguments is of type float16.
 
     Args:
       a: the expected numpy ndarray or anything can be converted to one.
@@ -3262,19 +3270,19 @@
                    str(value[tuple(subscript)]))
     if len(subscripts) > limit:
       lines.append(prefix + "...")
     return lines
 
   @py_func_if_in_function
   def assertAllInRange(self,
-                       target,
-                       lower_bound,
-                       upper_bound,
-                       open_lower_bound=False,
-                       open_upper_bound=False):
+                          target,
+                          lower_bound,
+                          upper_bound,
+                          open_lower_bound=False,
+                          open_upper_bound=False):
     """Assert that elements in a Tensor are all in a given range.
 
     Args:
       target: The numpy `ndarray`, or anything that can be converted into a
         numpy `ndarray` (including Tensor).
       lower_bound: lower bound of the range
       upper_bound: upper bound of the range
@@ -3481,15 +3489,15 @@
       b_ragged_rank = b.ragged_rank if ragged_tensor.is_ragged(b) else 0
       self.assertEqual(a_ragged_rank, b_ragged_rank, msg)
 
   def _assertListCloseRecursive(self, a, b, rtol, atol, msg, path="value"):
     self.assertEqual(type(a), type(b))
     if isinstance(a, (list, tuple)):
       self.assertLen(a, len(b), "Length differs for %s" % path)
-      for i in range(len(a)):
+      for i in enumerate(a):
         self._assertListCloseRecursive(a[i], b[i], rtol, atol, msg,
                                        "%s[%s]" % (path, i))
     else:
       self._assertAllCloseRecursive(a, b, rtol, atol, path, msg)
 
   # Fix Python 3+ compatibility issues
   if not six.PY2:
@@ -3508,15 +3516,16 @@
     """Set the session and its graph to global default and constrain devices."""
     if context.executing_eagerly():
       yield None
     else:
       with sess.graph.as_default(), sess.as_default():
         if force_gpu or use_gpu:
           if get_backend() != b"GPU":
-            logging.info("The test is set use_gpu = True, but you are using INTEL_XPU device with CPU backend")
+            logging.info("The test is set use_gpu = True, but you are using \
+                          INTEL_XPU device with CPU backend")
         if force_gpu:
           # Use the name of an actual device if one is detected, or
           # '/device:XPU:0' otherwise
           gpu_name = gpu_device_name()
           if not gpu_name:
             gpu_name = "/device:XPU:0"
           with sess.graph.device(gpu_name):
@@ -3726,14 +3735,15 @@
   This is useful to test tf.gradients() in tests that uses tf.GradientTape().
 
   Yields:
     gradient tape instance that's implemented by tf.gradients() underneath.
   """
   try:
     class FakeGradientTape:
+      """A dummy docstring."""
 
       def watch(self, x):
         pass
 
       def gradient(self, y, x, grad_ys=None):
         result = gradients_impl.gradients(y, x, grad_ys)
```

## Comparing `intel_extension_for_tensorflow-1.1.0.dist-info/LICENSE.txt` & `intel_extension_for_tensorflow-1.2.0rc0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `intel_extension_for_tensorflow-1.1.0.dist-info/METADATA` & `intel_extension_for_tensorflow-1.2.0rc0.dist-info/METADATA`

 * *Files 7% similar despite different names*

```diff
@@ -1,66 +1,66 @@
 Metadata-Version: 2.1
 Name: intel-extension-for-tensorflow
-Version: 1.1.0
+Version: 1.2.0rc0
 Summary: Intel Extension for Tensorflow*
 Home-page: https://github.com/intel/intel-extension-for-tensorflow
 Download-URL: https://github.com/intel/intel-extension-for-tensorflow/tags
 Author: Intel Corporation
 Author-email: itex.maintainers@intel.com
 License: Apache 2.0
 Project-URL: Bug Tracker, https://github.com/intel/intel-extension-for-tensorflow/issues
 Keywords: Intel Extension for Tensorflow*
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Education
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Classifier: Topic :: Scientific/Engineering
 Classifier: Topic :: Scientific/Engineering :: Mathematics
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Software Development
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
-Requires-Python: >=3.7
+Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE.txt
 Requires-Dist: grpcio (>=1.8.6)
 Requires-Dist: wheel
-Requires-Dist: tensorflow (>=2.10)
+Requires-Dist: tensorflow (>=2.12)
 Requires-Dist: numpy (<1.24)
 Provides-Extra: cpu
-Requires-Dist: intel-extension-for-tensorflow-lib (==1.1.0.0) ; extra == 'cpu'
+Requires-Dist: intel-extension-for-tensorflow-lib (==1.2.0.0rc0) ; extra == 'cpu'
 Provides-Extra: gpu
-Requires-Dist: intel-extension-for-tensorflow-lib (==1.1.0.1) ; extra == 'gpu'
+Requires-Dist: intel-extension-for-tensorflow-lib (==1.2.0.1rc0) ; extra == 'gpu'
 
 # Intel Extension for TensorFlow*
 
 [![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://pypi.org/project/intel-extension-for-tensorflow)
 [![version](https://img.shields.io/badge/release-1.1.0-green)](https://github.com/intel/intel-extension-for-tensorflow/releases)
 
 Intel Extension for TensorFlow* is a heterogeneous, high performance deep learning extension plugin based on TensorFlow [PluggableDevice](https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md) interface to bring Intel XPU(GPU, CPU, etc) devices into [TensorFlow](https://github.com/tensorflow/tensorflow) open source community for AI workload acceleration. It allows flexibly plugging an XPU into TensorFlow on-demand, and exposing computing power inside Intel's hardware.
 
 Documentation: [**Intel Extension for TensorFlow\* online document website**](https://intel.github.io/intel-extension-for-tensorflow/).
 
 ## Installation
 
 ### Install for GPU
 ```
-pip install tensorflow==2.11.0    # Supported version == 2.11.0 or 2.10.0
-pip install --upgrade intel-extension-for-tensorflow[gpu]
+pip install tensorflow==2.12.0
+pip install --upgrade intel-extension-for-tensorflow[gpu]==1.2.0rc0
 ```
 Please refer to [GPU installation](https://intel.github.io/intel-extension-for-tensorflow/latest/docs/install/install_for_gpu.html) for details.
 
 ### Install for CPU [Experimental]
 ```
-pip install tensorflow==2.11.0    # Supported version == 2.11.0 or 2.10.0
-pip install --upgrade intel-extension-for-tensorflow[cpu]
+pip install tensorflow==2.12.0
+pip install --upgrade intel-extension-for-tensorflow[cpu]==1.2.0rc0
 ```
 
 ## Security
 See Intel's [Security Center](https://www.intel.com/content/www/us/en/security-center/default.html) for information on how to report a potential security issue or vulnerability.
 
 See also: [Security Policy](https://intel.github.io/intel-extension-for-tensorflow/latest/SECURITY.html)
```

## Comparing `intel_extension_for_tensorflow-1.1.0.dist-info/RECORD` & `intel_extension_for_tensorflow-1.2.0rc0.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,28 +1,32 @@
-intel_extension_for_tensorflow/__init__.py,sha256=VACAdjdUSNBGNiL8Hz-Yeu37F28nVBLWwO6F1-dn_0g,1594
+intel_extension_for_tensorflow/__init__.py,sha256=HO5tjJj9IHVU7pbzu7cLTz6vS18FoJnS14fP5kfunJ8,1898
+intel_extension_for_tensorflow/__main__.py,sha256=VY9K7rQScqRvOS_nIs4DAj4FPKB6YEefzcxJqlwW6A0,243
 intel_extension_for_tensorflow/core/utils/protobuf/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py,sha256=lxC6232fwHMZvlzVxsii24kyx09f79PIQgr_6arDwio,16884
+intel_extension_for_tensorflow/core/utils/protobuf/config_pb2.py,sha256=nfE_Z9atjkGIeQFIvB4UZoTDyHMvV3g3rNxg9H-wrLI,3575
 intel_extension_for_tensorflow/python/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-intel_extension_for_tensorflow/python/device.py,sha256=peSwZfxkS-L-jtxhD2CYmZAKjcxs1m4tSEJl5VUVkmo,1593
-intel_extension_for_tensorflow/python/launch.py,sha256=pdnoFY2FQ2bod0YEAD86YthrdwwrGb7MAILIRZK5gP4,29820
-intel_extension_for_tensorflow/python/version.py,sha256=N4VoqYNnaK1tVUel4pnhGwxN57xEGRwLqVhThATPSwA,1091
+intel_extension_for_tensorflow/python/config.py,sha256=q3CHAtRHOkwSE0L_LAteGj98wxYbUOQrqEokAiVCHk8,1403
+intel_extension_for_tensorflow/python/device.py,sha256=lcHDdGA2N6hDwloSfJZLkySy4_GSxY7fJs-4kh9yz78,1214
+intel_extension_for_tensorflow/python/experimental_ops_override.py,sha256=1VvimSIDwVYKttJgIpXrWnWPep7kFlB_1ZKBYHijI10,26593
+intel_extension_for_tensorflow/python/gen_itex_version.py,sha256=s9Lqd7_Jhg76y6_sp1K2a4iQeUGcs-r3_TingzA-FzI,2190
+intel_extension_for_tensorflow/python/launch.py,sha256=LgPe4WAu9iSv5C13yugQPktp2MsNp3RvKtC7kG6bBao,31025
+intel_extension_for_tensorflow/python/version.py,sha256=4kESuBfZoiTOFggr_THilTz6vIVMIcXF_fhhUnJXBYs,1028
 intel_extension_for_tensorflow/python/ops/__init__.py,sha256=hI7KF7Xy5e39_h56lEMOSz_JJbR7UDx_EV5J037_oA8,1169
-intel_extension_for_tensorflow/python/ops/activations.py,sha256=UJLKmH-XR7IJL8hCh-CuDfO8PvLnY1-gQ8k1B6Dgo9o,2526
-intel_extension_for_tensorflow/python/ops/layer_norm.py,sha256=dHjmqm0TyP6I67YllDZHF_THXJJID8Jy4byTm59wMmo,17836
+intel_extension_for_tensorflow/python/ops/activations.py,sha256=Sll7242juaaP77YoDamXV2_fa_wOQVflLZ4V1UU8aZ0,2531
+intel_extension_for_tensorflow/python/ops/layer_norm.py,sha256=KhoKgq-PXAGRyeoFfyNcdMT1q94YM12tL6Dwl_V8xFQ,17867
 intel_extension_for_tensorflow/python/ops/load_ops_library.py,sha256=HAWSWdLGnpUhdUX5zqoZXApM3lPPY4luWIdQRz1Kt38,2668
-intel_extension_for_tensorflow/python/ops/ops_grad.py,sha256=R0BxAwp12BA00Bk583F_Q_EIuTUSoPXY-yO_-N07tbs,2593
-intel_extension_for_tensorflow/python/ops/optimizers.py,sha256=37lM3rh6HQJmkI_gg9CmmVo69ds6PB1KuUkFxcMR50g,10620
-intel_extension_for_tensorflow/python/ops/recurrent.py,sha256=Gan2ykiETKkZYju8TAgJPLfRjiET9K9Z6YSHA-LORl0,28127
-intel_extension_for_tensorflow/python/test_func/__init__.py,sha256=fNfH9_ZhAG8k0xnxeJe0fpe-Y8wTNbDJu2ZSu5alXGE,1017
-intel_extension_for_tensorflow/python/test_func/eager_test.py,sha256=nrbaej7jM_h_AYLmVh2ctSHKYOqPjLX4wJpG2QEFiRM,1117
-intel_extension_for_tensorflow/python/test_func/keras_parameterized.py,sha256=f96VqutZhYSf7wOhH_BoQJraMO2FVD3AMLVVb_4aUlQ,17631
-intel_extension_for_tensorflow/python/test_func/keras_testing_utils.py,sha256=ezUKdXpgGuVJ9t8BxW4vFnI7mBQzwQKlLlrooSwROb0,37515
-intel_extension_for_tensorflow/python/test_func/test.py,sha256=XJ5JzDoitEabdymqmy72fR2yGSykFKS4sx3YSEd9Ozo,6545
-intel_extension_for_tensorflow/python/test_func/test_util.py,sha256=Ei4ugMIfT2LbXgsD-2b2-bEm_-oaCxyyDWVMXZPXPGA,134568
+intel_extension_for_tensorflow/python/ops/ops_grad.py,sha256=XoDuTCKMFdCxw1B3LyQwMw1mxSt2Ose0MqkHlmVhoX8,3451
+intel_extension_for_tensorflow/python/ops/optimizers.py,sha256=TFVrhflnVM1oIOCe4DRwN_lCVr7zMeah7QacktquYW4,10707
+intel_extension_for_tensorflow/python/ops/recurrent.py,sha256=IXF1uG-Ip_46y2IBThNrXOL_BESZhYU1JYkazGtOUkk,28487
+intel_extension_for_tensorflow/python/test_func/__init__.py,sha256=eUT0rwTDAVlL6uaKzP2CFwyLcF9IsdyHcLRYuySiy0k,1038
+intel_extension_for_tensorflow/python/test_func/eager_test.py,sha256=O1hsCIPlYaJdSM5VBk7Dy9lYVh96IZMxhKNGUrjbQcs,1116
+intel_extension_for_tensorflow/python/test_func/keras_parameterized.py,sha256=RUlfk7OO2JlZl7fOYQCpZzQQ_pA-juQ2rDPIxMnKLo0,17632
+intel_extension_for_tensorflow/python/test_func/keras_testing_utils.py,sha256=NI6rCfgafWETp-8LxCENb504zHIbgFbg8j1CIIQpjOs,38058
+intel_extension_for_tensorflow/python/test_func/test.py,sha256=EC1dGZZ5Xr_8PnMU5FzHhDYFcsA3U2-oWk63NVmg59c,6345
+intel_extension_for_tensorflow/python/test_func/test_util.py,sha256=WKc-z-hCEsNk_FiI2vc_H-Qx7oJvuSEmkGlZ1T20Tb8,135335
 intel_extension_for_tensorflow/third-party-programs/THIRD-PARTY-PROGRAMS,sha256=38wrUdgRWcBxKeftLvRLlSSrFsbXbY91XNeA1Ln8sz8,81897
 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onednn.txt,sha256=0YEJW0dBbtk9OTCDxdI_p8Li3WTYdHSkKsXA5_5-CaM,28478
 intel_extension_for_tensorflow/third-party-programs/third-party-programs-of-onevpl.txt,sha256=i34dYM774UHk35qAHdW2rbqF3370_zixaOb3bxc5TWs,21801
-intel_extension_for_tensorflow-1.1.0.dist-info/LICENSE.txt,sha256=fAUT-S_f-DGBz-dvO3RJVmMav-KLvB9io_1yKXuR5po,10775
-intel_extension_for_tensorflow-1.1.0.dist-info/METADATA,sha256=uKCYbwEgBkUT4PwX7GVSVoZ-cyLfjTCgmVx4ND2Hamw,3447
-intel_extension_for_tensorflow-1.1.0.dist-info/WHEEL,sha256=_Tzqjbl1t_2pj9gfZPvG1ppyvaOyoxUa053nfh40rE8,103
-intel_extension_for_tensorflow-1.1.0.dist-info/top_level.txt,sha256=0DYGIzJhxYAd-U188jctMBN6lvmX-0gKs1TfZLd_gz0,31
-intel_extension_for_tensorflow-1.1.0.dist-info/RECORD,,
+intel_extension_for_tensorflow-1.2.0rc0.dist-info/LICENSE.txt,sha256=fAUT-S_f-DGBz-dvO3RJVmMav-KLvB9io_1yKXuR5po,10775
+intel_extension_for_tensorflow-1.2.0rc0.dist-info/METADATA,sha256=Culk_Fo9Tun3dAyuffonxXU8R_-s2Fq2H8F5OqfeiHo,3391
+intel_extension_for_tensorflow-1.2.0rc0.dist-info/WHEEL,sha256=CAuuk_9zz9TklvSaKKuyDpVMPSzU9xWV6t1vK7KC7-A,103
+intel_extension_for_tensorflow-1.2.0rc0.dist-info/top_level.txt,sha256=0DYGIzJhxYAd-U188jctMBN6lvmX-0gKs1TfZLd_gz0,31
+intel_extension_for_tensorflow-1.2.0rc0.dist-info/RECORD,,
```

