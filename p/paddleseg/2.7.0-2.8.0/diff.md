# Comparing `tmp/paddleseg-2.7.0-py3-none-any.whl.zip` & `tmp/paddleseg-2.8.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,156 +1,177 @@
-Zip file size: 349408 bytes, number of entries: 154
--rw-rw-r--  2.0 unx      676 b- defN 22-Nov-30 11:15 paddleseg/__init__.py
+Zip file size: 424336 bytes, number of entries: 175
+-rw-rw-r--  2.0 unx      688 b- defN 23-Apr-11 13:34 paddleseg/__init__.py
 -rwxrwxrwx  2.0 unx      754 b- defN 22-Jan-10 03:05 paddleseg/core/__init__.py
--rw-rw-r--  2.0 unx     8714 b- defN 22-Nov-01 02:56 paddleseg/core/infer.py
--rw-rw-r--  2.0 unx     5768 b- defN 22-Sep-23 07:32 paddleseg/core/predict.py
--rw-rw-r--  2.0 unx    14602 b- defN 22-Nov-28 09:25 paddleseg/core/train.py
--rw-rw-r--  2.0 unx    10978 b- defN 22-Nov-30 11:14 paddleseg/core/val.py
--rw-rw-r--  2.0 unx      684 b- defN 22-Sep-23 07:32 paddleseg/cvlibs/__init__.py
--rwxrwxrwx  2.0 unx     8773 b- defN 22-Jan-10 03:05 paddleseg/cvlibs/callbacks.py
--rw-rw-r--  2.0 unx    21419 b- defN 22-Oct-18 02:55 paddleseg/cvlibs/config.py
--rw-rw-r--  2.0 unx     4735 b- defN 22-Sep-23 07:32 paddleseg/cvlibs/manager.py
--rw-rw-r--  2.0 unx     3370 b- defN 22-Nov-30 10:02 paddleseg/cvlibs/optimizers.py
--rw-rw-r--  2.0 unx     5161 b- defN 22-Oct-18 02:55 paddleseg/cvlibs/param_init.py
--rw-rw-r--  2.0 unx     1153 b- defN 22-Sep-23 07:32 paddleseg/datasets/__init__.py
--rw-rw-r--  2.0 unx     4776 b- defN 22-Sep-23 07:32 paddleseg/datasets/ade.py
--rw-rw-r--  2.0 unx     4030 b- defN 22-Sep-23 07:32 paddleseg/datasets/chase_db1.py
--rw-rw-r--  2.0 unx     3083 b- defN 22-Sep-23 07:32 paddleseg/datasets/cityscapes.py
--rw-rw-r--  2.0 unx     2817 b- defN 22-Sep-23 07:32 paddleseg/datasets/cocostuff.py
--rw-rw-r--  2.0 unx     6886 b- defN 22-Oct-18 02:55 paddleseg/datasets/dataset.py
--rw-rw-r--  2.0 unx     3939 b- defN 22-Sep-23 07:32 paddleseg/datasets/drive.py
--rw-rw-r--  2.0 unx     5196 b- defN 22-Sep-23 07:32 paddleseg/datasets/eg1800.py
--rw-rw-r--  2.0 unx     3944 b- defN 22-Sep-23 07:32 paddleseg/datasets/hrf.py
--rw-rw-r--  2.0 unx     3803 b- defN 22-Sep-23 07:32 paddleseg/datasets/mini_deep_globe_road_extraction.py
--rw-rw-r--  2.0 unx     3812 b- defN 22-Sep-23 07:32 paddleseg/datasets/optic_disc_seg.py
--rw-rw-r--  2.0 unx     3492 b- defN 22-Oct-18 02:55 paddleseg/datasets/pascal_context.py
--rw-rw-r--  2.0 unx     3416 b- defN 22-Sep-23 07:32 paddleseg/datasets/pp_humanseg14k.py
--rw-rw-r--  2.0 unx     5265 b- defN 22-Oct-18 02:55 paddleseg/datasets/pssl.py
--rw-rw-r--  2.0 unx     3739 b- defN 22-Sep-23 07:32 paddleseg/datasets/stare.py
--rw-rw-r--  2.0 unx     5273 b- defN 22-Sep-23 07:32 paddleseg/datasets/supervisely.py
--rw-rw-r--  2.0 unx     4956 b- defN 22-Oct-18 02:55 paddleseg/datasets/voc.py
--rw-rw-r--  2.0 unx     2119 b- defN 22-Nov-30 11:14 paddleseg/models/__init__.py
--rwxrwxrwx  2.0 unx      507 b- defN 21-Nov-10 09:17 paddleseg/models/_test.py
--rw-rw-r--  2.0 unx    15779 b- defN 22-Sep-23 07:32 paddleseg/models/ann.py
--rw-rw-r--  2.0 unx     6499 b- defN 22-Oct-18 02:55 paddleseg/models/attention_unet.py
--rw-rw-r--  2.0 unx     1688 b- defN 22-Nov-30 09:12 paddleseg/models/base_model.py
--rw-rw-r--  2.0 unx    10259 b- defN 22-Oct-18 02:55 paddleseg/models/bisenet.py
--rw-rw-r--  2.0 unx     8133 b- defN 22-Sep-23 07:32 paddleseg/models/bisenetv1.py
--rw-rw-r--  2.0 unx     6942 b- defN 22-Sep-23 07:32 paddleseg/models/ccnet.py
--rw-rw-r--  2.0 unx     7367 b- defN 22-Sep-23 07:32 paddleseg/models/danet.py
--rw-rw-r--  2.0 unx    14345 b- defN 22-Sep-23 07:32 paddleseg/models/ddrnet.py
--rw-rw-r--  2.0 unx     9203 b- defN 22-Sep-23 07:32 paddleseg/models/decoupled_segnet.py
--rw-rw-r--  2.0 unx    10602 b- defN 22-Sep-23 07:32 paddleseg/models/deeplab.py
--rw-rw-r--  2.0 unx     5457 b- defN 22-Oct-25 06:47 paddleseg/models/dmnet.py
--rw-rw-r--  2.0 unx     9002 b- defN 22-Sep-23 07:32 paddleseg/models/dnlnet.py
--rw-rw-r--  2.0 unx     8326 b- defN 22-Oct-25 06:47 paddleseg/models/emanet.py
--rw-rw-r--  2.0 unx     7954 b- defN 22-Sep-23 07:32 paddleseg/models/encnet.py
--rw-rw-r--  2.0 unx    21755 b- defN 22-Oct-18 02:55 paddleseg/models/enet.py
--rw-rw-r--  2.0 unx    16979 b- defN 22-Sep-23 07:32 paddleseg/models/espnet.py
--rw-rw-r--  2.0 unx     9993 b- defN 22-Sep-23 07:32 paddleseg/models/espnetv1.py
--rw-rw-r--  2.0 unx    11996 b- defN 22-Oct-18 02:55 paddleseg/models/fast_scnn.py
--rw-rw-r--  2.0 unx     8380 b- defN 22-Sep-23 07:32 paddleseg/models/fastfcn.py
--rw-rw-r--  2.0 unx     5150 b- defN 22-Sep-23 07:32 paddleseg/models/fcn.py
--rw-rw-r--  2.0 unx     7998 b- defN 22-Sep-23 07:32 paddleseg/models/gcnet.py
--rw-rw-r--  2.0 unx    10388 b- defN 22-Sep-23 07:32 paddleseg/models/ginet.py
--rw-rw-r--  2.0 unx     7201 b- defN 22-Sep-23 07:32 paddleseg/models/glore.py
--rw-rw-r--  2.0 unx    13216 b- defN 22-Sep-23 07:32 paddleseg/models/gscnn.py
--rw-rw-r--  2.0 unx    11089 b- defN 22-Oct-18 02:55 paddleseg/models/hardnet.py
--rw-rw-r--  2.0 unx     4630 b- defN 22-Sep-23 07:32 paddleseg/models/hrnet_contrast.py
--rw-rw-r--  2.0 unx     7724 b- defN 22-Sep-23 07:32 paddleseg/models/isanet.py
--rw-rw-r--  2.0 unx     6385 b- defN 22-Sep-23 07:32 paddleseg/models/lraspp.py
--rw-rw-r--  2.0 unx     7555 b- defN 22-Sep-23 07:32 paddleseg/models/mla_transformer.py
--rw-rw-r--  2.0 unx    11412 b- defN 22-Sep-23 07:32 paddleseg/models/mobileseg.py
--rw-rw-r--  2.0 unx    13220 b- defN 22-Oct-18 02:55 paddleseg/models/mscale_ocrnet.py
--rw-rw-r--  2.0 unx     9290 b- defN 22-Sep-23 07:32 paddleseg/models/ocrnet.py
--rw-rw-r--  2.0 unx     7847 b- defN 22-Sep-23 07:32 paddleseg/models/pfpnnet.py
--rw-rw-r--  2.0 unx    35884 b- defN 22-Sep-23 07:32 paddleseg/models/pointrend.py
--rw-rw-r--  2.0 unx     7230 b- defN 22-Nov-01 02:56 paddleseg/models/portraitnet.py
--rw-rw-r--  2.0 unx    10544 b- defN 22-Oct-08 07:02 paddleseg/models/pp_liteseg.py
--rw-rw-r--  2.0 unx     7874 b- defN 22-Oct-18 02:55 paddleseg/models/pphumanseg_lite.py
--rw-rw-r--  2.0 unx     6018 b- defN 22-Oct-08 06:57 paddleseg/models/pspnet.py
--rw-rw-r--  2.0 unx    27207 b- defN 22-Nov-01 02:56 paddleseg/models/rtformer.py
--rw-rw-r--  2.0 unx     4345 b- defN 22-Oct-18 02:55 paddleseg/models/segformer.py
--rw-rw-r--  2.0 unx     9075 b- defN 22-Sep-23 07:32 paddleseg/models/segmenter.py
--rw-rw-r--  2.0 unx     5218 b- defN 22-Oct-18 02:55 paddleseg/models/segnet.py
--rw-rw-r--  2.0 unx    16651 b- defN 22-Sep-23 07:32 paddleseg/models/setr.py
--rw-rw-r--  2.0 unx     8712 b- defN 22-Sep-23 07:32 paddleseg/models/sfnet.py
--rw-rw-r--  2.0 unx    13815 b- defN 22-Oct-18 02:55 paddleseg/models/sinet.py
--rw-rw-r--  2.0 unx     8141 b- defN 22-Sep-23 07:32 paddleseg/models/stdcseg.py
--rw-rw-r--  2.0 unx     5497 b- defN 22-Oct-18 02:55 paddleseg/models/topformer.py
--rw-rw-r--  2.0 unx    17553 b- defN 22-Oct-18 02:55 paddleseg/models/u2net.py
--rw-rw-r--  2.0 unx     5293 b- defN 22-Oct-18 02:55 paddleseg/models/unet.py
--rw-rw-r--  2.0 unx    14231 b- defN 22-Sep-23 07:32 paddleseg/models/unet_3plus.py
--rw-rw-r--  2.0 unx     8424 b- defN 22-Oct-18 02:55 paddleseg/models/unet_plusplus.py
--rw-rw-r--  2.0 unx     6397 b- defN 22-Sep-23 07:32 paddleseg/models/upernet.py
--rw-rw-r--  2.0 unx      993 b- defN 22-Nov-30 11:14 paddleseg/models/backbones/__init__.py
--rw-rw-r--  2.0 unx    11130 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/ghostnet.py
--rw-rw-r--  2.0 unx    29516 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/hrnet.py
--rw-rw-r--  2.0 unx    35901 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/lite_hrnet.py
--rw-rw-r--  2.0 unx    18867 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/mix_transformer.py
--rw-rw-r--  2.0 unx     7995 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/mobilenetv2.py
--rw-rw-r--  2.0 unx    15488 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/mobilenetv3.py
--rw-rw-r--  2.0 unx    13290 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/resnet_vd.py
--rw-rw-r--  2.0 unx     9977 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/shufflenetv2.py
--rw-rw-r--  2.0 unx    12282 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/stdcnet.py
--rw-rw-r--  2.0 unx    27803 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/swin_transformer.py
--rw-rw-r--  2.0 unx    22573 b- defN 22-Nov-25 06:26 paddleseg/models/backbones/top_transformer.py
--rw-rw-r--  2.0 unx     2577 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/transformer_utils.py
--rw-rw-r--  2.0 unx    33695 b- defN 22-Nov-01 02:56 paddleseg/models/backbones/uhrnet.py
--rw-rw-r--  2.0 unx    12251 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/vision_transformer.py
--rw-rw-r--  2.0 unx    14070 b- defN 22-Oct-18 02:55 paddleseg/models/backbones/xception_deeplab.py
--rw-rw-r--  2.0 unx     1137 b- defN 22-Oct-18 02:55 paddleseg/models/layers/__init__.py
--rw-rw-r--  2.0 unx     2678 b- defN 22-Sep-23 07:32 paddleseg/models/layers/activation.py
--rw-rw-r--  2.0 unx     9657 b- defN 22-Sep-23 07:32 paddleseg/models/layers/attention.py
--rw-rw-r--  2.0 unx    10384 b- defN 22-Oct-18 02:55 paddleseg/models/layers/layer_libs.py
--rw-rw-r--  2.0 unx     5921 b- defN 22-Sep-23 07:32 paddleseg/models/layers/nonlocal2d.py
--rw-rw-r--  2.0 unx     4453 b- defN 22-Oct-18 02:55 paddleseg/models/layers/polaried_self_attention.py
--rw-rw-r--  2.0 unx     6736 b- defN 22-Sep-23 07:32 paddleseg/models/layers/pyramid_pool.py
--rw-rw-r--  2.0 unx    10360 b- defN 22-Oct-18 02:55 paddleseg/models/layers/tensor_fusion.py
--rw-rw-r--  2.0 unx     4216 b- defN 22-Sep-23 07:32 paddleseg/models/layers/tensor_fusion_helper.py
--rw-rw-r--  2.0 unx     2094 b- defN 22-Sep-23 07:32 paddleseg/models/layers/wrap_functions.py
--rw-rw-r--  2.0 unx     1728 b- defN 22-Sep-23 07:32 paddleseg/models/losses/__init__.py
--rw-rw-r--  2.0 unx     8292 b- defN 22-Sep-23 07:32 paddleseg/models/losses/binary_cross_entropy_loss.py
--rw-rw-r--  2.0 unx     2734 b- defN 22-Sep-23 07:32 paddleseg/models/losses/bootstrapped_cross_entropy.py
--rw-rw-r--  2.0 unx     9168 b- defN 22-Nov-30 11:14 paddleseg/models/losses/cross_entropy_loss.py
--rw-rw-r--  2.0 unx     5170 b- defN 22-Sep-23 07:32 paddleseg/models/losses/decoupledsegnet_relax_boundary_loss.py
--rw-rw-r--  2.0 unx     5697 b- defN 22-Oct-08 02:21 paddleseg/models/losses/detail_aggregate_loss.py
--rw-rw-r--  2.0 unx     2825 b- defN 22-Sep-23 07:32 paddleseg/models/losses/dice_loss.py
+-rw-rw-r--  2.0 unx     8903 b- defN 23-Mar-28 02:49 paddleseg/core/infer.py
+-rw-rw-r--  2.0 unx     5878 b- defN 23-Mar-14 12:00 paddleseg/core/predict.py
+-rw-rw-r--  2.0 unx    17684 b- defN 23-Mar-28 02:49 paddleseg/core/train.py
+-rw-rw-r--  2.0 unx    10978 b- defN 23-Mar-14 10:54 paddleseg/core/val.py
+-rw-rw-r--  2.0 unx      725 b- defN 23-Mar-14 12:00 paddleseg/cvlibs/__init__.py
+-rw-rw-r--  2.0 unx    12527 b- defN 23-Mar-14 12:00 paddleseg/cvlibs/builder.py
+-rw-rw-r--  2.0 unx     8773 b- defN 23-Feb-23 13:11 paddleseg/cvlibs/callbacks.py
+-rw-rw-r--  2.0 unx     8001 b- defN 23-Mar-14 12:00 paddleseg/cvlibs/config.py
+-rw-rw-r--  2.0 unx     9052 b- defN 23-Mar-14 12:00 paddleseg/cvlibs/config_checker.py
+-rw-rw-r--  2.0 unx     4779 b- defN 23-Mar-14 12:00 paddleseg/cvlibs/manager.py
+-rw-rw-r--  2.0 unx     7231 b- defN 23-Mar-28 02:49 paddleseg/cvlibs/param_init.py
+-rw-rw-r--  2.0 unx     1153 b- defN 23-Mar-17 02:52 paddleseg/datasets/__init__.py
+-rw-rw-r--  2.0 unx     4743 b- defN 23-Mar-28 02:49 paddleseg/datasets/ade.py
+-rw-rw-r--  2.0 unx     4088 b- defN 23-Mar-14 12:00 paddleseg/datasets/chase_db1.py
+-rw-rw-r--  2.0 unx     3141 b- defN 23-Mar-14 12:00 paddleseg/datasets/cityscapes.py
+-rw-rw-r--  2.0 unx     2875 b- defN 23-Mar-14 12:00 paddleseg/datasets/cocostuff.py
+-rw-rw-r--  2.0 unx     6886 b- defN 23-Feb-27 02:40 paddleseg/datasets/dataset.py
+-rw-rw-r--  2.0 unx     3997 b- defN 23-Mar-14 12:00 paddleseg/datasets/drive.py
+-rw-rw-r--  2.0 unx     5536 b- defN 23-Mar-28 02:49 paddleseg/datasets/eg1800.py
+-rw-rw-r--  2.0 unx     4002 b- defN 23-Mar-14 12:00 paddleseg/datasets/hrf.py
+-rw-rw-r--  2.0 unx     3861 b- defN 23-Mar-14 12:00 paddleseg/datasets/mini_deep_globe_road_extraction.py
+-rw-rw-r--  2.0 unx     3870 b- defN 23-Mar-14 12:00 paddleseg/datasets/optic_disc_seg.py
+-rw-rw-r--  2.0 unx     3550 b- defN 23-Mar-14 12:00 paddleseg/datasets/pascal_context.py
+-rw-rw-r--  2.0 unx     3474 b- defN 23-Mar-14 12:00 paddleseg/datasets/pp_humanseg14k.py
+-rw-rw-r--  2.0 unx     5311 b- defN 23-Mar-14 12:00 paddleseg/datasets/pssl.py
+-rw-rw-r--  2.0 unx     3797 b- defN 23-Mar-14 12:00 paddleseg/datasets/stare.py
+-rw-rw-r--  2.0 unx     5593 b- defN 23-Mar-28 02:49 paddleseg/datasets/supervisely.py
+-rw-rw-r--  2.0 unx     5014 b- defN 23-Mar-14 12:00 paddleseg/datasets/voc.py
+-rw-rw-r--  2.0 unx      576 b- defN 23-Mar-14 12:00 paddleseg/deploy/__init__.py
+-rw-rw-r--  2.0 unx     1247 b- defN 23-Mar-14 12:00 paddleseg/deploy/export.py
+-rw-rw-r--  2.0 unx     1528 b- defN 23-Mar-14 12:00 paddleseg/deploy/infer.py
+-rw-rw-r--  2.0 unx     2358 b- defN 23-Mar-28 02:49 paddleseg/models/__init__.py
+-rw-rw-r--  2.0 unx    15779 b- defN 23-Feb-27 02:40 paddleseg/models/ann.py
+-rw-rw-r--  2.0 unx     6499 b- defN 23-Feb-27 02:40 paddleseg/models/attention_unet.py
+-rw-rw-r--  2.0 unx    10259 b- defN 23-Feb-27 02:40 paddleseg/models/bisenet.py
+-rw-rw-r--  2.0 unx     8133 b- defN 23-Feb-27 02:40 paddleseg/models/bisenetv1.py
+-rw-rw-r--  2.0 unx     6942 b- defN 23-Feb-27 02:40 paddleseg/models/ccnet.py
+-rw-rw-r--  2.0 unx     7367 b- defN 23-Feb-27 02:40 paddleseg/models/danet.py
+-rw-rw-r--  2.0 unx    14345 b- defN 23-Mar-03 10:34 paddleseg/models/ddrnet.py
+-rw-rw-r--  2.0 unx     9203 b- defN 23-Feb-27 02:40 paddleseg/models/decoupled_segnet.py
+-rwxrwxrwx  2.0 unx    10602 b- defN 22-Sep-23 07:32 paddleseg/models/deeplab.py
+-rw-rw-r--  2.0 unx     5457 b- defN 23-Feb-27 02:40 paddleseg/models/dmnet.py
+-rw-rw-r--  2.0 unx     9002 b- defN 23-Feb-27 02:40 paddleseg/models/dnlnet.py
+-rw-rw-r--  2.0 unx     8326 b- defN 23-Feb-27 02:40 paddleseg/models/emanet.py
+-rw-rw-r--  2.0 unx     7954 b- defN 23-Feb-27 02:40 paddleseg/models/encnet.py
+-rw-rw-r--  2.0 unx    21755 b- defN 23-Feb-27 02:40 paddleseg/models/enet.py
+-rw-rw-r--  2.0 unx    17101 b- defN 23-Mar-28 02:49 paddleseg/models/espnet.py
+-rw-rw-r--  2.0 unx     9993 b- defN 23-Feb-27 02:40 paddleseg/models/espnetv1.py
+-rw-rw-r--  2.0 unx    11996 b- defN 23-Feb-27 02:40 paddleseg/models/fast_scnn.py
+-rw-rw-r--  2.0 unx     8380 b- defN 23-Feb-27 02:40 paddleseg/models/fastfcn.py
+-rwxrwxrwx  2.0 unx     5150 b- defN 22-Sep-23 07:32 paddleseg/models/fcn.py
+-rw-rw-r--  2.0 unx     7998 b- defN 23-Feb-27 02:40 paddleseg/models/gcnet.py
+-rw-rw-r--  2.0 unx    10388 b- defN 23-Feb-27 02:40 paddleseg/models/ginet.py
+-rw-rw-r--  2.0 unx     7201 b- defN 23-Feb-27 02:40 paddleseg/models/glore.py
+-rw-rw-r--  2.0 unx    13216 b- defN 23-Feb-27 02:40 paddleseg/models/gscnn.py
+-rw-rw-r--  2.0 unx    11089 b- defN 23-Feb-27 02:40 paddleseg/models/hardnet.py
+-rw-rw-r--  2.0 unx     4630 b- defN 23-Mar-14 12:00 paddleseg/models/hrnet_contrast.py
+-rw-rw-r--  2.0 unx     7724 b- defN 23-Feb-27 02:40 paddleseg/models/isanet.py
+-rw-rw-r--  2.0 unx    20736 b- defN 23-Mar-28 02:49 paddleseg/models/knet.py
+-rw-rw-r--  2.0 unx     4824 b- defN 23-Mar-28 02:49 paddleseg/models/lpsnet.py
+-rw-rw-r--  2.0 unx     6385 b- defN 23-Feb-27 02:40 paddleseg/models/lraspp.py
+-rw-rw-r--  2.0 unx    26677 b- defN 23-Mar-28 02:49 paddleseg/models/maskformer.py
+-rw-rw-r--  2.0 unx     7555 b- defN 23-Feb-27 02:40 paddleseg/models/mla_transformer.py
+-rw-rw-r--  2.0 unx    11412 b- defN 23-Feb-27 02:40 paddleseg/models/mobileseg.py
+-rw-rw-r--  2.0 unx    13220 b- defN 23-Feb-27 02:40 paddleseg/models/mscale_ocrnet.py
+-rwxrwxrwx  2.0 unx     9290 b- defN 22-Sep-23 07:32 paddleseg/models/ocrnet.py
+-rw-rw-r--  2.0 unx     7847 b- defN 23-Feb-27 02:40 paddleseg/models/pfpnnet.py
+-rw-rw-r--  2.0 unx    35884 b- defN 23-Feb-27 02:40 paddleseg/models/pointrend.py
+-rw-rw-r--  2.0 unx     7230 b- defN 23-Feb-27 02:40 paddleseg/models/portraitnet.py
+-rw-rw-r--  2.0 unx    10544 b- defN 23-Feb-27 02:40 paddleseg/models/pp_liteseg.py
+-rw-rw-r--  2.0 unx     4180 b- defN 23-Mar-28 02:49 paddleseg/models/pp_mobileseg.py
+-rw-rw-r--  2.0 unx     7874 b- defN 23-Feb-27 02:40 paddleseg/models/pphumanseg_lite.py
+-rwxrwxrwx  2.0 unx     6018 b- defN 22-Oct-08 06:57 paddleseg/models/pspnet.py
+-rw-rw-r--  2.0 unx    27207 b- defN 23-Feb-27 02:40 paddleseg/models/rtformer.py
+-rw-rw-r--  2.0 unx     4345 b- defN 23-Feb-27 02:40 paddleseg/models/segformer.py
+-rw-rw-r--  2.0 unx     9075 b- defN 23-Feb-27 02:40 paddleseg/models/segmenter.py
+-rw-rw-r--  2.0 unx     5218 b- defN 23-Feb-27 02:40 paddleseg/models/segnet.py
+-rw-rw-r--  2.0 unx     5976 b- defN 23-Mar-28 02:49 paddleseg/models/segnext.py
+-rw-rw-r--  2.0 unx    16651 b- defN 23-Feb-27 02:40 paddleseg/models/setr.py
+-rw-rw-r--  2.0 unx     8712 b- defN 23-Feb-27 02:40 paddleseg/models/sfnet.py
+-rw-rw-r--  2.0 unx    13815 b- defN 23-Feb-27 02:40 paddleseg/models/sinet.py
+-rw-rw-r--  2.0 unx     8141 b- defN 23-Feb-27 02:40 paddleseg/models/stdcseg.py
+-rw-rw-r--  2.0 unx     5497 b- defN 23-Feb-27 02:40 paddleseg/models/topformer.py
+-rw-rw-r--  2.0 unx    17553 b- defN 23-Feb-27 02:40 paddleseg/models/u2net.py
+-rw-rw-r--  2.0 unx     5293 b- defN 23-Feb-27 02:40 paddleseg/models/unet.py
+-rw-rw-r--  2.0 unx    14231 b- defN 23-Feb-27 02:40 paddleseg/models/unet_3plus.py
+-rw-rw-r--  2.0 unx     8424 b- defN 23-Feb-27 02:40 paddleseg/models/unet_plusplus.py
+-rw-rw-r--  2.0 unx     6397 b- defN 23-Feb-27 02:40 paddleseg/models/upernet.py
+-rw-rw-r--  2.0 unx    12497 b- defN 23-Mar-28 02:49 paddleseg/models/upernet_cae.py
+-rw-rw-r--  2.0 unx    10256 b- defN 23-Mar-28 02:49 paddleseg/models/upernet_vit_adapter.py
+-rw-rw-r--  2.0 unx     1111 b- defN 23-Mar-28 02:49 paddleseg/models/backbones/__init__.py
+-rw-rw-r--  2.0 unx    21707 b- defN 23-Mar-28 02:49 paddleseg/models/backbones/cae.py
+-rw-rw-r--  2.0 unx    11130 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/ghostnet.py
+-rw-rw-r--  2.0 unx    42666 b- defN 23-Mar-28 02:49 paddleseg/models/backbones/hrformer.py
+-rw-rw-r--  2.0 unx    29516 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/hrnet.py
+-rw-rw-r--  2.0 unx    35901 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/lite_hrnet.py
+-rw-rw-r--  2.0 unx    18867 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/mix_transformer.py
+-rw-rw-r--  2.0 unx     7995 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/mobilenetv2.py
+-rw-rw-r--  2.0 unx    15489 b- defN 23-Mar-28 02:49 paddleseg/models/backbones/mobilenetv3.py
+-rw-rw-r--  2.0 unx    12784 b- defN 23-Mar-28 02:49 paddleseg/models/backbones/mscan.py
+-rw-rw-r--  2.0 unx    13290 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/resnet_vd.py
+-rw-rw-r--  2.0 unx     9977 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/shufflenetv2.py
+-rw-rw-r--  2.0 unx    12282 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/stdcnet.py
+-rw-rw-r--  2.0 unx    26915 b- defN 23-Mar-28 02:49 paddleseg/models/backbones/strideformer.py
+-rw-rw-r--  2.0 unx    33211 b- defN 23-Apr-11 12:00 paddleseg/models/backbones/swin_transformer.py
+-rw-rw-r--  2.0 unx    22573 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/top_transformer.py
+-rw-rw-r--  2.0 unx     2592 b- defN 23-Mar-14 12:00 paddleseg/models/backbones/transformer_utils.py
+-rw-rw-r--  2.0 unx    33695 b- defN 23-Mar-14 12:00 paddleseg/models/backbones/uhrnet.py
+-rw-rw-r--  2.0 unx    12251 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/vision_transformer.py
+-rw-rw-r--  2.0 unx    15122 b- defN 23-Mar-28 02:49 paddleseg/models/backbones/vit_adapter.py
+-rw-rw-r--  2.0 unx    14070 b- defN 23-Feb-27 02:40 paddleseg/models/backbones/xception_deeplab.py
+-rw-rw-r--  2.0 unx     1187 b- defN 23-Mar-28 02:49 paddleseg/models/layers/__init__.py
+-rw-rw-r--  2.0 unx     2678 b- defN 23-Feb-27 02:40 paddleseg/models/layers/activation.py
+-rw-rw-r--  2.0 unx     9657 b- defN 23-Feb-27 02:40 paddleseg/models/layers/attention.py
+-rw-rw-r--  2.0 unx    12232 b- defN 23-Mar-28 02:49 paddleseg/models/layers/layer_libs.py
+-rw-rw-r--  2.0 unx     7267 b- defN 23-Mar-28 02:49 paddleseg/models/layers/ms_deformable_attention.py
+-rw-rw-r--  2.0 unx     5241 b- defN 23-Mar-28 02:49 paddleseg/models/layers/nmf_2d.py
+-rwxrwxrwx  2.0 unx     5921 b- defN 22-Sep-23 07:32 paddleseg/models/layers/nonlocal2d.py
+-rw-rw-r--  2.0 unx     4453 b- defN 23-Feb-27 02:40 paddleseg/models/layers/polaried_self_attention.py
+-rwxrwxrwx  2.0 unx     6736 b- defN 22-Sep-23 07:32 paddleseg/models/layers/pyramid_pool.py
+-rw-rw-r--  2.0 unx    10360 b- defN 23-Feb-27 02:40 paddleseg/models/layers/tensor_fusion.py
+-rw-rw-r--  2.0 unx     4216 b- defN 23-Feb-27 02:40 paddleseg/models/layers/tensor_fusion_helper.py
+-rw-rw-r--  2.0 unx    14758 b- defN 23-Mar-28 02:49 paddleseg/models/layers/vit_adapter_layers.py
+-rwxrwxrwx  2.0 unx     2094 b- defN 22-Sep-23 07:32 paddleseg/models/layers/wrap_functions.py
+-rw-rw-r--  2.0 unx     1772 b- defN 23-Mar-28 02:49 paddleseg/models/losses/__init__.py
+-rw-rw-r--  2.0 unx     8360 b- defN 23-Mar-28 02:49 paddleseg/models/losses/binary_cross_entropy_loss.py
+-rwxrwxrwx  2.0 unx     2734 b- defN 22-Sep-23 07:32 paddleseg/models/losses/bootstrapped_cross_entropy.py
+-rw-rw-r--  2.0 unx     9472 b- defN 23-Mar-28 02:49 paddleseg/models/losses/cross_entropy_loss.py
+-rw-rw-r--  2.0 unx     5170 b- defN 23-Feb-27 02:40 paddleseg/models/losses/decoupledsegnet_relax_boundary_loss.py
+-rw-rw-r--  2.0 unx     5697 b- defN 23-Feb-27 02:40 paddleseg/models/losses/detail_aggregate_loss.py
+-rw-rw-r--  2.0 unx     2825 b- defN 23-Feb-27 02:40 paddleseg/models/losses/dice_loss.py
 -rwxrwxrwx  2.0 unx     3100 b- defN 22-Jan-10 03:05 paddleseg/models/losses/edge_attention_loss.py
--rw-rw-r--  2.0 unx     4825 b- defN 22-Sep-23 07:32 paddleseg/models/losses/focal_loss.py
+-rw-rw-r--  2.0 unx     4893 b- defN 23-Mar-28 02:49 paddleseg/models/losses/focal_loss.py
 -rwxrwxrwx  2.0 unx     5172 b- defN 22-Jan-10 03:05 paddleseg/models/losses/gscnn_dual_task_loss.py
--rw-rw-r--  2.0 unx     3176 b- defN 22-Sep-23 07:32 paddleseg/models/losses/kl_loss.py
--rw-rw-r--  2.0 unx     4222 b- defN 22-Oct-18 02:55 paddleseg/models/losses/l1_loss.py
--rw-rw-r--  2.0 unx     8058 b- defN 22-Oct-18 02:55 paddleseg/models/losses/lovasz_loss.py
--rw-rw-r--  2.0 unx     2752 b- defN 22-Sep-23 07:32 paddleseg/models/losses/mean_square_error_loss.py
--rw-rw-r--  2.0 unx     1984 b- defN 22-Sep-23 07:32 paddleseg/models/losses/mixed_loss.py
--rw-rw-r--  2.0 unx     3894 b- defN 22-Nov-28 09:25 paddleseg/models/losses/ohem_cross_entropy_loss.py
--rw-rw-r--  2.0 unx     4633 b- defN 22-Nov-28 09:25 paddleseg/models/losses/ohem_edge_attention_loss.py
--rw-rw-r--  2.0 unx     7994 b- defN 22-Sep-23 07:32 paddleseg/models/losses/pixel_contrast_cross_entropy_loss.py
--rw-rw-r--  2.0 unx     6446 b- defN 22-Sep-23 07:32 paddleseg/models/losses/point_cross_entropy_loss.py
--rw-rw-r--  2.0 unx    10038 b- defN 22-Nov-01 02:56 paddleseg/models/losses/rmi_loss.py
--rw-rw-r--  2.0 unx     7540 b- defN 22-Sep-23 07:32 paddleseg/models/losses/semantic_connectivity_loss.py
--rw-rw-r--  2.0 unx     1698 b- defN 22-Sep-23 07:32 paddleseg/models/losses/semantic_encode_cross_entropy_loss.py
+-rwxrwxrwx  2.0 unx     3176 b- defN 22-Sep-23 07:32 paddleseg/models/losses/kl_loss.py
+-rw-rw-r--  2.0 unx     4222 b- defN 23-Feb-27 02:40 paddleseg/models/losses/l1_loss.py
+-rw-rw-r--  2.0 unx     8058 b- defN 23-Feb-27 02:40 paddleseg/models/losses/lovasz_loss.py
+-rw-rw-r--  2.0 unx    18810 b- defN 23-Apr-11 12:00 paddleseg/models/losses/maskformer_loss.py
+-rwxrwxrwx  2.0 unx     2752 b- defN 22-Sep-23 07:32 paddleseg/models/losses/mean_square_error_loss.py
+-rwxrwxrwx  2.0 unx     1984 b- defN 22-Sep-23 07:32 paddleseg/models/losses/mixed_loss.py
+-rw-rw-r--  2.0 unx     3894 b- defN 23-Feb-27 02:40 paddleseg/models/losses/ohem_cross_entropy_loss.py
+-rw-rw-r--  2.0 unx     4633 b- defN 23-Feb-27 02:40 paddleseg/models/losses/ohem_edge_attention_loss.py
+-rw-rw-r--  2.0 unx     7994 b- defN 23-Feb-27 02:40 paddleseg/models/losses/pixel_contrast_cross_entropy_loss.py
+-rw-rw-r--  2.0 unx     6446 b- defN 23-Feb-27 02:40 paddleseg/models/losses/point_cross_entropy_loss.py
+-rw-rw-r--  2.0 unx    10038 b- defN 23-Feb-27 02:40 paddleseg/models/losses/rmi_loss.py
+-rw-rw-r--  2.0 unx     7540 b- defN 23-Feb-27 02:40 paddleseg/models/losses/semantic_connectivity_loss.py
+-rw-rw-r--  2.0 unx     1698 b- defN 23-Feb-27 02:40 paddleseg/models/losses/semantic_encode_cross_entropy_loss.py
+-rw-rw-r--  2.0 unx      634 b- defN 23-Mar-28 02:49 paddleseg/optimizers/__init__.py
+-rw-rw-r--  2.0 unx     8756 b- defN 23-Mar-28 02:49 paddleseg/optimizers/custom_optimizers.py
+-rw-rw-r--  2.0 unx    10265 b- defN 23-Mar-28 02:49 paddleseg/optimizers/optimizer.py
 -rwxrwxrwx  2.0 unx      661 b- defN 22-Jan-10 03:05 paddleseg/transforms/__init__.py
--rw-rw-r--  2.0 unx     5468 b- defN 22-Oct-18 02:55 paddleseg/transforms/functional.py
--rw-rw-r--  2.0 unx    44145 b- defN 22-Oct-18 02:55 paddleseg/transforms/transforms.py
--rw-rw-r--  2.0 unx      827 b- defN 22-Oct-18 02:55 paddleseg/utils/__init__.py
--rwxrwxrwx  2.0 unx     2399 b- defN 22-Jan-10 03:05 paddleseg/utils/config_check.py
--rw-rw-r--  2.0 unx     5614 b- defN 22-Nov-30 11:14 paddleseg/utils/download.py
--rw-rw-r--  2.0 unx     3489 b- defN 22-Sep-23 07:32 paddleseg/utils/ema.py
--rw-rw-r--  2.0 unx     1395 b- defN 22-Sep-23 07:32 paddleseg/utils/logger.py
--rw-rw-r--  2.0 unx     8426 b- defN 22-Sep-23 07:32 paddleseg/utils/metrics.py
--rw-rw-r--  2.0 unx      776 b- defN 22-Sep-23 07:32 paddleseg/utils/op_flops_funs.py
--rw-rw-r--  2.0 unx     8137 b- defN 22-Sep-23 07:32 paddleseg/utils/progbar.py
--rw-rw-r--  2.0 unx     1615 b- defN 22-Sep-23 07:32 paddleseg/utils/timer.py
--rw-rw-r--  2.0 unx     4455 b- defN 22-Sep-23 07:32 paddleseg/utils/train_profiler.py
--rw-rw-r--  2.0 unx     6779 b- defN 22-Nov-30 11:14 paddleseg/utils/utils.py
--rw-rw-r--  2.0 unx     4750 b- defN 22-Sep-23 07:32 paddleseg/utils/visualize.py
+-rw-rw-r--  2.0 unx     5587 b- defN 23-Mar-28 02:49 paddleseg/transforms/functional.py
+-rw-rw-r--  2.0 unx    47887 b- defN 23-Mar-28 02:49 paddleseg/transforms/transforms.py
+-rw-rw-r--  2.0 unx      825 b- defN 23-Mar-28 02:49 paddleseg/utils/__init__.py
+-rw-rw-r--  2.0 unx     2399 b- defN 23-Feb-23 13:11 paddleseg/utils/config_check.py
+-rw-rw-r--  2.0 unx     6038 b- defN 23-Apr-11 12:00 paddleseg/utils/download.py
+-rw-rw-r--  2.0 unx     1648 b- defN 23-Mar-28 02:49 paddleseg/utils/ema.py
+-rw-rw-r--  2.0 unx     1395 b- defN 23-Feb-27 02:40 paddleseg/utils/logger.py
+-rw-rw-r--  2.0 unx     8426 b- defN 23-Mar-28 02:49 paddleseg/utils/metrics.py
+-rwxrwxrwx  2.0 unx      776 b- defN 22-Sep-23 07:32 paddleseg/utils/op_flops_funs.py
+-rw-rw-r--  2.0 unx     8137 b- defN 23-Feb-27 02:40 paddleseg/utils/progbar.py
+-rwxrwxrwx  2.0 unx     1615 b- defN 22-Sep-23 07:32 paddleseg/utils/timer.py
+-rwxrwxrwx  2.0 unx     4455 b- defN 22-Sep-23 07:32 paddleseg/utils/train_profiler.py
+-rw-rw-r--  2.0 unx    10523 b- defN 23-Apr-06 07:44 paddleseg/utils/utils.py
+-rw-rw-r--  2.0 unx     4750 b- defN 23-Feb-27 02:40 paddleseg/utils/visualize.py
 -rwxrwxrwx  2.0 unx      665 b- defN 22-Jan-10 03:05 paddleseg/utils/env/__init__.py
 -rwxrwxrwx  2.0 unx     1909 b- defN 22-Jan-10 03:05 paddleseg/utils/env/seg_env.py
--rw-rw-r--  2.0 unx     4370 b- defN 22-Sep-23 07:32 paddleseg/utils/env/sys_env.py
--rwxrwxrwx  2.0 unx    11357 b- defN 22-Nov-30 11:15 paddleseg-2.7.0.dist-info/LICENSE
--rw-rw-r--  2.0 unx     1265 b- defN 22-Nov-30 11:15 paddleseg-2.7.0.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 22-Nov-30 11:15 paddleseg-2.7.0.dist-info/WHEEL
--rwxrwxrwx  2.0 unx       10 b- defN 22-Nov-30 11:15 paddleseg-2.7.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    13841 b- defN 22-Nov-30 11:15 paddleseg-2.7.0.dist-info/RECORD
-154 files, 1259348 bytes uncompressed, 327500 bytes compressed:  74.0%
+-rw-rw-r--  2.0 unx     4370 b- defN 23-Feb-27 02:40 paddleseg/utils/env/sys_env.py
+-rwxrwxrwx  2.0 unx    11357 b- defN 23-Apr-11 13:34 paddleseg-2.8.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     1272 b- defN 23-Apr-11 13:34 paddleseg-2.8.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-11 13:34 paddleseg-2.8.0.dist-info/WHEEL
+-rwxrwxrwx  2.0 unx       10 b- defN 23-Apr-11 13:34 paddleseg-2.8.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    15780 b- defN 23-Apr-11 13:34 paddleseg-2.8.0.dist-info/RECORD
+175 files, 1558642 bytes uncompressed, 399374 bytes compressed:  74.4%
```

## zipnote {}

```diff
@@ -15,24 +15,27 @@
 
 Filename: paddleseg/core/val.py
 Comment: 
 
 Filename: paddleseg/cvlibs/__init__.py
 Comment: 
 
+Filename: paddleseg/cvlibs/builder.py
+Comment: 
+
 Filename: paddleseg/cvlibs/callbacks.py
 Comment: 
 
 Filename: paddleseg/cvlibs/config.py
 Comment: 
 
-Filename: paddleseg/cvlibs/manager.py
+Filename: paddleseg/cvlibs/config_checker.py
 Comment: 
 
-Filename: paddleseg/cvlibs/optimizers.py
+Filename: paddleseg/cvlibs/manager.py
 Comment: 
 
 Filename: paddleseg/cvlibs/param_init.py
 Comment: 
 
 Filename: paddleseg/datasets/__init__.py
 Comment: 
@@ -81,27 +84,30 @@
 
 Filename: paddleseg/datasets/supervisely.py
 Comment: 
 
 Filename: paddleseg/datasets/voc.py
 Comment: 
 
-Filename: paddleseg/models/__init__.py
+Filename: paddleseg/deploy/__init__.py
 Comment: 
 
-Filename: paddleseg/models/_test.py
+Filename: paddleseg/deploy/export.py
 Comment: 
 
-Filename: paddleseg/models/ann.py
+Filename: paddleseg/deploy/infer.py
 Comment: 
 
-Filename: paddleseg/models/attention_unet.py
+Filename: paddleseg/models/__init__.py
+Comment: 
+
+Filename: paddleseg/models/ann.py
 Comment: 
 
-Filename: paddleseg/models/base_model.py
+Filename: paddleseg/models/attention_unet.py
 Comment: 
 
 Filename: paddleseg/models/bisenet.py
 Comment: 
 
 Filename: paddleseg/models/bisenetv1.py
 Comment: 
@@ -168,17 +174,26 @@
 
 Filename: paddleseg/models/hrnet_contrast.py
 Comment: 
 
 Filename: paddleseg/models/isanet.py
 Comment: 
 
+Filename: paddleseg/models/knet.py
+Comment: 
+
+Filename: paddleseg/models/lpsnet.py
+Comment: 
+
 Filename: paddleseg/models/lraspp.py
 Comment: 
 
+Filename: paddleseg/models/maskformer.py
+Comment: 
+
 Filename: paddleseg/models/mla_transformer.py
 Comment: 
 
 Filename: paddleseg/models/mobileseg.py
 Comment: 
 
 Filename: paddleseg/models/mscale_ocrnet.py
@@ -195,14 +210,17 @@
 
 Filename: paddleseg/models/portraitnet.py
 Comment: 
 
 Filename: paddleseg/models/pp_liteseg.py
 Comment: 
 
+Filename: paddleseg/models/pp_mobileseg.py
+Comment: 
+
 Filename: paddleseg/models/pphumanseg_lite.py
 Comment: 
 
 Filename: paddleseg/models/pspnet.py
 Comment: 
 
 Filename: paddleseg/models/rtformer.py
@@ -213,14 +231,17 @@
 
 Filename: paddleseg/models/segmenter.py
 Comment: 
 
 Filename: paddleseg/models/segnet.py
 Comment: 
 
+Filename: paddleseg/models/segnext.py
+Comment: 
+
 Filename: paddleseg/models/setr.py
 Comment: 
 
 Filename: paddleseg/models/sfnet.py
 Comment: 
 
 Filename: paddleseg/models/sinet.py
@@ -243,20 +264,32 @@
 
 Filename: paddleseg/models/unet_plusplus.py
 Comment: 
 
 Filename: paddleseg/models/upernet.py
 Comment: 
 
+Filename: paddleseg/models/upernet_cae.py
+Comment: 
+
+Filename: paddleseg/models/upernet_vit_adapter.py
+Comment: 
+
 Filename: paddleseg/models/backbones/__init__.py
 Comment: 
 
+Filename: paddleseg/models/backbones/cae.py
+Comment: 
+
 Filename: paddleseg/models/backbones/ghostnet.py
 Comment: 
 
+Filename: paddleseg/models/backbones/hrformer.py
+Comment: 
+
 Filename: paddleseg/models/backbones/hrnet.py
 Comment: 
 
 Filename: paddleseg/models/backbones/lite_hrnet.py
 Comment: 
 
 Filename: paddleseg/models/backbones/mix_transformer.py
@@ -264,23 +297,29 @@
 
 Filename: paddleseg/models/backbones/mobilenetv2.py
 Comment: 
 
 Filename: paddleseg/models/backbones/mobilenetv3.py
 Comment: 
 
+Filename: paddleseg/models/backbones/mscan.py
+Comment: 
+
 Filename: paddleseg/models/backbones/resnet_vd.py
 Comment: 
 
 Filename: paddleseg/models/backbones/shufflenetv2.py
 Comment: 
 
 Filename: paddleseg/models/backbones/stdcnet.py
 Comment: 
 
+Filename: paddleseg/models/backbones/strideformer.py
+Comment: 
+
 Filename: paddleseg/models/backbones/swin_transformer.py
 Comment: 
 
 Filename: paddleseg/models/backbones/top_transformer.py
 Comment: 
 
 Filename: paddleseg/models/backbones/transformer_utils.py
@@ -288,14 +327,17 @@
 
 Filename: paddleseg/models/backbones/uhrnet.py
 Comment: 
 
 Filename: paddleseg/models/backbones/vision_transformer.py
 Comment: 
 
+Filename: paddleseg/models/backbones/vit_adapter.py
+Comment: 
+
 Filename: paddleseg/models/backbones/xception_deeplab.py
 Comment: 
 
 Filename: paddleseg/models/layers/__init__.py
 Comment: 
 
 Filename: paddleseg/models/layers/activation.py
@@ -303,14 +345,20 @@
 
 Filename: paddleseg/models/layers/attention.py
 Comment: 
 
 Filename: paddleseg/models/layers/layer_libs.py
 Comment: 
 
+Filename: paddleseg/models/layers/ms_deformable_attention.py
+Comment: 
+
+Filename: paddleseg/models/layers/nmf_2d.py
+Comment: 
+
 Filename: paddleseg/models/layers/nonlocal2d.py
 Comment: 
 
 Filename: paddleseg/models/layers/polaried_self_attention.py
 Comment: 
 
 Filename: paddleseg/models/layers/pyramid_pool.py
@@ -318,14 +366,17 @@
 
 Filename: paddleseg/models/layers/tensor_fusion.py
 Comment: 
 
 Filename: paddleseg/models/layers/tensor_fusion_helper.py
 Comment: 
 
+Filename: paddleseg/models/layers/vit_adapter_layers.py
+Comment: 
+
 Filename: paddleseg/models/layers/wrap_functions.py
 Comment: 
 
 Filename: paddleseg/models/losses/__init__.py
 Comment: 
 
 Filename: paddleseg/models/losses/binary_cross_entropy_loss.py
@@ -360,14 +411,17 @@
 
 Filename: paddleseg/models/losses/l1_loss.py
 Comment: 
 
 Filename: paddleseg/models/losses/lovasz_loss.py
 Comment: 
 
+Filename: paddleseg/models/losses/maskformer_loss.py
+Comment: 
+
 Filename: paddleseg/models/losses/mean_square_error_loss.py
 Comment: 
 
 Filename: paddleseg/models/losses/mixed_loss.py
 Comment: 
 
 Filename: paddleseg/models/losses/ohem_cross_entropy_loss.py
@@ -387,14 +441,23 @@
 
 Filename: paddleseg/models/losses/semantic_connectivity_loss.py
 Comment: 
 
 Filename: paddleseg/models/losses/semantic_encode_cross_entropy_loss.py
 Comment: 
 
+Filename: paddleseg/optimizers/__init__.py
+Comment: 
+
+Filename: paddleseg/optimizers/custom_optimizers.py
+Comment: 
+
+Filename: paddleseg/optimizers/optimizer.py
+Comment: 
+
 Filename: paddleseg/transforms/__init__.py
 Comment: 
 
 Filename: paddleseg/transforms/functional.py
 Comment: 
 
 Filename: paddleseg/transforms/transforms.py
@@ -441,23 +504,23 @@
 
 Filename: paddleseg/utils/env/seg_env.py
 Comment: 
 
 Filename: paddleseg/utils/env/sys_env.py
 Comment: 
 
-Filename: paddleseg-2.7.0.dist-info/LICENSE
+Filename: paddleseg-2.8.0.dist-info/LICENSE
 Comment: 
 
-Filename: paddleseg-2.7.0.dist-info/METADATA
+Filename: paddleseg-2.8.0.dist-info/METADATA
 Comment: 
 
-Filename: paddleseg-2.7.0.dist-info/WHEEL
+Filename: paddleseg-2.8.0.dist-info/WHEEL
 Comment: 
 
-Filename: paddleseg-2.7.0.dist-info/top_level.txt
+Filename: paddleseg-2.8.0.dist-info/top_level.txt
 Comment: 
 
-Filename: paddleseg-2.7.0.dist-info/RECORD
+Filename: paddleseg-2.8.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## paddleseg/__init__.py

```diff
@@ -8,10 +8,10 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from . import models, datasets, transforms
+from . import models, datasets, transforms, optimizers
 
-__version__ = '2.7.0'
+__version__ = '2.8.0'
```

## paddleseg/core/infer.py

```diff
@@ -91,16 +91,16 @@
     Return:
         Tensor: The logit of input image.
     """
     h_im, w_im = im.shape[-2:]
     w_crop, h_crop = crop_size
     w_stride, h_stride = stride
     # calculate the crop nums
-    rows = np.int(np.ceil(1.0 * (h_im - h_crop) / h_stride)) + 1
-    cols = np.int(np.ceil(1.0 * (w_im - w_crop) / w_stride)) + 1
+    rows = int(np.ceil(1.0 * (h_im - h_crop) / h_stride)) + 1
+    cols = int(np.ceil(1.0 * (w_im - w_crop) / w_stride)) + 1
     # prevent negative sliding rounds when imgs after scaling << crop_size
     rows = 1 if h_im <= h_crop else rows
     cols = 1 if w_im <= w_crop else cols
     # TODO 'Tensor' object does not support item assignment. If support, use tensor to calculation.
     final_logit = None
     count = np.zeros([1, 1, h_im, w_im])
     for r in range(rows):
@@ -204,29 +204,31 @@
     elif not isinstance(scales, (tuple, list)):
         raise TypeError(
             '`scales` expects float/tuple/list type, but received {}'.format(
                 type(scales)))
     final_logit = 0
     h_input, w_input = im.shape[-2], im.shape[-1]
     flip_comb = flip_combination(flip_horizontal, flip_vertical)
+    num_augs = len(scales) * len(flip_comb)
     for scale in scales:
         h = int(h_input * scale + 0.5)
         w = int(w_input * scale + 0.5)
-        im = F.interpolate(im, [h, w], mode='bilinear')
+        im_scale = F.interpolate(im, [h, w], mode='bilinear')
         for flip in flip_comb:
-            im_flip = tensor_flip(im, flip)
+            im_flip = tensor_flip(im_scale, flip)
             logit = inference(
                 model,
                 im_flip,
                 is_slide=is_slide,
                 crop_size=crop_size,
                 stride=stride)
             logit = tensor_flip(logit, flip)
             logit = F.interpolate(logit, [h_input, w_input], mode='bilinear')
-
-            logit = F.softmax(logit, axis=1)
-            final_logit = final_logit + logit
-
+            # Accumulate final logits in place
+            final_logit += logit
+    # We average the accumulated logits to make the numeric values of `final_logit`
+    # comparable to single-scale logits
+    final_logit /= num_augs
     final_logit = reverse_transform(final_logit, trans_info, mode='bilinear')
     pred = paddle.argmax(final_logit, axis=1, keepdim=True, dtype='int32')
 
     return pred, final_logit
```

## paddleseg/core/predict.py

```diff
@@ -141,7 +141,10 @@
             pred_mask = utils.visualize.get_pseudo_color_map(pred, color_map)
             pred_saved_path = os.path.join(
                 pred_saved_dir, os.path.splitext(im_file)[0] + ".png")
             mkdir(pred_saved_path)
             pred_mask.save(pred_saved_path)
 
             progbar_pred.update(i + 1)
+
+    logger.info("Predicted images are saved in {} and {} .".format(
+        added_saved_dir, pred_saved_dir))
```

## paddleseg/core/train.py

```diff
@@ -12,20 +12,22 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 import time
 from collections import deque
 import shutil
+from copy import deepcopy
 
 import paddle
 import paddle.nn.functional as F
 
 from paddleseg.utils import (TimeAverager, calculate_eta, resume, logger,
-                             worker_init_fn, train_profiler, op_flops_funs)
+                             worker_init_fn, train_profiler, op_flops_funs,
+                             init_ema_params, update_ema_model)
 from paddleseg.core.val import evaluate
 
 
 def check_logits_losses(logits_list, losses):
     len_logits = len(logits_list)
     len_losses = len(losses['types'])
     if len_logits != len_losses:
@@ -64,14 +66,15 @@
           iters=10000,
           batch_size=2,
           resume_model=None,
           save_interval=1000,
           log_iters=10,
           num_workers=0,
           use_vdl=False,
+          use_ema=False,
           losses=None,
           keep_checkpoint_max=5,
           test_config=None,
           precision='fp32',
           amp_level='O1',
           profiler_options=None,
           to_static_training=False):
@@ -98,14 +101,21 @@
         precision (str, optional): Use AMP if precision='fp16'. If precision='fp32', the training is normal.
         amp_level (str, optional): Auto mixed precision level. Accepted values are “O1” and “O2”: O1 represent mixed precision, 
             the input data type of each operator will be casted by white_list and black_list; O2 represent Pure fp16, all operators 
             parameters and input data will be casted to fp16, except operators in black_list, don’t support fp16 kernel and batchnorm. Default is O1(amp)
         profiler_options (str, optional): The option of train profiler.
         to_static_training (bool, optional): Whether to use @to_static for training.
     """
+
+    if use_ema:
+        ema_model = deepcopy(model)
+        ema_model.eval()
+        for param in ema_model.parameters():
+            param.stop_gradient = True
+
     model.train()
     nranks = paddle.distributed.ParallelEnv().nranks
     local_rank = paddle.distributed.ParallelEnv().local_rank
 
     start_iter = 0
     if resume_model is not None:
         start_iter = resume(model, optimizer, resume_model)
@@ -150,86 +160,105 @@
         model = paddle.jit.to_static(model)
         logger.info("Successfully applied @to_static")
 
     avg_loss = 0.0
     avg_loss_list = []
     iters_per_epoch = len(batch_sampler)
     best_mean_iou = -1.0
+    best_ema_mean_iou = -1.0
     best_model_iter = -1
     reader_cost_averager = TimeAverager()
     batch_cost_averager = TimeAverager()
     save_models = deque()
     batch_start = time.time()
-
     iter = start_iter
     while iter < iters:
+        if iter == start_iter and use_ema:
+            init_ema_params(ema_model, model)
         for data in loader:
             iter += 1
             if iter > iters:
                 version = paddle.__version__
                 if version == '2.1.2':
                     continue
                 else:
                     break
             reader_cost_averager.record(time.time() - batch_start)
             images = data['img']
             labels = data['label'].astype('int64')
             edges = None
             if 'edge' in data.keys():
                 edges = data['edge'].astype('int64')
+
             if hasattr(model, 'data_format') and model.data_format == 'NHWC':
                 images = images.transpose((0, 2, 3, 1))
 
             if precision == 'fp16':
                 with paddle.amp.auto_cast(
                         level=amp_level,
                         enable=True,
                         custom_white_list={
                             "elementwise_add", "batch_norm", "sync_batch_norm"
                         },
                         custom_black_list={'bilinear_interp_v2'}):
                     logits_list = ddp_model(images) if nranks > 1 else model(
                         images)
-                    loss_list = loss_computation(
-                        logits_list=logits_list,
-                        labels=labels,
-                        edges=edges,
-                        losses=losses)
+                    if nranks > 1 and hasattr(ddp_model._layers,
+                                              'loss_computation'):
+                        loss_list = ddp_model._layers.loss_computation(
+                            logits_list, losses, data)
+                    elif nranks == 1 and hasattr(model, 'loss_computation'):
+                        loss_list = model.loss_computation(logits_list, losses,
+                                                           data)
+                    else:
+                        loss_list = loss_computation(
+                            logits_list=logits_list,
+                            labels=labels,
+                            edges=edges,
+                            losses=losses)
                     loss = sum(loss_list)
 
                 scaled = scaler.scale(loss)  # scale the loss
                 scaled.backward()  # do backward
                 if isinstance(optimizer, paddle.distributed.fleet.Fleet):
                     scaler.minimize(optimizer.user_defined_optimizer, scaled)
                 else:
                     scaler.minimize(optimizer, scaled)  # update parameters
             else:
                 logits_list = ddp_model(images) if nranks > 1 else model(images)
-                loss_list = loss_computation(
-                    logits_list=logits_list,
-                    labels=labels,
-                    edges=edges,
-                    losses=losses)
+
+                if nranks > 1 and hasattr(ddp_model._layers,
+                                          'loss_computation'):
+                    loss_list = ddp_model._layers.loss_computation(logits_list,
+                                                                   losses, data)
+                elif nranks == 1 and hasattr(model, 'loss_computation'):
+                    loss_list = model.loss_computation(logits_list, losses,
+                                                       data)
+                else:
+                    loss_list = loss_computation(
+                        logits_list=logits_list,
+                        labels=labels,
+                        edges=edges,
+                        losses=losses)
                 loss = sum(loss_list)
                 loss.backward()
-                # if the optimizer is ReduceOnPlateau, the loss is the one which has been pass into step.
-                if isinstance(optimizer, paddle.optimizer.lr.ReduceOnPlateau):
-                    optimizer.step(loss)
-                else:
-                    optimizer.step()
+                optimizer.step()
 
             lr = optimizer.get_lr()
 
             # update lr
             if isinstance(optimizer, paddle.distributed.fleet.Fleet):
                 lr_sche = optimizer.user_defined_optimizer._learning_rate
             else:
                 lr_sche = optimizer._learning_rate
             if isinstance(lr_sche, paddle.optimizer.lr.LRScheduler):
-                lr_sche.step()
+                if isinstance(lr_sche, paddle.optimizer.lr.ReduceOnPlateau):
+                    lr_sche.step(loss)
+                else:
+                    lr_sche.step()
 
             train_profiler.add_profiler_step(profiler_options)
 
             model.clear_gradients()
             avg_loss += float(loss)
             if not avg_loss_list:
                 avg_loss_list = [l.numpy() for l in loss_list]
@@ -269,14 +298,17 @@
                     log_writer.add_scalar('Train/reader_cost',
                                           avg_train_reader_cost, iter)
                 avg_loss = 0.0
                 avg_loss_list = []
                 reader_cost_averager.reset()
                 batch_cost_averager.reset()
 
+            if use_ema:
+                update_ema_model(ema_model, model, step=iter)
+
             if (iter % save_interval == 0 or
                     iter == iters) and (val_dataset is not None):
                 num_workers = 1 if num_workers > 0 else 0
 
                 if test_config is None:
                     test_config = {}
 
@@ -284,25 +316,40 @@
                     model,
                     val_dataset,
                     num_workers=num_workers,
                     precision=precision,
                     amp_level=amp_level,
                     **test_config)
 
+                if use_ema:
+                    ema_mean_iou, ema_acc, _, _, _ = evaluate(
+                        ema_model,
+                        val_dataset,
+                        num_workers=num_workers,
+                        precision=precision,
+                        amp_level=amp_level,
+                        **test_config)
+
                 model.train()
 
             if (iter % save_interval == 0 or iter == iters) and local_rank == 0:
                 current_save_dir = os.path.join(save_dir,
                                                 "iter_{}".format(iter))
                 if not os.path.isdir(current_save_dir):
                     os.makedirs(current_save_dir)
                 paddle.save(model.state_dict(),
                             os.path.join(current_save_dir, 'model.pdparams'))
                 paddle.save(optimizer.state_dict(),
                             os.path.join(current_save_dir, 'model.pdopt'))
+
+                if use_ema:
+                    paddle.save(
+                        ema_model.state_dict(),
+                        os.path.join(current_save_dir, 'ema_model.pdparams'))
+
                 save_models.append(current_save_dir)
                 if len(save_models) > keep_checkpoint_max > 0:
                     model_to_remove = save_models.popleft()
                     shutil.rmtree(model_to_remove)
 
                 if val_dataset is not None:
                     if mean_iou > best_mean_iou:
@@ -311,18 +358,36 @@
                         best_model_dir = os.path.join(save_dir, "best_model")
                         paddle.save(
                             model.state_dict(),
                             os.path.join(best_model_dir, 'model.pdparams'))
                     logger.info(
                         '[EVAL] The model with the best validation mIoU ({:.4f}) was saved at iter {}.'
                         .format(best_mean_iou, best_model_iter))
+                    if use_ema:
+                        if ema_mean_iou > best_ema_mean_iou:
+                            best_ema_mean_iou = ema_mean_iou
+                            best_ema_model_iter = iter
+                            best_ema_model_dir = os.path.join(save_dir,
+                                                              "ema_best_model")
+                            paddle.save(ema_model.state_dict(),
+                                        os.path.join(best_ema_model_dir,
+                                                     'ema_model.pdparams'))
+                        logger.info(
+                            '[EVAL] The EMA model with the best validation mIoU ({:.4f}) was saved at iter {}.'
+                            .format(best_ema_mean_iou, best_ema_model_iter))
 
                     if use_vdl:
                         log_writer.add_scalar('Evaluate/mIoU', mean_iou, iter)
                         log_writer.add_scalar('Evaluate/Acc', acc, iter)
+
+                        if use_ema:
+                            log_writer.add_scalar('Evaluate/Ema_mIoU',
+                                                  ema_mean_iou, iter)
+                            log_writer.add_scalar('Evaluate/Ema_Acc', ema_acc,
+                                                  iter)
             batch_start = time.time()
 
     # Calculate flops.
     if local_rank == 0 and not (precision == 'fp16' and amp_level == 'O2'):
         _, c, h, w = images.shape
         _ = paddle.flops(
             model, [1, c, h, w],
```

## paddleseg/cvlibs/__init__.py

```diff
@@ -11,7 +11,8 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from . import manager
 from . import param_init
 from .config import Config
+from .builder import Builder, SegBuilder
```

## paddleseg/cvlibs/config.py

```diff
@@ -8,543 +8,226 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import six
 import codecs
 import os
-from typing import Any, Dict, Generic
-import warnings
 from ast import literal_eval
+from typing import Any, Dict, Optional
 
-import paddle
 import yaml
-import six
+import paddle
 
+from paddleseg.cvlibs import config_checker as checker
 from paddleseg.cvlibs import manager
-from paddleseg.utils import logger
+from paddleseg.utils import logger, utils
+
+_INHERIT_KEY = '_inherited_'
+_BASE_KEY = '_base_'
 
 
 class Config(object):
-    '''
-    Training configuration parsing. The only yaml/yml file is supported.
+    """
+    Configuration parsing.
 
     The following hyper-parameters are available in the config file:
         batch_size: The number of samples per gpu.
         iters: The total training steps.
         train_dataset: A training data config including type/data_root/transforms/mode.
             For data type, please refer to paddleseg.datasets.
             For specific transforms, please refer to paddleseg.transforms.transforms.
         val_dataset: A validation data config including type/data_root/transforms/mode.
-        optimizer: A optimizer config, but currently PaddleSeg only supports sgd with momentum in config file.
-            In addition, weight_decay could be set as a regularization.
-        learning_rate: A learning rate config. If decay is configured, learning _rate value is the starting learning rate,
-             where only poly decay is supported using the config file. In addition, decay power and end_lr are tuned experimentally.
-        loss: A loss config. Multi-loss config is available. The loss type order is consistent with the seg model outputs,
-            where the coef term indicates the weight of corresponding loss. Note that the number of coef must be the same as the number of
-            model outputs, and there could be only one loss type if using the same loss type among the outputs, otherwise the number of
+        optimizer: A optimizer config. Please refer to paddleseg.optimizers.
+        loss: A loss config. Multi-loss config is available. The loss type order is 
+            consistent with the seg model outputs, where the coef term indicates the 
+            weight of corresponding loss. Note that the number of coef must be the 
+            same as the number of model outputs, and there could be only one loss type 
+            if using the same loss type among the outputs, otherwise the number of
             loss type must be consistent with coef.
         model: A model config including type/backbone and model-dependent arguments.
             For model type, please refer to paddleseg.models.
             For backbone, please refer to paddleseg.models.backbones.
 
     Args:
         path (str) : The path of config file, supports yaml format only.
+        opts (list, optional): Use opts to update the key-value pairs of all options.
 
-    Examples:
+    """
 
-        from paddleseg.cvlibs.config import Config
-
-        # Create a cfg object with yaml file path.
-        cfg = Config(yaml_cfg_path)
-
-        # Parsing the argument when its property is used.
-        train_dataset = cfg.train_dataset
-
-        # the argument of model should be parsed after dataset,
-        # since the model builder uses some properties in dataset.
-        model = cfg.model
-        ...
-    '''
-
-    def __init__(self,
-                 path: str,
-                 learning_rate: float=None,
-                 batch_size: int=None,
-                 iters: int=None,
-                 opts: list=None):
-        if not path:
-            raise ValueError('Please specify the configuration file path.')
-
-        if not os.path.exists(path):
-            raise FileNotFoundError('File {} does not exist'.format(path))
-
-        self._model = None
-        self._losses = None
-        if path.endswith('yml') or path.endswith('yaml'):
-            self.dic = self._parse_from_yaml(path)
-        else:
-            raise RuntimeError('Config file should in yaml format!')
-
-        self.update(
+    def __init__(
+            self,
+            path: str,
+            learning_rate: Optional[float]=None,
+            batch_size: Optional[int]=None,
+            iters: Optional[int]=None,
+            opts: Optional[list]=None,
+            checker: Optional[checker.ConfigChecker]=None, ):
+        assert os.path.exists(path), \
+            'Config path ({}) does not exist'.format(path)
+        assert path.endswith('yml') or path.endswith('yaml'), \
+            'Config file ({}) should be yaml format'.format(path)
+
+        self.dic = self._parse_from_yaml(path)
+        self.dic = self.update_config_dict(
+            self.dic,
             learning_rate=learning_rate,
             batch_size=batch_size,
             iters=iters,
             opts=opts)
 
-        model_cfg = self.dic.get('model', None)
-        if model_cfg is None:
-            raise RuntimeError('No model specified in the configuration file.')
-        if (not self.train_dataset_config) and (not self.val_dataset_config):
-            raise ValueError(
-                'One of `train_dataset` or `val_dataset should be given, but there are none.'
-            )
-
-    def _update_dic(self, dic, base_dic):
-        """
-        Update config from dic based base_dic
-        """
-        base_dic = base_dic.copy()
-        dic = dic.copy()
-
-        if dic.get('_inherited_', True) == False:
-            dic.pop('_inherited_')
-            return dic
-
-        for key, val in dic.items():
-            if isinstance(val, dict) and key in base_dic:
-                base_dic[key] = self._update_dic(val, base_dic[key])
-            else:
-                base_dic[key] = val
-        dic = base_dic
-        return dic
-
-    def _parse_from_yaml(self, path: str):
-        '''Parse a yaml file and build config'''
-        with codecs.open(path, 'r', 'utf-8') as file:
-            dic = yaml.load(file, Loader=yaml.FullLoader)
-
-        if '_base_' in dic:
-            cfg_dir = os.path.dirname(path)
-            base_path = dic.pop('_base_')
-            base_path = os.path.join(cfg_dir, base_path)
-            base_dic = self._parse_from_yaml(base_path)
-            dic = self._update_dic(dic, base_dic)
-        return dic
-
-    def update(self,
-               learning_rate: float=None,
-               batch_size: int=None,
-               iters: int=None,
-               opts: list=None):
-        '''Update config'''
-        if learning_rate:
-            if 'lr_scheduler' in self.dic:
-                self.dic['lr_scheduler']['learning_rate'] = learning_rate
-            else:
-                self.dic['learning_rate']['value'] = learning_rate
-
-        if batch_size:
-            self.dic['batch_size'] = batch_size
-
-        if iters:
-            self.dic['iters'] = iters
-
-        # fix parameters by --opts of command
-        if opts is not None:
-            if len(opts) % 2 != 0 or len(opts) == 0:
-                raise ValueError(
-                    "Command line options config `--opts` format error! It should be even length like: k1 v1 k2 v2 ... Please check it: {}".
-                    format(opts))
-            for key, value in zip(opts[0::2], opts[1::2]):
-                if isinstance(value, six.string_types):
-                    try:
-                        value = literal_eval(value)
-                    except ValueError:
-                        pass
-                    except SyntaxError:
-                        pass
-                key_list = key.split('.')
-                dic = self.dic
-                for subkey in key_list[:-1]:
-                    dic.setdefault(subkey, dict())
-                    dic = dic[subkey]
-                dic[key_list[-1]] = value
+        if checker is None:
+            checker = self._build_default_checker()
+        checker.apply_all_rules(self)
 
     @property
     def batch_size(self) -> int:
-        return self.dic.get('batch_size', 1)
+        return self.dic.get('batch_size')
 
     @property
     def iters(self) -> int:
-        iters = self.dic.get('iters')
-        if not iters:
-            raise RuntimeError('No iters specified in the configuration file.')
-        return iters
-
-    @property
-    def lr_scheduler(self) -> paddle.optimizer.lr.LRScheduler:
-        if 'lr_scheduler' not in self.dic:
-            raise RuntimeError(
-                'No `lr_scheduler` specified in the configuration file.')
-        params = self.dic.get('lr_scheduler')
-
-        use_warmup = False
-        if 'warmup_iters' in params:
-            use_warmup = True
-            warmup_iters = params.pop('warmup_iters')
-            assert 'warmup_start_lr' in params, \
-                "When use warmup, please set warmup_start_lr and warmup_iters in lr_scheduler"
-            warmup_start_lr = params.pop('warmup_start_lr')
-            end_lr = params['learning_rate']
-
-        lr_type = params.pop('type')
-        if lr_type == 'PolynomialDecay':
-            iters = self.iters - warmup_iters if use_warmup else self.iters
-            iters = max(iters, 1)
-            params.setdefault('decay_steps', iters)
-            params.setdefault('end_lr', 0)
-            params.setdefault('power', 0.9)
-        lr_sche = getattr(paddle.optimizer.lr, lr_type)(**params)
-
-        if use_warmup:
-            lr_sche = paddle.optimizer.lr.LinearWarmup(
-                learning_rate=lr_sche,
-                warmup_steps=warmup_iters,
-                start_lr=warmup_start_lr,
-                end_lr=end_lr)
-
-        return lr_sche
-
-    @property
-    def learning_rate(self) -> paddle.optimizer.lr.LRScheduler:
-        logger.warning(
-            '''`learning_rate` in configuration file will be deprecated, please use `lr_scheduler` instead. E.g
-            lr_scheduler:
-                type: PolynomialDecay
-                learning_rate: 0.01''')
-
-        _learning_rate = self.dic.get('learning_rate', {})
-        if isinstance(_learning_rate, float):
-            return _learning_rate
-
-        _learning_rate = self.dic.get('learning_rate', {}).get('value')
-        if not _learning_rate:
-            raise RuntimeError(
-                'No learning rate specified in the configuration file.')
-
-        args = self.decay_args
-        decay_type = args.pop('type')
-
-        if decay_type == 'poly':
-            lr = _learning_rate
-            return paddle.optimizer.lr.PolynomialDecay(lr, **args)
-        elif decay_type == 'piecewise':
-            values = _learning_rate
-            return paddle.optimizer.lr.PiecewiseDecay(values=values, **args)
-        elif decay_type == 'stepdecay':
-            lr = _learning_rate
-            return paddle.optimizer.lr.StepDecay(lr, **args)
-        else:
-            raise RuntimeError('Only poly and piecewise decay support.')
+        return self.dic.get('iters')
 
     @property
-    def optimizer(self) -> paddle.optimizer.Optimizer:
-        if 'lr_scheduler' in self.dic:
-            lr = self.lr_scheduler
-        else:
-            lr = self.learning_rate
-        args = self.optimizer_args
-        optimizer_type = args.pop('type')
-
-        params = self.model.parameters()
-        if 'backbone_lr_mult' in args:
-            if not hasattr(self.model, 'backbone'):
-                logger.warning('The backbone_lr_mult is not effective because'
-                               ' the model does not have backbone')
-            else:
-                backbone_lr_mult = args.pop('backbone_lr_mult')
-                backbone_params = self.model.backbone.parameters()
-                backbone_params_id = [id(x) for x in backbone_params]
-                other_params = [
-                    x for x in params if id(x) not in backbone_params_id
-                ]
-                params = [{
-                    'params': backbone_params,
-                    'learning_rate': backbone_lr_mult
-                }, {
-                    'params': other_params
-                }]
-
-        if optimizer_type == 'sgd':
-            return paddle.optimizer.Momentum(lr, parameters=params, **args)
-        elif optimizer_type == 'adam':
-            return paddle.optimizer.Adam(lr, parameters=params, **args)
-        elif optimizer_type in paddle.optimizer.__all__:
-            return getattr(paddle.optimizer, optimizer_type)(lr,
-                                                             parameters=params,
-                                                             **args)
-
-        raise RuntimeError('Unknown optimizer type {}.'.format(optimizer_type))
-
-    @property
-    def optimizer_args(self) -> dict:
-        args = self.dic.get('optimizer', {}).copy()
-        if args['type'] == 'sgd':
-            args.setdefault('momentum', 0.9)
-
-        return args
-
-    @property
-    def decay_args(self) -> dict:
-        args = self.dic.get('learning_rate', {}).get(
-            'decay', {'type': 'poly',
-                      'power': 0.9}).copy()
-
-        if args['type'] == 'poly':
-            args.setdefault('decay_steps', self.iters)
-            args.setdefault('end_lr', 0)
-
-        return args
-
-    @property
-    def loss(self) -> dict:
-        if self._losses is None:
-            self._losses = self._prepare_loss('loss')
-        return self._losses
-
-    @property
-    def distill_loss(self) -> dict:
-        if not hasattr(self, '_distill_losses'):
-            self._distill_losses = self._prepare_loss('distill_loss')
-        return self._distill_losses
-
-    def _prepare_loss(self, loss_name):
-        """
-        Parse the loss parameters and load the loss layers.
-
-        Args:
-            loss_name (str): The root name of loss in the yaml file.
-        Returns:
-            dict: A dict including the loss parameters and layers.
-        """
-        args = self.dic.get(loss_name, {}).copy()
-        if 'types' in args and 'coef' in args:
-            len_types = len(args['types'])
-            len_coef = len(args['coef'])
-            if len_types != len_coef:
-                if len_types == 1:
-                    args['types'] = args['types'] * len_coef
-                else:
-                    raise ValueError(
-                        'The length of types should equal to coef or equal to 1 in loss config, but they are {} and {}.'
-                        .format(len_types, len_coef))
-        else:
-            raise ValueError(
-                'Loss config should contain keys of "types" and "coef"')
-
-        losses = dict()
-        for key, val in args.items():
-            if key == 'types':
-                losses['types'] = []
-                for item in args['types']:
-                    if item['type'] != 'MixedLoss':
-                        if 'ignore_index' in item:
-                            assert item['ignore_index'] == self.train_dataset.ignore_index, 'If ignore_index of loss is set, '\
-                            'the ignore_index of loss and train_dataset must be the same. \nCurrently, loss ignore_index = {}, '\
-                            'train_dataset ignore_index = {}. \nIt is recommended not to set loss ignore_index, so it is consistent with '\
-                            'train_dataset by default.'.format(item['ignore_index'], self.train_dataset.ignore_index)
-                        item['ignore_index'] = \
-                            self.train_dataset.ignore_index
-                    losses['types'].append(self._load_object(item))
-            else:
-                losses[key] = val
-        if len(losses['coef']) != len(losses['types']):
-            raise RuntimeError(
-                'The length of coef should equal to types in loss config: {} != {}.'
-                .format(len(losses['coef']), len(losses['types'])))
-        return losses
-
-    @property
-    def model(self) -> paddle.nn.Layer:
-        model_cfg = self.dic.get('model').copy()
-        if not self._model:
-            self._model = self._load_object(model_cfg)
-        return self._model
+    def to_static_training(self) -> bool:
+        return self.dic.get('to_static_training', False)
 
     @property
-    def train_dataset_config(self) -> Dict:
-        return self.dic.get('train_dataset', {}).copy()
+    def model_cfg(self) -> Dict:
+        return self.dic.get('model', {}).copy()
 
     @property
-    def val_dataset_config(self) -> Dict:
-        return self.dic.get('val_dataset', {}).copy()
+    def loss_cfg(self) -> Dict:
+        return self.dic.get('loss', {}).copy()
 
     @property
-    def train_dataset_class(self) -> Generic:
-        dataset_type = self.train_dataset_config['type']
-        return self._load_component(dataset_type)
-
-    @property
-    def val_dataset_class(self) -> Generic:
-        dataset_type = self.val_dataset_config['type']
-        return self._load_component(dataset_type)
-
-    @property
-    def train_dataset(self) -> paddle.io.Dataset:
-        _train_dataset = self.train_dataset_config
-        if not _train_dataset:
-            return None
-        return self._load_object(_train_dataset)
-
-    @property
-    def val_dataset(self) -> paddle.io.Dataset:
-        _val_dataset = self.val_dataset_config
-        if not _val_dataset:
-            return None
-        return self._load_object(_val_dataset)
-
-    def _load_component(self, com_name: str) -> Any:
-        com_list = [
-            manager.MODELS, manager.BACKBONES, manager.DATASETS,
-            manager.TRANSFORMS, manager.LOSSES
-        ]
-
-        for com in com_list:
-            if com_name in com.components_dict:
-                return com[com_name]
-        else:
-            raise RuntimeError(
-                'The specified component was not found {}.'.format(com_name))
+    def distill_loss_cfg(self) -> Dict:
+        return self.dic.get('distill_loss', {}).copy()
 
-    def _load_object(self, cfg: dict) -> Any:
-        cfg = cfg.copy()
-        if 'type' not in cfg:
-            raise RuntimeError('No object information in {}.'.format(cfg))
-
-        component = self._load_component(cfg.pop('type'))
-
-        params = {}
-        for key, val in cfg.items():
-            if self._is_meta_type(val):
-                params[key] = self._load_object(val)
-            elif isinstance(val, list):
-                params[key] = [
-                    self._load_object(item)
-                    if self._is_meta_type(item) else item for item in val
-                ]
-            else:
-                params[key] = val
+    @property
+    def lr_scheduler_cfg(self) -> Dict:
+        return self.dic.get('lr_scheduler', {}).copy()
 
-        return component(**params)
+    @property
+    def optimizer_cfg(self) -> Dict:
+        return self.dic.get('optimizer', {}).copy()
 
     @property
-    def test_config(self) -> Dict:
-        return self.dic.get('test_config', {})
+    def train_dataset_cfg(self) -> Dict:
+        return self.dic.get('train_dataset', {}).copy()
 
     @property
-    def export_config(self) -> Dict:
-        return self.dic.get('export', {})
+    def val_dataset_cfg(self) -> Dict:
+        return self.dic.get('val_dataset', {}).copy()
 
+    # TODO merge test_config into val_dataset
     @property
-    def to_static_training(self) -> bool:
-        '''Whether to use @to_static for training'''
-        return self.dic.get('to_static_training', False)
+    def test_config(self) -> Dict:
+        return self.dic.get('test_config', {}).copy()
+
+    @classmethod
+    def update_config_dict(cls, dic: dict, *args, **kwargs) -> dict:
+        return update_config_dict(dic, *args, **kwargs)
+
+    @classmethod
+    def _parse_from_yaml(cls, path: str, *args, **kwargs) -> dict:
+        return parse_from_yaml(path, *args, **kwargs)
+
+    @classmethod
+    def _build_default_checker(cls):
+        rules = []
+        rules.append(checker.DefaultPrimaryRule())
+        rules.append(checker.DefaultSyncNumClassesRule())
+        rules.append(checker.DefaultSyncImgChannelsRule())
+        # Losses
+        rules.append(checker.DefaultLossRule('loss'))
+        rules.append(checker.DefaultSyncIgnoreIndexRule('loss'))
+        # Distillation losses
+        rules.append(checker.DefaultLossRule('distill_loss'))
+        rules.append(checker.DefaultSyncIgnoreIndexRule('distill_loss'))
 
-    def _is_meta_type(self, item: Any) -> bool:
-        return isinstance(item, dict) and 'type' in item
+        return checker.ConfigChecker(rules, allow_update=True)
 
     def __str__(self) -> str:
-        return yaml.dump(self.dic)
+        # Use NoAliasDumper to avoid yml anchor 
+        return yaml.dump(self.dic, Dumper=utils.NoAliasDumper)
 
-    @property
-    def val_transforms(self) -> list:
-        """Get val_transform from val_dataset"""
-        _val_dataset = self.val_dataset_config
-        if not _val_dataset:
-            return []
-        _transforms = _val_dataset.get('transforms', [])
-        transforms = []
-        for i in _transforms:
-            transforms.append(self._load_object(i))
-        return transforms
-
-    def check_sync_info(self) -> None:
-        """
-        Check and sync the info, such as num_classes and img_channels, 
-        between the config of model, train_dataset and val_dataset.
-        """
-        self._check_sync_num_classes()
-        self._check_sync_img_channels()
-
-    def _check_sync_num_classes(self):
-        num_classes_set = set()
-
-        if self.dic['model'].get('num_classes', None) is not None:
-            num_classes_set.add(self.dic['model'].get('num_classes'))
-        if self.train_dataset_config:
-            if hasattr(self.train_dataset_class, 'NUM_CLASSES'):
-                num_classes_set.add(self.train_dataset_class.NUM_CLASSES)
-            elif 'num_classes' in self.train_dataset_config:
-                num_classes_set.add(self.train_dataset_config['num_classes'])
-        if self.val_dataset_config:
-            if hasattr(self.val_dataset_class, 'NUM_CLASSES'):
-                num_classes_set.add(self.val_dataset_class.NUM_CLASSES)
-            elif 'num_classes' in self.val_dataset_config:
-                num_classes_set.add(self.val_dataset_config['num_classes'])
-
-        if len(num_classes_set) == 0:
-            raise ValueError(
-                '`num_classes` is not found. Please set it in model, train_dataset or val_dataset'
-            )
-        elif len(num_classes_set) > 1:
-            raise ValueError(
-                '`num_classes` is not consistent: {}. Please set it consistently in model or train_dataset or val_dataset'
-                .format(num_classes_set))
-
-        num_classes = num_classes_set.pop()
-        self.dic['model']['num_classes'] = num_classes
-        if self.train_dataset_config and \
-            (not hasattr(self.train_dataset_class, 'NUM_CLASSES')):
-            self.dic['train_dataset']['num_classes'] = num_classes
-        if self.val_dataset_config and \
-            (not hasattr(self.val_dataset_class, 'NUM_CLASSES')):
-            self.dic['val_dataset']['num_classes'] = num_classes
-
-    def _check_sync_img_channels(self):
-        img_channels_set = set()
-        model_cfg = self.dic['model']
-
-        # If the model has backbone, in_channels is the input params of backbone.
-        # Otherwise, in_channels is the input params of the model.
-        if 'backbone' in model_cfg:
-            x = model_cfg['backbone'].get('in_channels', None)
-            if x is not None:
-                img_channels_set.add(x)
-        elif model_cfg.get('in_channels', None) is not None:
-            img_channels_set.add(model_cfg.get('in_channels'))
-        if self.train_dataset_config and \
-            ('img_channels' in self.train_dataset_config):
-            img_channels_set.add(self.train_dataset_config['img_channels'])
-        if self.val_dataset_config and \
-            ('img_channels' in self.val_dataset_config):
-            img_channels_set.add(self.val_dataset_config['img_channels'])
-
-        if len(img_channels_set) > 1:
-            raise ValueError(
-                '`img_channels` is not consistent: {}. Please set it consistently in model or train_dataset or val_dataset'
-                .format(img_channels_set))
-
-        img_channels = 3 if len(img_channels_set) == 0 \
-            else img_channels_set.pop()
-        if 'backbone' in model_cfg:
-            self.dic['model']['backbone']['in_channels'] = img_channels
-        else:
-            self.dic['model']['in_channels'] = img_channels
-        if self.train_dataset_config and \
-            self.train_dataset_config['type'] == "Dataset":
-            self.dic['train_dataset']['img_channels'] = img_channels
-        if self.val_dataset_config and \
-            self.val_dataset_config['type'] == "Dataset":
-            self.dic['val_dataset']['img_channels'] = img_channels
+
+def parse_from_yaml(path: str):
+    """Parse a yaml file and build config"""
+    with codecs.open(path, 'r', 'utf-8') as file:
+        dic = yaml.load(file, Loader=yaml.FullLoader)
+
+    if _BASE_KEY in dic:
+        base_files = dic.pop(_BASE_KEY)
+        if isinstance(base_files, str):
+            base_files = [base_files]
+        for bf in base_files:
+            base_path = os.path.join(os.path.dirname(path), bf)
+            base_dic = parse_from_yaml(base_path)
+            dic = merge_config_dicts(dic, base_dic)
+
+    return dic
+
+
+def merge_config_dicts(dic, base_dic):
+    """Merge dic to base_dic and return base_dic."""
+    base_dic = base_dic.copy()
+    dic = dic.copy()
+
+    if not dic.get(_INHERIT_KEY, True):
+        dic.pop(_INHERIT_KEY)
+        return dic
+
+    for key, val in dic.items():
+        if isinstance(val, dict) and key in base_dic:
+            base_dic[key] = merge_config_dicts(val, base_dic[key])
+        else:
+            base_dic[key] = val
+
+    return base_dic
+
+
+def update_config_dict(dic: dict,
+                       learning_rate: Optional[float]=None,
+                       batch_size: Optional[int]=None,
+                       iters: Optional[int]=None,
+                       opts: Optional[list]=None):
+    """Update config"""
+    # TODO: If the items to update are marked as anchors in the yaml file,
+    # we should synchronize the references.
+    dic = dic.copy()
+
+    if learning_rate:
+        dic['lr_scheduler']['learning_rate'] = learning_rate
+    if batch_size:
+        dic['batch_size'] = batch_size
+    if iters:
+        dic['iters'] = iters
+
+    if opts is not None:
+        for item in opts:
+            assert ('=' in item) and (len(item.split('=')) == 2), "--opts params should be key=value," \
+                " such as `--opts batch_size=1 test_config.scales=0.75,1.0,1.25`, " \
+                "but got ({})".format(opts)
+
+            key, value = item.split('=')
+            if isinstance(value, six.string_types):
+                try:
+                    value = literal_eval(value)
+                except ValueError:
+                    pass
+                except SyntaxError:
+                    pass
+            key_list = key.split('.')
+
+            tmp_dic = dic
+            for subkey in key_list[:-1]:
+                assert subkey in tmp_dic, "Can not update {}, because it is not in config.".format(
+                    key)
+                tmp_dic = tmp_dic[subkey]
+            tmp_dic[key_list[-1]] = value
+
+    return dic
```

## paddleseg/cvlibs/manager.py

```diff
@@ -141,7 +141,8 @@
 
 
 MODELS = ComponentManager("models")
 BACKBONES = ComponentManager("backbones")
 DATASETS = ComponentManager("datasets")
 TRANSFORMS = ComponentManager("transforms")
 LOSSES = ComponentManager("losses")
+OPTIMIZERS = ComponentManager("optimizers")
```

## paddleseg/cvlibs/param_init.py

```diff
@@ -9,14 +9,37 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import paddle.nn as nn
+import math
+
+
+def uniform_init(param, **kwargs):
+    """
+    Initialize the `param` with uniform distribution.
+
+    Args:
+        param (Tensor): Tensor that needs to be initialized.
+
+    Examples:
+
+        from paddleseg.cvlibs import param_init
+        import paddle.nn as nn
+
+        linear = nn.Linear(2, 2)
+        param_init.uniform_init(linear.bias,  low=-0.5, high=0。5)
+        print(linear.bias.numpy())
+        # result is [-0.2734719   0.23939109]
+
+    """
+    initializer = nn.initializer.Uniform(**kwargs)
+    initializer(param, param.block)
 
 
 def constant_init(param, **kwargs):
     """
     Initialize the `param` with constants.
 
     Args:
@@ -159,8 +182,56 @@
         from paddleseg.cvlibs import param_init
         import paddle.nn as nn
 
         linear = nn.Linear(2, 4)
         param_init.xavier_uniform(linear.weight)
     """
     initializer = nn.initializer.XavierUniform(**kwargs)
-    initializer(param, param.block)
+    initializer(param, param.block)
+
+
+def multihead_fill(layer, qkv_same_embed_dim=True):
+    """
+    The default initialization of multi-head attention.
+
+    Example:
+        from paddleseg.cvlibs import param_init
+        import paddle.nn as nn
+        
+        self_attn = nn.MultiHeadAttention(
+            128, 8, dropout=False)
+        param_init.multihead_fill(self_attn, True)
+    """
+
+    def _init_param_as_combined_linear_weight(p):
+        bound = math.sqrt(6 / (3 * p.shape[0] + p.shape[1]))
+        nn.initializer.Uniform(low=-bound, high=bound)(p)
+
+    if qkv_same_embed_dim:
+        _init_param_as_combined_linear_weight(layer.q_proj.weight)
+        _init_param_as_combined_linear_weight(layer.k_proj.weight)
+        _init_param_as_combined_linear_weight(layer.v_proj.weight)
+        xavier_uniform(layer.out_proj.weight)
+    else:
+        for p in layer.parameters():
+            if p.dim() > 1:
+                xavier_uniform(p)
+
+
+def th_linear_fill(layer):
+    """
+    The default way of linear initialization.
+    
+    Example:
+        from paddleseg.cvlibs import param_init
+        import paddle.nn as nn
+        
+        linear = nn.Linear(128, 128)
+        param_init.linear_fill(linear)
+    """
+    nn.initializer.KaimingUniform(
+        negative_slope=math.sqrt(5), nonlinearity='leaky_relu')(layer.weight)
+
+    if getattr(layer, 'bias', None) is not None:
+        fan_in = layer.weight.shape[0]
+        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
+        nn.initializer.Uniform(low=-bound, high=bound)(layer.bias)
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## paddleseg/datasets/ade.py

```diff
@@ -35,23 +35,25 @@
     Args:
         transforms (list): A list of image transformations.
         dataset_root (str, optional): The ADK20K dataset directory. Default: None.
         mode (str, optional): A subset of the entire dataset. It should be one of ('train', 'val'). Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False
     """
     NUM_CLASSES = 150
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self, transforms, dataset_root=None, mode='train', edge=False):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         mode = mode.lower()
         self.mode = mode
         self.file_list = list()
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
         self.edge = edge
 
         if mode not in ['train', 'val']:
             raise ValueError(
                 "`mode` should be one of ('train', 'val') in ADE20K dataset, but got {}."
                 .format(mode))
 
@@ -93,27 +95,25 @@
         data['trans_info'] = []
         image_path, label_path = self.file_list[idx]
         data['img'] = image_path
         data['gt_fields'] = [
         ]  # If key in gt_fields, the data[key] have transforms synchronous.
 
         if self.mode == 'val':
-            data = self.transforms(data)
             label = np.asarray(Image.open(label_path))
             # The class 0 is ignored. And it will equal to 255 after
             # subtracted 1, because the dtype of label is uint8.
             label = label - 1
+            data = self.transforms(data)
             label = label[np.newaxis, :, :]
             data['label'] = label
             return data
         else:
-            data['label'] = label_path
+            data['label'] = np.asarray(Image.open(label_path))
             data['gt_fields'].append('label')
-            data = self.transforms(data)
             data['label'] = data['label'] - 1
-            # Recover the ignore pixels adding by transform
-            data['label'][data['label'] == 254] = 255
+            data = self.transforms(data)
             if self.edge:
                 edge_mask = F.mask_to_binary_edge(
                     label, radius=2, num_classes=self.num_classes)
                 data['edge'] = edge_mask
-            return data
+            return data
```

## paddleseg/datasets/chase_db1.py

```diff
@@ -35,28 +35,30 @@
     Args:
         transforms (list): Transforms for image.
         dataset_root (str): The dataset directory. Default: None
         edge (bool): whether extract edge infor in the output
         mode (str, optional): Which part of dataset to use. it is one of ('train', 'val', 'test'). Default: 'train'.
     """
     NUM_CLASSES = 2
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  dataset_root=None,
                  transforms=None,
                  edge=False,
                  mode='train'):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         mode = mode.lower()
         self.mode = mode
         self.edge = edge
         self.file_list = list()
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255  # labels only have 1/0, thus ignore_index is not necessary
+        self.ignore_index = self.IGNORE_INDEX  # labels only have 1/0, thus ignore_index is not necessary
 
         if mode not in ['train', 'val', 'test']:
             raise ValueError(
                 "`mode` should be 'train', 'val' or 'test', but got {}.".format(
                     mode))
 
         if self.transforms is None:
```

## paddleseg/datasets/cityscapes.py

```diff
@@ -43,23 +43,25 @@
     Args:
         transforms (list): Transforms for image.
         dataset_root (str): Cityscapes dataset directory.
         mode (str, optional): Which part of dataset to use. it is one of ('train', 'val', 'test'). Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False
     """
     NUM_CLASSES = 19
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self, transforms, dataset_root, mode='train', edge=False):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         self.file_list = list()
         mode = mode.lower()
         self.mode = mode
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
         self.edge = edge
 
         if mode not in ['train', 'val', 'test']:
             raise ValueError(
                 "mode should be 'train', 'val' or 'test', but got {}.".format(
                     mode))
```

## paddleseg/datasets/cocostuff.py

```diff
@@ -40,23 +40,25 @@
     Args:
         transforms (list): Transforms for image.
         dataset_root (str): Cityscapes dataset directory.
         mode (str): Which part of dataset to use. it is one of ('train', 'val'). Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False
     """
     NUM_CLASSES = 171
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self, transforms, dataset_root, mode='train', edge=False):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         self.file_list = list()
         mode = mode.lower()
         self.mode = mode
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
         self.edge = edge
 
         if mode not in ['train', 'val']:
             raise ValueError(
                 "mode should be 'train', 'val', but got {}.".format(mode))
 
         if self.transforms is None:
```

## paddleseg/datasets/drive.py

```diff
@@ -33,28 +33,30 @@
     Args:
         transforms (list): Transforms for image.
         dataset_root (str): The dataset directory. Default: None
         edge (bool): whether extract edge infor in the output
         mode (str, optional): Which part of dataset to use. it is one of ('train', 'val', 'test'). Default: 'train'.
     """
     NUM_CLASSES = 2
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  dataset_root=None,
                  transforms=None,
                  edge=False,
                  mode='train'):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         mode = mode.lower()
         self.mode = mode
         self.edge = edge
         self.file_list = list()
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255  # labels only have 1/0, thus ignore_index is not necessary
+        self.ignore_index = self.IGNORE_INDEX  # labels only have 1/0, thus ignore_index is not necessary
 
         if mode not in ['train', 'val', 'test']:
             raise ValueError(
                 "`mode` should be 'train', 'val' or 'test', but got {}.".format(
                     mode))
 
         if self.transforms is None:
```

## paddleseg/datasets/eg1800.py

```diff
@@ -38,14 +38,16 @@
         transforms1 (list): A list of image transformations for the first input of portrait net.
         transforms2 (list): A list of image transformations for the second input of portrait net.
         dataset_root (str, optional): The EG1800 dataset directory. Default: None.
         mode (str, optional): A subset of the entire dataset. It should be one of ('train', 'val'). Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False
     """
     NUM_CLASSES = 2
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  common_transforms,
                  transforms1,
                  transforms2,
                  dataset_root=None,
                  mode='train',
@@ -54,15 +56,15 @@
         self.common_transforms = Compose(common_transforms)
         self.transforms = self.common_transforms
         if transforms1 is not None:
             self.transforms1 = Compose(transforms1, to_rgb=False)
         if transforms2 is not None:
             self.transforms2 = Compose(transforms2, to_rgb=False)
         mode = mode.lower()
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
         self.mode = mode
         self.num_classes = self.NUM_CLASSES
         self.input_width = 224
         self.input_height = 224
 
         if self.dataset_root is None:
             self.dataset_root = download_file_and_uncompress(
@@ -101,24 +103,27 @@
     def __getitem__(self, item):
         image_path, label_path = self.file_list[item]
         im = cv2.imread(image_path)
         label = cv2.imread(label_path, 0)
         label[label > 1] = 0
 
         if self.mode == "val":
-            common_im, label = self.common_transforms(im=im, label=label)
+            common_data = self.common_transforms(dict(img=im, label=label))
+            common_im, label = common_data['img'], common_data['label']
             im = np.float32(common_im[::-1, :, :])  # RGB => BGR
             im_aug = copy.deepcopy(im)
         else:
-            common_im, label = self.common_transforms(im=im, label=label)
+            common_data = self.common_transforms(dict(img=im, label=label))
+            common_im, label = common_data['img'], common_data['label']
             common_im = np.transpose(common_im, [1, 2, 0])
             # add augmentation
-            im, _ = self.transforms1(common_im)
-            im_aug, _ = self.transforms2(common_im)
-
+            data = self.transforms1(dict(img=common_im))
+            im = data['img']
+            data = self.transforms2(dict(img=common_im))
+            im_aug = data['img']
             im = np.float32(im[::-1, :, :])  # RGB => BGR
             im_aug = np.float32(im_aug[::-1, :, :])  # RGB => BGR
 
         label = cv2.resize(
             np.uint8(label), (self.input_width, self.input_height),
             interpolation=cv2.INTER_NEAREST)
 
@@ -126,12 +131,13 @@
         label = np.uint8(cv2.blur(label, (5, 5)))
         label[label >= 0.5] = 1
         label[label < 0.5] = 0
 
         edge_mask = F.mask_to_binary_edge(
             label, radius=4, num_classes=self.num_classes)
         edge_mask = np.transpose(edge_mask, [1, 2, 0]).squeeze(axis=-1)
-        im = np.concatenate([im_aug, im])
+        #im = np.concatenate([im_aug, im])
+        #im = im_aug
         if self.mode == "train":
-            return im, label, edge_mask
+            return dict(img=im, label=label, edge=edge_mask)
         else:
-            return im, label
+            return dict(img=im, label=label)
```

## paddleseg/datasets/hrf.py

```diff
@@ -32,28 +32,30 @@
     Args:
         transforms (list): Transforms for image.
         dataset_root (str): The dataset directory. Default: None
         edge (bool): whether extract edge infor in the output
         mode (str, optional): Which part of dataset to use. it is one of ('train', 'val', 'test'). Default: 'train'.
     """
     NUM_CLASSES = 2
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  dataset_root=None,
                  transforms=None,
                  edge=False,
                  mode='train'):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         mode = mode.lower()
         self.mode = mode
         self.edge = edge
         self.file_list = list()
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
 
         if mode not in ['train', 'val', 'test']:
             raise ValueError(
                 "`mode` should be 'train', 'val' or 'test', but got {}.".format(
                     mode))
 
         if self.transforms is None:
```

## paddleseg/datasets/mini_deep_globe_road_extraction.py

```diff
@@ -33,27 +33,29 @@
     Args:
         dataset_root (str, optional): The dataset directory. Default: None.
         transforms (list, optional): Transforms for image. Default: None.
         mode (str, optional): Which part of dataset to use. It is one of ('train', 'val'). Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False.
     """
     NUM_CLASSES = 2
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  dataset_root=None,
                  transforms=None,
                  mode='train',
                  edge=False):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         mode = mode.lower()
         self.mode = mode
         self.file_list = list()
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
         self.edge = edge
 
         if mode not in ['train', 'val']:
             raise ValueError(
                 "`mode` should be 'train' or 'val', but got {}.".format(mode))
 
         if self.transforms is None:
```

## paddleseg/datasets/optic_disc_seg.py

```diff
@@ -32,27 +32,29 @@
     Args:
         transforms (list): Transforms for image.
         dataset_root (str): The dataset directory. Default: None
         mode (str, optional): Which part of dataset to use. it is one of ('train', 'val', 'test'). Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False
     """
     NUM_CLASSES = 2
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  dataset_root=None,
                  transforms=None,
                  mode='train',
                  edge=False):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         mode = mode.lower()
         self.mode = mode
         self.file_list = list()
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
         self.edge = edge
 
         if mode not in ['train', 'val', 'test']:
             raise ValueError(
                 "`mode` should be 'train', 'val' or 'test', but got {}.".format(
                     mode))
```

## paddleseg/datasets/pascal_context.py

```diff
@@ -30,27 +30,29 @@
         transforms (list): Transforms for image.
         dataset_root (str): The dataset directory. Default: None
         mode (str): Which part of dataset to use. it is one of ('train', 'trainval', 'context', 'val').
             If you want to set mode to 'context', please make sure the dataset have been augmented. Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False
     """
     NUM_CLASSES = 60
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  transforms=None,
                  dataset_root=None,
                  mode='train',
                  edge=False):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         mode = mode.lower()
         self.mode = mode
         self.file_list = list()
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
         self.edge = edge
 
         if mode not in ['train', 'trainval', 'val']:
             raise ValueError(
                 "`mode` should be one of ('train', 'trainval', 'val') in PascalContext dataset, but got {}."
                 .format(mode))
```

## paddleseg/datasets/pp_humanseg14k.py

```diff
@@ -32,27 +32,29 @@
     Args:
         dataset_root (str, optional): The dataset directory. Default: None.
         transforms (list, optional): Transforms for image. Default: None.
         mode (str, optional): Which part of dataset to use. It is one of ('train', 'val'). Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False.
     """
     NUM_CLASSES = 2
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  dataset_root=None,
                  transforms=None,
                  mode='train',
                  edge=False):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         mode = mode.lower()
         self.mode = mode
         self.file_list = list()
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
         self.edge = edge
 
         if mode not in ['train', 'val', 'test']:
             raise ValueError(
                 "`mode` should be 'train', 'val' or 'test', but got {}.".format(
                     mode))
```

## paddleseg/datasets/pssl.py

```diff
@@ -67,14 +67,16 @@
         imagenet_root (str): The path to the original ImageNet dataset.
         pssl_root (str): The path to the PSSL dataset.
         mode (str, optional): Which part of dataset to use. it is one of ('train', 'val', 'test'). Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False.
     """
     ignore_index = 1001  # 0~999 is target class, 1000 is bg
     NUM_CLASSES = 1001  # consider target class and bg
+    IGNORE_INDEX = 1001
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  transforms,
                  imagenet_root,
                  pssl_root,
                  mode='train',
                  edge=False):
@@ -85,15 +87,15 @@
             raise ValueError("`transforms` is necessary, but it is None.")
 
         self.transforms = Compose(transforms)
         self.mode = mode
         self.edge = edge
 
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = self.num_classes  # 1001
+        self.ignore_index = self.IGNORE_INDEX  # 1001
         self.file_list = []
         self.class_id_dict = {}
 
         if imagenet_root is None or not os.path.isdir(pssl_root):
             raise ValueError(
                 "The dataset is not Found or the folder structure is nonconfoumance."
             )
```

## paddleseg/datasets/stare.py

```diff
@@ -32,28 +32,30 @@
     Args:
         transforms (list): Transforms for image.
         dataset_root (str): The dataset directory. Default: None
         edge (bool): whether extract edge infor in the output
         mode (str, optional): Which part of dataset to use. it is one of ('train', 'val', 'test'). Default: 'train'.
     """
     NUM_CLASSES = 2
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  dataset_root=None,
                  transforms=None,
                  edge=False,
                  mode='train'):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         mode = mode.lower()
         self.mode = mode
         self.edge = edge
         self.file_list = list()
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
 
         if mode not in ['train', 'val', 'test']:
             raise ValueError(
                 "`mode` should be 'train', 'val' or 'test', but got {}.".format(
                     mode))
 
         if self.transforms is None:
```

## paddleseg/datasets/supervisely.py

```diff
@@ -38,14 +38,16 @@
         transforms1 (list): A list of image transformations for the first input of portrait net.
         transforms2 (list): A list of image transformations for the second input of portrait net.
         dataset_root (str, optional): The Supervise.ly dataset directory. Default: None.
         mode (str, optional): A subset of the entire dataset. It should be one of ('train', 'val'). Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False
     """
     NUM_CLASSES = 2
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self,
                  common_transforms,
                  transforms1,
                  transforms2,
                  dataset_root=None,
                  mode='train',
@@ -54,15 +56,15 @@
         self.common_transforms = Compose(common_transforms)
         self.transforms = self.common_transforms
         if transforms1 is not None:
             self.transforms1 = Compose(transforms1, to_rgb=False)
         if transforms2 is not None:
             self.transforms2 = Compose(transforms2, to_rgb=False)
         mode = mode.lower()
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
         self.mode = mode
         self.num_classes = self.NUM_CLASSES
         self.input_width = 224
         self.input_height = 224
 
         if self.dataset_root is None:
             self.dataset_root = download_file_and_uncompress(
@@ -100,23 +102,27 @@
     def __getitem__(self, item):
         image_path, label_path = self.file_list[item]
         im = cv2.imread(image_path)
         label = cv2.imread(label_path, 0)
         label[label > 0] = 1
 
         if self.mode == "val":
-            common_im, label = self.common_transforms(im=im, label=label)
+            common_data = self.common_transforms(dict(img=im, label=label))
+            common_im, label = common_data['img'], common_data['label']
             im = np.float32(common_im[::-1, :, :])  # RGB => BGR
             im_aug = copy.deepcopy(im)
         else:
-            common_im, label = self.common_transforms(im=im, label=label)
+            common_data = self.common_transforms(dict(img=im, label=label))
+            common_im, label = common_data['img'], common_data['label']
             common_im = np.transpose(common_im, [1, 2, 0])
             # add augmentation
-            im, _ = self.transforms1(common_im)
-            im_aug, _ = self.transforms2(common_im)
+            data = self.transforms1(dict(img=common_im))
+            im = data['img']
+            data = self.transforms2(dict(img=common_im))
+            im_aug = data['img']
 
             im = np.float32(im[::-1, :, :])  # RGB => BGR
             im_aug = np.float32(im_aug[::-1, :, :])  # RGB => BGR
 
         label = cv2.resize(
             np.uint8(label), (self.input_width, self.input_height),
             interpolation=cv2.INTER_NEAREST)
@@ -125,12 +131,12 @@
         label = np.uint8(cv2.blur(label, (5, 5)))
         label[label >= 0.5] = 1
         label[label < 0.5] = 0
 
         edge_mask = F.mask_to_binary_edge(
             label, radius=4, num_classes=self.num_classes)
         edge_mask = np.transpose(edge_mask, [1, 2, 0]).squeeze(axis=-1)
-        im = np.concatenate([im_aug, im])
+        #im = np.concatenate([im_aug, im])
         if self.mode == "train":
-            return im, label, edge_mask
+            return dict(img=im, label=label, edge=edge_mask)
         else:
-            return im, label
+            return dict(img=im, label=label)
```

## paddleseg/datasets/voc.py

```diff
@@ -33,23 +33,25 @@
         transforms (list): Transforms for image.
         dataset_root (str): The dataset directory. Default: None
         mode (str, optional): Which part of dataset to use. it is one of ('train', 'trainval', 'trainaug', 'val').
             If you want to set mode to 'trainaug', please make sure the dataset have been augmented. Default: 'train'.
         edge (bool, optional): Whether to compute edge while training. Default: False
     """
     NUM_CLASSES = 21
+    IGNORE_INDEX = 255
+    IMG_CHANNELS = 3
 
     def __init__(self, transforms, dataset_root=None, mode='train', edge=False):
         self.dataset_root = dataset_root
         self.transforms = Compose(transforms)
         mode = mode.lower()
         self.mode = mode
         self.file_list = list()
         self.num_classes = self.NUM_CLASSES
-        self.ignore_index = 255
+        self.ignore_index = self.IGNORE_INDEX
         self.edge = edge
 
         if mode not in ['train', 'trainval', 'trainaug', 'val']:
             raise ValueError(
                 "`mode` should be one of ('train', 'trainval', 'trainaug', 'val') in PascalVOC dataset, but got {}."
                 .format(mode))
```

## paddleseg/models/__init__.py

```diff
@@ -57,12 +57,19 @@
 from .fastfcn import FastFCN
 from .pfpnnet import PFPNNet
 from .glore import GloRe
 from .ddrnet import DDRNet_23
 from .ccnet import CCNet
 from .mobileseg import MobileSeg
 from .upernet import UPerNet
+from .upernet_cae import UPerNetCAE
 from .sinet import SINet
 from .lraspp import LRASPP
 from .mscale_ocrnet import MscaleOCRNet
 from .topformer import TopFormer
 from .rtformer import RTFormer
+from .upernet_vit_adapter import UPerNetViTAdapter
+from .lpsnet import LPSNet
+from .maskformer import MaskFormer
+from .segnext import SegNeXt
+from .knet import KNet
+from .pp_mobileseg import PPMobileSeg
```

## paddleseg/models/espnet.py

```diff
@@ -337,20 +337,23 @@
         eesp_out = self.eesp(x)
         output = paddle.concat([avg_out, eesp_out], axis=1)
 
         if inputs is not None:
             w1 = paddle.shape(avg_out)[2]
             w2 = paddle.shape(inputs)[2]
 
+            inputs = paddle.reshape(inputs, [
+                inputs.shape[0], inputs.shape[1], paddle.shape(inputs)[2],
+                paddle.shape(inputs)[3]
+            ])
             while w2 != w1:
                 inputs = F.avg_pool2d(
                     inputs, kernel_size=3, padding=1, stride=2)
                 w2 = paddle.shape(inputs)[2]
-            # import pdb
-            # pdb.set_trace()
+
             output = output + self.shortcut_layer(inputs)
         return self._act(output)
 
 
 class EESPNetBackbone(nn.Layer):
     """
     The EESPNetBackbone implementation based on PaddlePaddle.
```

## paddleseg/models/hrnet_contrast.py

```diff
@@ -27,51 +27,51 @@
     The HRNetW48Contrast implementation based on PaddlePaddle.
 
     The original article refers to
     Wenguan Wang, Tianfei Zhou, et al. "Exploring Cross-Image Pixel Contrast for Semantic Segmentation"
     (https://arxiv.org/abs/2101.11939).
 
     Args:
-        in_channels (int): The output dimensions of backbone.
+        bb_channels (int): The output dimensions of backbone.
         num_classes (int): The unique number of target classes.
         backbone (Paddle.nn.Layer): Backbone network, currently support HRNet_W48.
         drop_prob (float): The probability of dropout.
         proj_dim (int): The projection dimensions.
         align_corners (bool, optional): An argument of F.interpolate. It should be set to False when the feature size is even,
             e.g. 1024x512, otherwise it is True, e.g. 769x769. Default: False.
         pretrained (str, optional): The path or url of pretrained model. Default: None.
     """
 
     def __init__(self,
-                 in_channels,
+                 bb_channels,
                  num_classes,
                  backbone,
                  drop_prob,
                  proj_dim,
                  align_corners=False,
                  pretrained=None):
         super().__init__()
-        self.in_channels = in_channels
+        self.bb_channels = bb_channels
         self.backbone = backbone
         self.num_classes = num_classes
         self.proj_dim = proj_dim
         self.align_corners = align_corners
 
         self.cls_head = nn.Sequential(
             layers.ConvBNReLU(
-                in_channels, in_channels, kernel_size=3, stride=1, padding=1),
+                bb_channels, bb_channels, kernel_size=3, stride=1, padding=1),
             nn.Dropout2D(drop_prob),
             nn.Conv2D(
-                in_channels,
+                bb_channels,
                 num_classes,
                 kernel_size=1,
                 stride=1,
                 bias_attr=False), )
         self.proj_head = ProjectionHead(
-            dim_in=in_channels, proj_dim=self.proj_dim)
+            dim_in=bb_channels, proj_dim=self.proj_dim)
 
         self.pretrained = pretrained
         self.init_weight()
 
     def init_weight(self):
         if self.pretrained is not None:
             utils.load_entire_model(self, self.pretrained)
```

## paddleseg/models/backbones/__init__.py

```diff
@@ -20,9 +20,14 @@
 from .swin_transformer import *
 from .mobilenetv2 import *
 from .mix_transformer import *
 from .stdcnet import *
 from .lite_hrnet import *
 from .shufflenetv2 import *
 from .ghostnet import *
+from .cae import *
 from .top_transformer import *
 from .uhrnet import *
+from .strideformer import *
+from .vit_adapter import *
+from .hrformer import *
+from .mscan import *
```

## paddleseg/models/backbones/mobilenetv3.py

```diff
@@ -167,15 +167,15 @@
             out_c=_make_divisible(inplanes * self.scale),
             filter_size=3,
             stride=2,
             padding=1,
             num_groups=1,
             if_act=True,
             act="hardswish")
-        self.blocks = nn.Sequential(*[
+        self.blocks = nn.Sequential(* [
             ResidualUnit(
                 in_c=_make_divisible(inplanes * self.scale if i == 0 else
                                      self.cfg[i - 1][2] * self.scale),
                 mid_c=_make_divisible(self.scale * exp),
                 out_c=_make_divisible(self.scale * c),
                 filter_size=k,
                 stride=s,
```

## paddleseg/models/backbones/swin_transformer.py

```diff
@@ -710,27 +710,119 @@
     def train(self):
         """Convert the model into training mode while keep layers freezed."""
         super(SwinTransformer, self).train()
         self._freeze_stages()
 
 
 @manager.BACKBONES.add_component
+class SwinTransformer_tiny_patch4_window7_224_maskformer(SwinTransformer):
+    def __init__(self, **kwargs):
+        super().__init__(
+            pretrain_img_size=224,
+            embed_dim=96,
+            depths=[2, 2, 6, 2],
+            num_heads=[3, 6, 12, 24],
+            window_size=7,
+            drop_path_rate=0.3,
+            patch_norm=True,
+            **kwargs)
+
+        self._out_features = ["res2", "res3", "res4", "res5"]
+
+        self._out_feature_strides = {
+            "res2": 4,
+            "res3": 8,
+            "res4": 16,
+            "res5": 32,
+        }
+        self._out_feature_channels = {
+            "res2": self.feat_channels[0],
+            "res3": self.feat_channels[1],
+            "res4": self.feat_channels[2],
+            "res5": self.feat_channels[3],
+        }
+
+    def forward(self, x):
+        outputs = {}
+        y = super(SwinTransformer_tiny_patch4_window7_224_maskformer,
+                  self).forward(x)
+        for i, k in enumerate(self._out_features):
+            outputs[k] = y[i]
+        return outputs
+
+    def output_shape(self):
+        return {
+            name: {
+                "channels": self._out_feature_channels[name],
+                "stride": self._out_feature_strides[name]
+            }
+            for name in self._out_features
+        }
+
+
+@manager.BACKBONES.add_component
 def SwinTransformer_tiny_patch4_window7_224(**kwargs):
     model = SwinTransformer(
         pretrain_img_size=224,
         embed_dim=96,
         depths=[2, 2, 6, 2],
         num_heads=[3, 6, 12, 24],
         window_size=7,
         **kwargs)
 
     return model
 
 
 @manager.BACKBONES.add_component
+class SwinTransformer_small_patch4_window7_224_maskformer(SwinTransformer):
+    def __init__(self, **kwargs):
+        super().__init__(
+            pretrain_img_size=224,
+            embed_dim=96,
+            depths=[2, 2, 18, 2],
+            num_heads=[3, 6, 12, 24],
+            window_size=7,
+            drop_path_rate=0.3,
+            patch_norm=True,
+            **kwargs)
+
+        self._out_features = ["res2", "res3", "res4", "res5"]
+
+        self._out_feature_strides = {
+            "res2": 4,
+            "res3": 8,
+            "res4": 16,
+            "res5": 32,
+        }
+        self._out_feature_channels = {
+            "res2": self.feat_channels[0],
+            "res3": self.feat_channels[1],
+            "res4": self.feat_channels[2],
+            "res5": self.feat_channels[3],
+        }
+
+    def forward(self, x):
+        outputs = {}
+        y = super(SwinTransformer_small_patch4_window7_224_maskformer,
+                  self).forward(x)
+        for i, k in enumerate(self._out_features):
+            outputs[k] = y[i]
+        return outputs
+
+    def output_shape(self):
+        return {
+            name: {
+                "channels": self._out_feature_channels[name],
+                "stride": self._out_feature_strides[name]
+            }
+            for name in self._out_features
+        }
+
+
+@manager.BACKBONES.add_component
 def SwinTransformer_small_patch4_window7_224(**kwargs):
     model = SwinTransformer(
         pretrain_img_size=224,
         embed_dim=96,
         depths=[2, 2, 18, 2],
         num_heads=[3, 6, 12, 24],
         window_size=7,
@@ -749,14 +841,60 @@
         window_size=7,
         **kwargs)
 
     return model
 
 
 @manager.BACKBONES.add_component
+class SwinTransformer_base_patch4_window7_384_maskformer(SwinTransformer):
+    def __init__(self, **kwargs):
+        super().__init__(
+            pretrain_img_size=384,
+            embed_dim=128,
+            depths=[2, 2, 18, 2],
+            num_heads=[4, 8, 16, 32],
+            window_size=12,
+            drop_path_rate=0.3,
+            patch_norm=True,
+            **kwargs)
+
+        self._out_features = ["res2", "res3", "res4", "res5"]
+
+        self._out_feature_strides = {
+            "res2": 4,
+            "res3": 8,
+            "res4": 16,
+            "res5": 32,
+        }
+        self._out_feature_channels = {
+            "res2": self.feat_channels[0],
+            "res3": self.feat_channels[1],
+            "res4": self.feat_channels[2],
+            "res5": self.feat_channels[3],
+        }
+
+    def forward(self, x):
+        outputs = {}
+        y = super(SwinTransformer_base_patch4_window7_384_maskformer,
+                  self).forward(x)
+        for i, k in enumerate(self._out_features):
+            outputs[k] = y[i]
+        return outputs
+
+    def output_shape(self):
+        return {
+            name: {
+                "channels": self._out_feature_channels[name],
+                "stride": self._out_feature_strides[name]
+            }
+            for name in self._out_features
+        }
+
+
+@manager.BACKBONES.add_component
 def SwinTransformer_base_patch4_window12_384(**kwargs):
     model = SwinTransformer(
         pretrain_img_size=384,
         embed_dim=128,
         depths=[2, 2, 18, 2],
         num_heads=[4, 8, 16, 32],
         window_size=12,
@@ -775,14 +913,60 @@
         window_size=7,
         **kwargs)
 
     return model
 
 
 @manager.BACKBONES.add_component
+class SwinTransformer_large_patch4_window7_384_maskformer(SwinTransformer):
+    def __init__(self, **kwargs):
+        super().__init__(
+            pretrain_img_size=384,
+            embed_dim=192,
+            depths=[2, 2, 18, 2],
+            num_heads=[6, 12, 24, 48],
+            window_size=12,
+            drop_path_rate=0.3,
+            patch_norm=True,
+            **kwargs)
+
+        self._out_features = ["res2", "res3", "res4", "res5"]
+
+        self._out_feature_strides = {
+            "res2": 4,
+            "res3": 8,
+            "res4": 16,
+            "res5": 32,
+        }
+        self._out_feature_channels = {
+            "res2": self.feat_channels[0],
+            "res3": self.feat_channels[1],
+            "res4": self.feat_channels[2],
+            "res5": self.feat_channels[3],
+        }
+
+    def forward(self, x):
+        outputs = {}
+        y = super(SwinTransformer_large_patch4_window7_384_maskformer,
+                  self).forward(x)
+        for i, k in enumerate(self._out_features):
+            outputs[k] = y[i]
+        return outputs
+
+    def output_shape(self):
+        return {
+            name: {
+                "channels": self._out_feature_channels[name],
+                "stride": self._out_feature_strides[name]
+            }
+            for name in self._out_features
+        }
+
+
+@manager.BACKBONES.add_component
 def SwinTransformer_large_patch4_window12_384(**kwargs):
     model = SwinTransformer(
         pretrain_img_size=384,
         embed_dim=192,
         depths=[2, 2, 18, 2],
         num_heads=[6, 12, 24, 48],
         window_size=12,
```

## paddleseg/models/backbones/transformer_utils.py

```diff
@@ -29,15 +29,15 @@
 def drop_path(x, drop_prob=0., training=False):
     """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
     the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
     See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...
     """
     if drop_prob == 0. or not training:
         return x
-    keep_prob = paddle.to_tensor(1 - drop_prob)
+    keep_prob = paddle.to_tensor(1 - drop_prob, dtype=x.dtype)
     shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)
     random_tensor = keep_prob + paddle.rand(shape).astype(x.dtype)
     random_tensor = paddle.floor(random_tensor)  # binarize
     output = x.divide(keep_prob) * random_tensor
     return output
```

## paddleseg/models/backbones/uhrnet.py

```diff
@@ -642,15 +642,15 @@
         return y
 
 
 class SELayer(nn.Layer):
     def __init__(self, num_channels, num_filters, reduction_ratio, name=None):
         super(SELayer, self).__init__()
 
-        self.pool2d_gap = nn.AdaptiveAvgPool2d(1)
+        self.pool2d_gap = nn.AdaptiveAvgPool2D(1)
 
         self._num_channels = num_channels
 
         med_ch = int(num_channels / reduction_ratio)
         stdv = 1.0 / math.sqrt(num_channels * 1.0)
         self.squeeze = nn.Linear(
             num_channels,
```

## paddleseg/models/layers/__init__.py

```diff
@@ -8,15 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from .layer_libs import ConvBNReLU, ConvBN, SeparableConvBNReLU, DepthwiseConvBN, AuxLayer, SyncBatchNorm, JPU, ConvBNPReLU, ConvBNAct, ConvBNLeakyReLU
+from .layer_libs import ConvBNReLU, ConvBN, SeparableConvBNReLU, DepthwiseConvBN, AuxLayer, SyncBatchNorm, JPU, ConvBNPReLU, ConvBNAct, ConvBNLeakyReLU, ConvNormAct, ConvGNAct
 from .activation import Activation
 from .pyramid_pool import ASPPModule, PPModule
 from .attention import AttentionBlock
 from .nonlocal2d import NonLocal2D
 from .wrap_functions import *
 from .tensor_fusion import UAFM_SpAtten, UAFM_SpAtten_S, UAFM_ChAtten, UAFM_ChAtten_S, UAFM, UAFMMobile, UAFMMobile_SpAtten
 from .polaried_self_attention import PolarizedSelfAttentionModule
+from .nmf_2d import NMF2D
```

## paddleseg/models/layers/layer_libs.py

```diff
@@ -54,14 +54,79 @@
     def forward(self, x):
         x = self._conv(x)
         x = self._batch_norm(x)
         x = self._relu(x)
         return x
 
 
+class ConvGNAct(nn.Layer):
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 kernel_size,
+                 padding="same",
+                 num_groups=32,
+                 act_type=None,
+                 **kwargs):
+        super().__init__()
+        self._conv = nn.Conv2D(
+            in_channels, out_channels, kernel_size, padding=padding, **kwargs)
+
+        if "data_format" in kwargs:
+            data_format = kwargs["data_format"]
+        else:
+            data_format = "NCHW"
+        self._group_norm = nn.GroupNorm(
+            num_groups, out_channels, data_format=data_format)
+        self._act_type = act_type
+        if act_type is not None:
+            self._act = layers.Activation(act_type)
+
+    def forward(self, x):
+        x = self._conv(x)
+        x = self._group_norm(x)
+        if self._act_type is not None:
+            x = self._act(x)
+        return x
+
+
+class ConvNormAct(nn.Layer):
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 kernel_size,
+                 padding='same',
+                 act_type=None,
+                 norm=None,
+                 **kwargs):
+        super().__init__()
+
+        self._conv = nn.Conv2D(
+            in_channels, out_channels, kernel_size, padding=padding, **kwargs)
+
+        if 'data_format' in kwargs:
+            data_format = kwargs['data_format']
+        else:
+            data_format = 'NCHW'
+
+        self._norm = norm if norm is not None else None
+
+        self._act_type = act_type
+        if act_type is not None:
+            self._act = layers.Activation(act_type)
+
+    def forward(self, x):
+        x = self._conv(x)
+        if self._norm is not None:
+            x = self._norm(x)
+        if self._act_type is not None:
+            x = self._act(x)
+        return x
+
+
 class ConvBNAct(nn.Layer):
     def __init__(self,
                  in_channels,
                  out_channels,
                  kernel_size,
                  padding='same',
                  act_type=None,
```

## paddleseg/models/losses/__init__.py

```diff
@@ -30,7 +30,8 @@
 from .kl_loss import KLLoss
 from .rmi_loss import RMILoss
 from .detail_aggregate_loss import DetailAggregateLoss
 from .point_cross_entropy_loss import PointCrossEntropyLoss
 from .pixel_contrast_cross_entropy_loss import PixelContrastCrossEntropyLoss
 from .semantic_encode_cross_entropy_loss import SECrossEntropyLoss
 from .semantic_connectivity_loss import SemanticConnectivityLoss
+from .maskformer_loss import MaskFormerLoss
```

## paddleseg/models/losses/binary_cross_entropy_loss.py

```diff
@@ -129,14 +129,15 @@
             label (Tensor): Label tensor, the data type is int64. Shape is (N, C), where each
                 value is 0 or 1, and if shape is more than 2D, this is
                 (N, C, D1, D2,..., Dk), k >= 1.
         """
         if len(label.shape) != len(logit.shape):
             label = paddle.unsqueeze(label, 1)
         mask = (label != self.ignore_index)
+        label = paddle.where(mask, label, paddle.zeros_like(label))
         mask = paddle.cast(mask, 'float32')
         # label.shape should equal to the logit.shape
         if label.shape[1] != logit.shape[1]:
             label = label.squeeze(1)
             label = F.one_hot(label, logit.shape[1])
             label = label.transpose((0, 3, 1, 2))
         if isinstance(self.weight, str):
```

## paddleseg/models/losses/cross_entropy_loss.py

```diff
@@ -29,25 +29,28 @@
             given to each class. Its length must be equal to the number of classes.
             Default ``None``.
         ignore_index (int64, optional): Specifies a target value that is ignored
             and does not contribute to the input gradient. Default ``255``.
         top_k_percent_pixels (float, optional): the value lies in [0.0, 1.0].
             When its value < 1.0, only compute the loss for the top k percent pixels
             (e.g., the top 20% pixels). This is useful for hard pixel mining. Default ``1.0``.
+        avg_non_ignore (bool, optional): Whether the loss is only averaged over non-ignored value of pixels. Default: True.
         data_format (str, optional): The tensor format to use, 'NCHW' or 'NHWC'. Default ``'NCHW'``.
     """
 
     def __init__(self,
                  weight=None,
                  ignore_index=255,
                  top_k_percent_pixels=1.0,
+                 avg_non_ignore=True,
                  data_format='NCHW'):
         super(CrossEntropyLoss, self).__init__()
         self.ignore_index = ignore_index
         self.top_k_percent_pixels = top_k_percent_pixels
+        self.avg_non_ignore = avg_non_ignore
         self.EPS = 1e-8
         self.data_format = data_format
         if weight is not None:
             self.weight = paddle.to_tensor(weight, dtype='float32')
         else:
             self.weight = None
 
@@ -103,18 +106,20 @@
             loss (Tensor): Loss tensor which is the output of cross_entropy. If soft_label
                 is False in cross_entropy, the shape of loss should be the same as the label.
                 If soft_label is True in cross_entropy, the shape of loss should be
                 (N, D1, D2,..., Dk, 1).
         Returns:
             (Tensor): The average loss.
         """
-        mask = label != self.ignore_index
-        mask = paddle.cast(mask, 'float32')
-        label.stop_gradient = True
+        if self.avg_non_ignore:
+            mask = paddle.cast(label != self.ignore_index, dtype='float32')
+        else:
+            mask = paddle.ones(label.shape, dtype='float32')
         mask.stop_gradient = True
+        label.stop_gradient = True
 
         if loss.ndim > mask.ndim:
             loss = paddle.squeeze(loss, axis=-1)
         loss = loss * mask
         if semantic_weights is not None:
             loss = loss * semantic_weights
```

## paddleseg/models/losses/focal_loss.py

```diff
@@ -45,27 +45,28 @@
     def forward(self, logit, label):
         """
         Forward computation.
 
         Args:
             logit (Tensor): Logit tensor, the data type is float32, float64. Shape is
                 (N, C, H, W), where C is number of classes.
-            label (Tensor): Label tensor, the data type is int64. Shape is (N, W, W),
+            label (Tensor): Label tensor, the data type is int64. Shape is (N, H, W),
                 where each value is 0 <= label[i] <= C-1.
         Returns:
             (Tensor): The average loss.
         """
         assert logit.ndim == 4, "The ndim of logit should be 4."
         assert logit.shape[1] == 2, "The channel of logit should be 2."
         assert label.ndim == 3, "The ndim of label should be 3."
 
         class_num = logit.shape[1]  # class num is 2
         logit = paddle.transpose(logit, [0, 2, 3, 1])  # N,C,H,W => N,H,W,C
 
         mask = label != self.ignore_index  # N,H,W
+        label = paddle.where(mask, label, paddle.zeros_like(label))
         mask = paddle.unsqueeze(mask, 3)
         mask = paddle.cast(mask, 'float32')
         mask.stop_gradient = True
 
         label = F.one_hot(label, class_num)  # N,H,W,C
         label = paddle.cast(label, logit.dtype)
         label.stop_gradient = True
@@ -106,15 +107,15 @@
     def forward(self, logit, label):
         """
         Forward computation.
 
         Args:
             logit (Tensor): Logit tensor, the data type is float32, float64. Shape is
                 (N, C, H, W), where C is number of classes.
-            label (Tensor): Label tensor, the data type is int64. Shape is (N, W, W),
+            label (Tensor): Label tensor, the data type is int64. Shape is (N, H, W),
                 where each value is 0 <= label[i] <= C-1.
         Returns:
             (Tensor): The average loss.
         """
         assert logit.ndim == 4, "The ndim of logit should be 4."
         assert label.ndim == 3, "The ndim of label should be 3."
```

## paddleseg/transforms/functional.py

```diff
@@ -14,14 +14,20 @@
 
 import cv2
 import numpy as np
 from PIL import Image, ImageEnhance
 from scipy.ndimage import distance_transform_edt
 
 
+def crop(img, crop_coordinate):
+    x1, y1, x2, y2 = crop_coordinate
+    img = img[y1:y2, x1:x2, ...]
+    return img
+
+
 def rescale_size(img_size, target_size):
     scale = min(
         max(target_size) / max(img_size), min(target_size) / min(img_size))
     rescaled_size = [round(i * scale) for i in img_size]
     return rescaled_size, scale
```

## paddleseg/transforms/transforms.py

```diff
@@ -239,21 +239,29 @@
 @manager.TRANSFORMS.add_component
 class ResizeByShort:
     """
     Resize the short side of an image to given size, and then scale the other side proportionally.
 
     Args:
         short_size (int): The target size of short side.
+        max_size(int): The maximum length of resized image's long edge, if the resized image's long edge exceed this length, short size will be adjusted.
     """
 
-    def __init__(self, short_size):
+    def __init__(self, short_size, max_size=1e10):
+        if isinstance(short_size, list):
+            short_size = random.choice(short_size)
         self.short_size = short_size
+        self.max_size = max_size
 
     def __call__(self, data):
+        h, w = data['img'].shape[0:2]
         data['trans_info'].append(('resize', data['img'].shape[0:2]))
+        if self.short_size / min(h, w) * max(h, w) > self.max_size:
+            self.short_size = int((self.max_size / max(h, w)) * min(h, w))
+
         data['img'] = functional.resize_short(data['img'], self.short_size)
         for key in data.get('gt_fields', []):
             data[key] = functional.resize_short(data[key], self.short_size,
                                                 cv2.INTER_NEAREST)
 
         return data
 
@@ -546,89 +554,117 @@
     Crop a sub-image from a raw image and annotation image randomly. If the target cropping size
     is larger than original image, then the bottom-right padding will be added.
 
     Args:
         crop_size (tuple, optional): The target cropping size. Default: (512, 512).
         im_padding_value (float, optional): The padding value of raw image. Default: 127.5.
         label_padding_value (int, optional): The padding value of annotation image. Default: 255.
+        category_max_ratio (float, optional): The maximum ratio that single category could occupy. 
+            Default: 1.0.
+        ignore_index (int, optional): The value that should be ignored in the annotation image. 
+            Default: 255.
+        loop_times (int, optional): The maximum number of attempts to crop an image. Default: 10.
 
     Raises:
         TypeError: When crop_size is neither list nor tuple.
         ValueError: When the length of crop_size is not 2.
     """
 
     def __init__(self,
                  crop_size=(512, 512),
                  im_padding_value=127.5,
-                 label_padding_value=255):
+                 label_padding_value=255,
+                 category_max_ratio=1.0,
+                 ignore_index=255,
+                 loop_times=10):
         if isinstance(crop_size, list) or isinstance(crop_size, tuple):
             if len(crop_size) != 2:
                 raise ValueError(
-                    'Type of `crop_size` is list or tuple. It should include 2 elements, but it is {}'
+                    'Type of `crop_size` is list or tuple. It should include 2 elements, but it is {}.'
                     .format(crop_size))
         else:
             raise TypeError(
-                "The type of `crop_size` is invalid. It should be list or tuple, but it is {}"
+                "The type of `crop_size` is invalid. It should be list or tuple, but it is {}."
                 .format(type(crop_size)))
-        self.crop_size = crop_size
+        if category_max_ratio <= 0:
+            raise ValueError(
+                "The value of `category_max_ratio` must be greater than 0, but got {}.".
+                format(category_max_ratio))
+        if loop_times <= 0:
+            raise ValueError(
+                "The value of `loop_times` must be greater than 0, but got {}.".
+                format(loop_times))
+
+        self.crop_size = tuple(reversed(crop_size))
         self.im_padding_value = im_padding_value
         self.label_padding_value = label_padding_value
-
-    def __call__(self, data):
-
-        if isinstance(self.crop_size, int):
-            crop_width = self.crop_size
-            crop_height = self.crop_size
-        else:
-            crop_width = self.crop_size[0]
-            crop_height = self.crop_size[1]
-
-        img_height = data['img'].shape[0]
-        img_width = data['img'].shape[1]
-
-        if img_height == crop_height and img_width == crop_width:
-            return data
-        else:
-            pad_height = max(crop_height - img_height, 0)
-            pad_width = max(crop_width - img_width, 0)
-            img_channels = 1 if data['img'].ndim == 2 else data['img'].shape[2]
-            if (pad_height > 0 or pad_width > 0):
-                data['img'] = cv2.copyMakeBorder(
-                    data['img'],
+        self.category_max_ratio = category_max_ratio
+        self.ignore_index = ignore_index
+        self.loop_times = loop_times
+
+    def _get_crop_coordinates(self, origin_size):
+        margin_h = max(origin_size[0] - self.crop_size[0], 0)
+        margin_w = max(origin_size[1] - self.crop_size[1], 0)
+        offset_h = np.random.randint(0, margin_h + 1)
+        offset_w = np.random.randint(0, margin_w + 1)
+        crop_y1, crop_y2 = offset_h, offset_h + self.crop_size[0]
+        crop_x1, crop_x2 = offset_w, offset_w + self.crop_size[1]
+
+        return crop_x1, crop_y1, crop_x2, crop_y2
+
+    def _padding(self, data):
+        img_shape = data['img'].shape[:2]
+        pad_height = max(self.crop_size[0] - img_shape[0], 0)
+        pad_width = max(self.crop_size[1] - img_shape[1], 0)
+        img_channels = 1 if data['img'].ndim == 2 else data['img'].shape[2]
+        if (pad_height > 0 or pad_width > 0):
+            data['img'] = cv2.copyMakeBorder(
+                data['img'],
+                0,
+                pad_height,
+                0,
+                pad_width,
+                cv2.BORDER_CONSTANT,
+                value=(self.im_padding_value, ) * img_channels)
+            for key in data.get('gt_fields', []):
+                data[key] = cv2.copyMakeBorder(
+                    data[key],
                     0,
                     pad_height,
                     0,
                     pad_width,
                     cv2.BORDER_CONSTANT,
-                    value=(self.im_padding_value, ) * img_channels)
-                for key in data.get('gt_fields', []):
-                    data[key] = cv2.copyMakeBorder(
-                        data[key],
-                        0,
-                        pad_height,
-                        0,
-                        pad_width,
-                        cv2.BORDER_CONSTANT,
-                        value=self.label_padding_value)
-                img_height = data['img'].shape[0]
-                img_width = data['img'].shape[1]
-
-            if crop_height > 0 and crop_width > 0:
-                h_off = np.random.randint(img_height - crop_height + 1)
-                w_off = np.random.randint(img_width - crop_width + 1)
-
-                if data['img'].ndim == 2:
-                    data['img'] = data['img'][h_off:(crop_height + h_off),
-                                              w_off:(w_off + crop_width)]
-                else:
-                    data['img'] = data['img'][h_off:(crop_height + h_off),
-                                              w_off:(w_off + crop_width), :]
-                for key in data.get('gt_fields', []):
-                    data[key] = data[key][h_off:(crop_height + h_off), w_off:(
-                        w_off + crop_width)]
+                    value=self.label_padding_value)
+        return data
+
+    def __call__(self, data):
+        img_shape = data['img'].shape[:2]
+        if img_shape[0] == self.crop_size[0] and img_shape[1] == self.crop_size[
+                1]:
+            return data
+
+        data = self._padding(data)
+        img_shape = data['img'].shape[:2]
+        crop_coordinates = self._get_crop_coordinates(img_shape)
+
+        if self.category_max_ratio < 1.0:
+            for _ in range(self.loop_times):
+                seg_temp = functional.crop(data["label"], crop_coordinates)
+                labels, cnt = np.unique(seg_temp, return_counts=True)
+                cnt = cnt[labels != self.ignore_index]
+                if len(cnt) > 1 and np.max(cnt) / np.sum(
+                        cnt) < self.category_max_ratio:
+                    data['img'] = seg_temp
+                    break
+                crop_coordinates = self._get_crop_coordinates(img_shape)
+        else:
+            data['img'] = functional.crop(data['img'], crop_coordinates)
+        for key in data.get("gt_fields", []):
+            data[key] = functional.crop(data[key], crop_coordinates)
+
         return data
 
 
 @manager.TRANSFORMS.add_component
 class RandomCenterCrop:
     """
     Crops the given the input data at the center.
@@ -1122,7 +1158,65 @@
                 np.uint8(data[key]),
                 matrix,
                 tuple(self.size),
                 flags=cv2.INTER_NEAREST,
                 borderMode=cv2.BORDER_CONSTANT,
                 borderValue=self.label_padding_value)
         return data
+
+
+@manager.TRANSFORMS.add_component
+class GenerateInstanceTargets:
+    """
+    Generate instance targets from ground-truth labels.
+
+    Args:
+        num_classes (int): The number of classes.
+        ignore_index (int, optional): Specifies a target value that is ignored. Default: 255.
+    """
+
+    def __init__(self, num_classes, ignore_index=255):
+        self.num_classes = num_classes
+        self.ignore_index = ignore_index
+
+    def __call__(self, data):
+        if 'label' in data:
+            sem_seg_gt = data['label']
+            instances = {"image_shape": data['img'].shape[1:]}
+            classes = np.unique(sem_seg_gt)
+            classes = classes[classes != self.ignore_index]
+
+            # To make data compatible with dataloader
+            classes_cpt = np.array([
+                self.ignore_index
+                for _ in range(self.num_classes - len(classes))
+            ])
+            classes_cpt = np.append(classes, classes_cpt)
+            instances["gt_classes"] = np.asarray(classes_cpt).astype('int64')
+
+            masks = []
+            for cid in classes:
+                masks.append(sem_seg_gt == cid)  # [C, H, W] 
+
+            shape = [self.num_classes - len(masks)] + list(data['label'].shape)
+            masks_cpt = np.zeros(shape, dtype='int64')
+
+            if len(masks) == 0:
+                # Some images do not have annotation and will all be ignored
+                instances['gt_masks'] = np.zeros(
+                    (self.num_classes, sem_seg_gt.shape[-2],
+                     sem_seg_gt.shape[-1]),
+                    dtype='int64')
+
+            else:
+                instances['gt_masks'] = np.concatenate(
+                    [
+                        np.stack([
+                            np.ascontiguousarray(x).astype('float32')
+                            for x in masks
+                        ]), masks_cpt
+                    ],
+                    axis=0)
+
+            data['instances'] = instances
+
+        return data
```

## paddleseg/utils/__init__.py

```diff
@@ -15,8 +15,8 @@
 from . import logger
 from . import download
 from . import metrics
 from .env import seg_env, get_sys_env
 from .utils import *
 from .timer import TimeAverager, calculate_eta
 from . import visualize
-from .ema import EMA
+from .ema import *
```

## paddleseg/utils/download.py

```diff
@@ -142,25 +142,37 @@
             shutil.rmtree(savepath)
         if os.path.exists(savename):
             shutil.rmtree(savename)
         if os.path.exists(extraname):
             shutil.rmtree(extraname)
     full_path = os.path.join(extraname,
                              filename) if filename is not None else extraname
+    rank_id_curr_node = int(os.environ.get("PADDLE_RANK_IN_NODE", 0))
     if not os.path.exists(
             full_path):  # If pretrained model exists, skip download process.
-        if not os.path.exists(savename):
-            if not os.path.exists(savepath):
-                _download_file(url, savepath, print_progress)
+        lock_path = extraname + '.download.lock'
+        with open(lock_path, 'w'):  # touch    
+            os.utime(lock_path, None)
+        if rank_id_curr_node == 0:
+            if not os.path.exists(savename):
+                if not os.path.exists(savepath):
+                    _download_file(url, savepath, print_progress)
 
-            if (not tarfile.is_tarfile(savepath)) and (
-                    not zipfile.is_zipfile(savepath)):
-                if not os.path.exists(extraname):
-                    os.makedirs(extraname)
-                shutil.move(savepath, extraname)
-                return extraname
+                if (not tarfile.is_tarfile(savepath)) and (
+                        not zipfile.is_zipfile(savepath)):
+                    if not os.path.exists(extraname):
+                        os.makedirs(extraname)
+                    shutil.move(savepath, extraname)
+
+                else:
+                    savename = _uncompress_file(savepath, extrapath,
+                                                delete_file, print_progress)
+                    savename = os.path.join(extrapath, savename)
+                    shutil.move(savename, extraname)
+
+            os.remove(lock_path)
+
+        else:
+            while os.path.exists(lock_path):
+                time.sleep(0.5)
 
-            savename = _uncompress_file(savepath, extrapath, delete_file,
-                                        print_progress)
-            savename = os.path.join(extrapath, savename)
-        shutil.move(savename, extraname)
     return extraname
```

## paddleseg/utils/ema.py

```diff
@@ -12,93 +12,37 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import numpy as np
 import paddle
 
 
-class EMA(object):
-    """
-    The implementation of Exponential Moving Average for the trainable parameters.
-
-    Args:
-        model (nn.Layer): The model for applying EMA.
-        decay (float, optional): Decay is used to calculate ema_variable by
-            `ema_variable = decay * ema_variable + (1 - decay) * new_variable`.
-            Default: 0.99.
-    
-    Returns:
-        None
-    
-    Examples:
-        .. code-block:: python
-
-            # 1. Define model and dataset
-        
-            # 2. Create EMA
-            ema = EMA(model, decay=0.99)
-
-            # 3. Train stage
-            for data in dataloader():
-                ...
-                optimizer.step()
-                ema.step()
-
-            # 4. Evaluate stage
-            ema.apply()     # Use the EMA data to replace the origin data
-
-            for data in dataloader():
-                ...
-            
-            ema.restore()   # Restore the origin data to the model
-
-    """
-
-    def __init__(self, model, decay=0.99):
-        super().__init__()
-
-        assert isinstance(model, paddle.nn.Layer), \
-            "The model should be the instance of paddle.nn.Layer."
-        assert decay >= 0 and decay <= 1.0, \
-            "The decay = {} should in [0.0, 1.0]".format(decay)
-
-        self._model = model
-        self._decay = decay
-        self._ema_data = {}
-        self._backup_data = {}
-
-        for name, param in self._model.named_parameters():
-            if not param.stop_gradient:
-                self._ema_data[name] = param.numpy()
-
-    def step(self):
-        """
-        Calculate the EMA data for all trainable parameters.
-        """
-        for name, param in self._model.named_parameters():
-            if not param.stop_gradient:
-                assert name in self._ema_data, \
-                    "The param ({}) isn't in the model".format(name)
-                self._ema_data[name] = self._decay * self._ema_data[name] \
-                    + (1.0 - self._decay) * param.numpy()
-
-    def apply(self):
-        """
-        Save the origin data and use the EMA data to replace the origin data.
-        """
-        for name, param in self._model.named_parameters():
-            if not param.stop_gradient:
-                assert name in self._ema_data, \
-                    "The param ({}) isn't in the model".format(name)
-                self._backup_data[name] = param.numpy()
-                param.set_value(self._ema_data[name])
-
-    def restore(self):
-        """
-        Restore the origin data to the model.
-        """
-        for name, param in self._model.named_parameters():
-            if not param.stop_gradient:
-                assert name in self._backup_data, \
-                    "The param ({}) isn't in the model".format(name)
-                param.set_value(self._backup_data[name])
-        self._backup_data = {}
+def judge_params_equal(ema_model, model):
+    for ema_param, param in zip(ema_model.named_parameters(),
+                                model.named_parameters()):
+        if not paddle.equal_all(ema_param[1], param[1]):
+            # print("Difference in", ema_param[0])
+            return False
+    return True
+
+
+def init_ema_params(ema_model, model):
+    state = {}
+    msd = model.state_dict()
+    for k, v in ema_model.state_dict().items():
+        if paddle.is_floating_point(v):
+            v = msd[k].detach()
+        state[k] = v
+    ema_model.set_state_dict(state)
+
+
+def update_ema_model(ema_model, model, step=0, decay=0.999):
+    with paddle.no_grad():
+        state = {}
+        decay = min(1 - 1 / (step + 1), decay)
+        msd = model.state_dict()
+        for k, v in ema_model.state_dict().items():
+            if paddle.is_floating_point(v):
+                v *= decay
+                v += (1.0 - decay) * msd[k].detach()
+            state[k] = v
+        ema_model.set_state_dict(state)
```

## paddleseg/utils/metrics.py

```diff
@@ -46,17 +46,17 @@
     intersect_area = []
     mask = label != ignore_index
 
     for i in range(num_classes):
         pred_i = paddle.logical_and(pred == i, mask)
         label_i = label == i
         intersect_i = paddle.logical_and(pred_i, label_i)
-        pred_area.append(paddle.sum(paddle.cast(pred_i, "int32")))
-        label_area.append(paddle.sum(paddle.cast(label_i, "int32")))
-        intersect_area.append(paddle.sum(paddle.cast(intersect_i, "int32")))
+        pred_area.append(paddle.sum(paddle.cast(pred_i, "int64")))
+        label_area.append(paddle.sum(paddle.cast(label_i, "int64")))
+        intersect_area.append(paddle.sum(paddle.cast(intersect_i, "int64")))
 
     pred_area = paddle.concat(pred_area)
     label_area = paddle.concat(label_area)
     intersect_area = paddle.concat(intersect_area)
 
     return intersect_area, pred_area, label_area
```

## paddleseg/utils/utils.py

```diff
@@ -8,42 +8,114 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import contextlib
-import filelock
 import os
+import contextlib
 import tempfile
-import numpy as np
 import random
 from urllib.parse import urlparse, unquote
 
+import yaml
+import numpy as np
 import paddle
+import cv2
 
-from paddleseg.utils import logger, seg_env
+from paddleseg.utils import logger, seg_env, get_sys_env
 from paddleseg.utils.download import download_file_and_uncompress
 
 
+def set_seed(seed=None):
+    if seed is not None:
+        paddle.seed(seed)
+        np.random.seed(seed)
+        random.seed(seed)
+
+
+def show_env_info():
+    env_info = get_sys_env()
+    info = ['{}: {}'.format(k, v) for k, v in env_info.items()]
+    info = '\n'.join(['', format('Environment Information', '-^48s')] + info +
+                     ['-' * 48])
+    logger.info(info)
+
+
+def show_cfg_info(config):
+    msg = '\n---------------Config Information---------------\n'
+    ordered_module = ('batch_size', 'iters', 'train_dataset', 'val_dataset',
+                      'optimizer', 'lr_scheduler', 'loss', 'model')
+    all_module = set(config.dic.keys())
+    for module in ordered_module:
+        if module in config.dic:
+            module_dic = {module: config.dic[module]}
+            msg += str(yaml.dump(module_dic, Dumper=NoAliasDumper))
+            all_module.remove(module)
+    for module in all_module:
+        module_dic = {module: config.dic[module]}
+        msg += str(yaml.dump(module_dic, Dumper=NoAliasDumper))
+    msg += '------------------------------------------------\n'
+    logger.info(msg)
+
+
+def set_device(device):
+    env_info = get_sys_env()
+    if device == 'gpu' and env_info['Paddle compiled with cuda'] \
+        and env_info['GPUs used']:
+        place = 'gpu'
+    elif device == 'xpu' and paddle.is_compiled_with_xpu():
+        place = 'xpu'
+    elif device == 'npu' and "npu" in paddle.device.get_all_custom_device_type(
+    ):
+        place = 'npu'
+    elif device == 'mlu' and paddle.is_compiled_with_mlu():
+        place = 'mlu'
+    else:
+        place = 'cpu'
+    paddle.set_device(place)
+    logger.info("Set device: {}".format(place))
+
+
+def convert_sync_batchnorm(model, device):
+    # Convert bn to sync_bn when use multi GPUs
+    env_info = get_sys_env()
+    if device == 'gpu' and env_info['Paddle compiled with cuda'] \
+        and env_info['GPUs used'] and paddle.distributed.ParallelEnv().nranks > 1:
+        model = paddle.nn.SyncBatchNorm.convert_sync_batchnorm(model)
+        logger.info("Convert bn to sync_bn")
+    return model
+
+
+def set_cv2_num_threads(num_workers):
+    # Limit cv2 threads if too many subprocesses are spawned.
+    # This should reduce resource allocation and thus boost performance.
+    nranks = paddle.distributed.ParallelEnv().nranks
+    if nranks >= 8 and num_workers >= 8:
+        logger.warning("The number of threads used by OpenCV is " \
+            "set to 1 to improve performance.")
+        cv2.setNumThreads(1)
+
+
 @contextlib.contextmanager
 def generate_tempdir(directory: str=None, **kwargs):
     '''Generate a temporary directory'''
     directory = seg_env.TMP_HOME if not directory else directory
     with tempfile.TemporaryDirectory(dir=directory, **kwargs) as _dir:
         yield _dir
 
 
 def load_entire_model(model, pretrained):
     if pretrained is not None:
         load_pretrained_model(model, pretrained)
     else:
-        logger.warning('Not all pretrained params of {} are loaded, ' \
-                       'training from scratch or a pretrained backbone.'.format(model.__class__.__name__))
+        logger.warning('Weights are not loaded for {} model since the '
+                       'path of weights is None'.format(
+                           model.__class__.__name__))
 
 
 def download_pretrained_model(pretrained_model):
     """
     Download pretrained model from url.
     Args:
         pretrained_model (str): the url of pretrained weight
@@ -58,22 +130,22 @@
         savename = pretrained_model.split('/')[-2]
         filename = pretrained_model.split('/')[-1]
     else:
         savename = savename.split('.')[0]
         filename = 'model.pdparams'
 
     with generate_tempdir() as _dir:
-        with filelock.FileLock(os.path.join(seg_env.TMP_HOME, savename)):
-            pretrained_model = download_file_and_uncompress(
-                pretrained_model,
-                savepath=_dir,
-                extrapath=seg_env.PRETRAINED_MODEL_HOME,
-                extraname=savename,
-                filename=filename)
-            pretrained_model = os.path.join(pretrained_model, filename)
+        pretrained_model = download_file_and_uncompress(
+            pretrained_model,
+            savepath=_dir,
+            cover=False,
+            extrapath=seg_env.PRETRAINED_MODEL_HOME,
+            extraname=savename,
+            filename=filename)
+        pretrained_model = os.path.join(pretrained_model, filename)
     return pretrained_model
 
 
 def load_pretrained_model(model, pretrained_model):
     if pretrained_model is not None:
         logger.info('Loading pretrained model from {}'.format(pretrained_model))
 
@@ -173,7 +245,51 @@
         )
 
     if len(image_list) == 0:
         raise RuntimeError(
             'There are not image file in `--image_path`={}'.format(image_path))
 
     return image_list, image_dir
+
+
+class NoAliasDumper(yaml.SafeDumper):
+    def ignore_aliases(self, data):
+        return True
+
+
+class CachedProperty(object):
+    """
+    A property that is only computed once per instance and then replaces itself with an ordinary attribute.
+
+    The implementation refers to https://github.com/pydanny/cached-property/blob/master/cached_property.py .
+        Note that this implementation does NOT work in multi-thread or coroutine senarios.
+    """
+
+    def __init__(self, func):
+        super().__init__()
+        self.func = func
+        self.__doc__ = getattr(func, '__doc__', '')
+
+    def __get__(self, obj, cls):
+        if obj is None:
+            return self
+        val = self.func(obj)
+        # Hack __dict__ of obj to inject the value
+        # Note that this is only executed once
+        obj.__dict__[self.func.__name__] = val
+        return val
+
+
+def get_in_channels(model_cfg):
+    if 'backbone' in model_cfg:
+        return model_cfg['backbone'].get('in_channels', None)
+    else:
+        return model_cfg.get('in_channels', None)
+
+
+def set_in_channels(model_cfg, in_channels):
+    model_cfg = model_cfg.copy()
+    if 'backbone' in model_cfg:
+        model_cfg['backbone']['in_channels'] = in_channels
+    else:
+        model_cfg['in_channels'] = in_channels
+    return model_cfg
```

## Comparing `paddleseg-2.7.0.dist-info/LICENSE` & `paddleseg-2.8.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `paddleseg-2.7.0.dist-info/METADATA` & `paddleseg-2.8.0.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: paddleseg
-Version: 2.7.0
+Version: 2.8.0
 Summary: End-to-end image segmentation kit based on PaddlePaddle.
 Home-page: https://github.com/PaddlePaddle/PaddleSeg
 Author: PaddlePaddle Author
 Author-email: 
 License: Apache 2.0
 Keywords: paddleseg paddlepaddle semantic-segmentation instance-segmentation
 Platform: UNKNOWN
@@ -20,16 +20,16 @@
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Software Development
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 License-File: LICENSE
 Requires-Dist: pyyaml (>=5.1)
 Requires-Dist: visualdl (>=2.2.0)
-Requires-Dist: opencv-python
+Requires-Dist: opencv-python (<=4.6.0)
 Requires-Dist: tqdm
 Requires-Dist: filelock
 Requires-Dist: scipy
 Requires-Dist: prettytable
-Requires-Dist: sklearn (==0.0)
+Requires-Dist: scikit-learn
 
 UNKNOWN
```

## Comparing `paddleseg-2.7.0.dist-info/RECORD` & `paddleseg-2.8.0.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,154 +1,175 @@
-paddleseg/__init__.py,sha256=WnI_Oxnz26xVGEf9mX6dwoVmFlztexad6-DjZBaxfm0,676
+paddleseg/__init__.py,sha256=KKZEZ8Vg2QMSbI9aGuL8_zaR7MKguQlhFp0nxvH2E2o,688
 paddleseg/core/__init__.py,sha256=fYh9V6fUDu96WT8ipmgsJ0pptH9q3GItBKFgbxGQNv4,754
-paddleseg/core/infer.py,sha256=Og4wA2VLNjsuAlfCXlWGL3AWvfVBkG4NijobiPUt-bU,8714
-paddleseg/core/predict.py,sha256=-mUb1808TafIpXNyyZITd237qJVT5kIhi9Ec3fwME5A,5768
-paddleseg/core/train.py,sha256=g7K6luA-dwe271WiZWZ5r_NYol-YVJt0rAL2z3Wpeqo,14602
+paddleseg/core/infer.py,sha256=sFIq04OXwP6R16RL_8H7IMepHKqTAtXmWxe9iacg7DA,8903
+paddleseg/core/predict.py,sha256=2QLrjDZdPuS9wHQ37Tqs_9NKMoWtd4APl4snsFO8STA,5878
+paddleseg/core/train.py,sha256=5JOyKD_umDn4XSCzddqCQJ3Ov4Gm6ZdycE61XMe_aDU,17684
 paddleseg/core/val.py,sha256=8w8r6xS3_D0nQzZW3NnohE1zI6q9FSKHS2lxNSSGcME,10978
-paddleseg/cvlibs/__init__.py,sha256=k_oo_ThszEYsUe3-fAMxZnwxPVpjhXz7Ygj_Xf_ysPA,684
+paddleseg/cvlibs/__init__.py,sha256=UkDR0lgFgZT8nYYOxx0wYFsoTEf_Jf1Hnvs5alibCnQ,725
+paddleseg/cvlibs/builder.py,sha256=D8sxJvtl-GkRTexmqyAOHURLofiKS49rlbsmk-jC6KQ,12527
 paddleseg/cvlibs/callbacks.py,sha256=3k43TTz1cFSfS-_JZ2_zieHWT2qSigdDi6QAH3WOrd4,8773
-paddleseg/cvlibs/config.py,sha256=DJQdNYLkY2OXOo9SVMy-RG5DA0iB0ct2nDpw8ETV73g,21419
-paddleseg/cvlibs/manager.py,sha256=C3KiTkZ94qV6qUpVXBzoYOXHyYYfmz1xo4NBgmRV5ys,4735
-paddleseg/cvlibs/optimizers.py,sha256=y4J0TzEuK1fHvmEdp-heYoXmf2eYK25HdEqAf8LR4QY,3370
-paddleseg/cvlibs/param_init.py,sha256=ZOLplB73h0VeoElvln5T2qOH-bZOczs1GWhH10UFJ9g,5161
+paddleseg/cvlibs/config.py,sha256=VgdXtEwuqdBA5fXKn8UxQhdiLtJZUi3ZWTnJSBhntLs,8001
+paddleseg/cvlibs/config_checker.py,sha256=5TWBH4T1tIan_BwAw3JRypZDCVpy_fRHSlPKVFMduaQ,9052
+paddleseg/cvlibs/manager.py,sha256=_S0FCEDmegH-_C-HtD0W_RXCTERjtByZymX1afG_rPU,4779
+paddleseg/cvlibs/param_init.py,sha256=ATNBIb15IcapghPPok4UGJb6hjIE2qjQElrSvtbRrR4,7231
 paddleseg/datasets/__init__.py,sha256=t2YbeotoZcmnWWSscoRH-OHVbaafe_ee4-AY82U5zWU,1153
-paddleseg/datasets/ade.py,sha256=KXLaC4jYJBWnCtD2lC5z-a9zIC_xkx-a5ixXPhONqDI,4776
-paddleseg/datasets/chase_db1.py,sha256=fnhxwZgTiKtVmZpOKsmlw98TDqPYynuREneEcKCsQDU,4030
-paddleseg/datasets/cityscapes.py,sha256=puidtrbE3dTuVKN4GIBwAWlQBmoSJaYu2Mz6zs_G8Uc,3083
-paddleseg/datasets/cocostuff.py,sha256=ziqNbdI4JPofgwE9SIb0FKPwdk-3SMjKrRjaR0VD8Jo,2817
+paddleseg/datasets/ade.py,sha256=vMMgd-HD-tylW69qKLOm4nxARRO0WeZj0YxKF41MzEY,4743
+paddleseg/datasets/chase_db1.py,sha256=snON8oyH1wsIHT2E7YN-ZUxSi-4wKRagAoP21CWPbbA,4088
+paddleseg/datasets/cityscapes.py,sha256=M6XPPeLBUpPGSOF09cMfMdfRMeB2XBFfIRE9ca-PIuY,3141
+paddleseg/datasets/cocostuff.py,sha256=XySOiJ2-3wRAf58qqYM8w9NFv1D3Ls8mWU07SljDsHg,2875
 paddleseg/datasets/dataset.py,sha256=X4_dZ-wUkXDU_8_S2vJ9zUKU4zMEiNJ1mJi30MId_NA,6886
-paddleseg/datasets/drive.py,sha256=Bqp1iQbYEAfKKJxZsEVmxEv2zMH9_fzJnQeOqCy9rIQ,3939
-paddleseg/datasets/eg1800.py,sha256=jqBkV8FZ_UQaG71Psavsf4x01_pyC7bYaBvKBi3K94s,5196
-paddleseg/datasets/hrf.py,sha256=K6FZw5Lvu03XqSzEi-AjnTzsPrhg0puJNrwgWgyg1hc,3944
-paddleseg/datasets/mini_deep_globe_road_extraction.py,sha256=ebcz1jFRsUWVLVncuDxmt0TpDT3rxvC5CtdqND0ESOU,3803
-paddleseg/datasets/optic_disc_seg.py,sha256=fVb7Qx2TuSHM5f0XAy08F7bmDJHBcqHmkP6fpdRXC6I,3812
-paddleseg/datasets/pascal_context.py,sha256=8y7sVC58vcPY77Rwu2R8HPBA82WB6F1lH57korpZ5KI,3492
-paddleseg/datasets/pp_humanseg14k.py,sha256=WDmiymwp-F5lc6tGZDuK_0tp6DDU1Jcjj1H_K8n1N7U,3416
-paddleseg/datasets/pssl.py,sha256=ReZFUMvzFbT9tirNkc-ovnIBvK6Xfai3t_TEFm9KDPI,5265
-paddleseg/datasets/stare.py,sha256=CJQA_aC_YQgyXU_poy7b9rvYxbsR54SAiFDjgFWKaUo,3739
-paddleseg/datasets/supervisely.py,sha256=84jaVcSHNwyxqJWiYAAl-Sym1dJE8fev_BPXflmrdJ4,5273
-paddleseg/datasets/voc.py,sha256=WA5o5XfxX2why7kfORiaFNjjRMiA7KeOfYpEXCkadKw,4956
-paddleseg/models/__init__.py,sha256=PCQ4pNZEQy82uucUOyw4i9w9G3vlKvJF6YepcfQESrE,2119
-paddleseg/models/_test.py,sha256=F-YvbsxF_vf4MVnT0N97r6lhrlXzUV7lsBpv_GBu3NQ,507
+paddleseg/datasets/drive.py,sha256=lq6SbRoivQUnIb0yRieZwHMRZ1Yb_H-aCPXG9lctM1U,3997
+paddleseg/datasets/eg1800.py,sha256=h89Z6BXhW2VYn3uXoe5Wxdvr_k2HzZ66ypG819UW3gw,5536
+paddleseg/datasets/hrf.py,sha256=X_0r_Z9f7-7Ef69ywkklH-iTYxTzsZTlrxkzyGpjz7I,4002
+paddleseg/datasets/mini_deep_globe_road_extraction.py,sha256=67vhuK-diAZPp4xBJ2dwwhuk8AaOUQ6BNc4BI-2-61s,3861
+paddleseg/datasets/optic_disc_seg.py,sha256=W5VMAj8ft6NmqLXoS645A3s8oNN1734ZtyuVlE-0WFo,3870
+paddleseg/datasets/pascal_context.py,sha256=QMNqup4g12hm5uI7SCVmet86H730_FXWA4NRvT3LNYw,3550
+paddleseg/datasets/pp_humanseg14k.py,sha256=arK7KHBD4k28Uz3tnzY36vh0n-aZfvPwhNRthoOvNhU,3474
+paddleseg/datasets/pssl.py,sha256=1qqS4m1qjt4tHDlm3RhN-AJhnfbP27_8SLWEFpH3L8o,5311
+paddleseg/datasets/stare.py,sha256=ss5azCqY_Pw2qv3GLx2PUjWhdJ2-4-2ioAezzFvklTU,3797
+paddleseg/datasets/supervisely.py,sha256=BoeR_yUQGsg9np6iJH-d56iB_zBc3pqJGmSrg2fPPq0,5593
+paddleseg/datasets/voc.py,sha256=GbsVISmhcupD4HUKDngM_qPoOvPPWb2NGJuVNK5E-pk,5014
+paddleseg/deploy/__init__.py,sha256=TjcQhX96rKmx141BBKkNVf9p7__uKWtafw1pFWG8nu8,576
+paddleseg/deploy/export.py,sha256=9B3PDDJUrziDEw7_4676x0AEWFADAiKbPXgIw-h5934,1247
+paddleseg/deploy/infer.py,sha256=V-Y4zyXBRxsBTvsc99xokINsGnOQ5q4hRPlyf2skD10,1528
+paddleseg/models/__init__.py,sha256=KHARnD5DwnpET6RseBGwJThLTqTSj9M6ToYHewQy_EY,2358
 paddleseg/models/ann.py,sha256=OOUfiRIAZJ8OmHvfXjomTcEKAI_Y9MslIdcAhfdouPk,15779
 paddleseg/models/attention_unet.py,sha256=-l5gXZc_eWjxy50UcHQ1UXWGFyAKFoxIHyqVSLZVV9Y,6499
-paddleseg/models/base_model.py,sha256=aCsm6Kbkm593RsWg1H4omwq3qL7oCfthp_qzONDs54g,1688
 paddleseg/models/bisenet.py,sha256=3jIj65Esk7FprKFCbKRrZkSCedHxUTJgRkO22ZBSRzo,10259
 paddleseg/models/bisenetv1.py,sha256=iZjcHCDurdYAloy1cW3rMNlCH9BVOGFSGR_Al-YgyMw,8133
 paddleseg/models/ccnet.py,sha256=7kytsM0UkLCp4iac-XMd5699dXj9YL8eEDCXK3czx-s,6942
 paddleseg/models/danet.py,sha256=dQKY0zqPQgktRH-2NSqO4mfZzLmrWyc-u-oeJJeAmZg,7367
 paddleseg/models/ddrnet.py,sha256=-IhVS7scjFgRzSp4XFysYvy7NrFOm-UOjtpq5SZkP9Q,14345
 paddleseg/models/decoupled_segnet.py,sha256=TBRfvgsl-4vXvw_-Wt7DJtsPBCDXcEyNma25oVk_hZ0,9203
 paddleseg/models/deeplab.py,sha256=QCFK49GHNnRWuZUsH6k4BShRdHolN05cckELkB5qTnQ,10602
 paddleseg/models/dmnet.py,sha256=VlkAlZJQeXvKwiHjsai-EBFalZWcgyCLMq-KZ3WsEAM,5457
 paddleseg/models/dnlnet.py,sha256=Ncxdf6ZPSSporJJFVmSGP1Es187XM9ZhMdMyBgumtsk,9002
 paddleseg/models/emanet.py,sha256=mIwHqWuvXhyVNHJyfs0JHj-tRqlHnvdwV6V2kgexzpE,8326
 paddleseg/models/encnet.py,sha256=JhslSaro-oVyivwfVBJGp1uZpDGkakHcLDbuZo18n8M,7954
 paddleseg/models/enet.py,sha256=DbI3DNGlZfVyGggJOrLMTkR6evwBYNKuNhMqtstSAu4,21755
-paddleseg/models/espnet.py,sha256=Up_hGSLNNBc49U5XGwUYbnqd2PmB631VaqjOWvAe0p0,16979
+paddleseg/models/espnet.py,sha256=mcR2ZC0L3IRmywTmtlYSQ-M5fVHT2jxzPaAQ8srW1mA,17101
 paddleseg/models/espnetv1.py,sha256=LO75OgiPcB1hDe_dXKP1xK-1TZNzD10yNbz29cDZVZc,9993
 paddleseg/models/fast_scnn.py,sha256=5WouE2YwOETXPdk-60OX2K8bXAjRNfbP8IXsxtiXBwM,11996
 paddleseg/models/fastfcn.py,sha256=u7bd29U3Zk60ED1_5jufkW3LGARSXk_QLtnV12jy0Ck,8380
 paddleseg/models/fcn.py,sha256=xlh5VvcVds7g-X6c4PtdtLZt2AvZNQCUZJXW7O8WkeQ,5150
 paddleseg/models/gcnet.py,sha256=b5WWeO9954q8h9RqtwP-TpAEfEWVeGUj0Xxr8X5m2Ys,7998
 paddleseg/models/ginet.py,sha256=3-_Y0XGOsOl1HpJ0UZJfD9MEt5OcpRBA4HIJ_e7o174,10388
 paddleseg/models/glore.py,sha256=-CuogUO7ppGiyus-rVKFtraTxxqrSiWA_nRTKywGMGY,7201
 paddleseg/models/gscnn.py,sha256=VilOX6Nj1YXQe0ArRkMyaqnnhzJ4hgBzjGgu16zKbxw,13216
 paddleseg/models/hardnet.py,sha256=fQ8AgLe6qaw0xzda_zvJYpIBsR3-aTlyVon3vWWC494,11089
-paddleseg/models/hrnet_contrast.py,sha256=nq4TtysL6AJywOD4pizuXQP0ctmBQCbwVZt0Eet4FxA,4630
+paddleseg/models/hrnet_contrast.py,sha256=JJa_T02Icpob8n4zSwOQKjbLUAXNeC0OyQc3whLIHC8,4630
 paddleseg/models/isanet.py,sha256=5uUKdMd8GqluDE4xdauPGFHaPJAZ3KQY325LIJgwAgY,7724
+paddleseg/models/knet.py,sha256=ebbRdyK6N6zv93K5puhLYUTWY_hXthEYlYg9zfKVrVc,20736
+paddleseg/models/lpsnet.py,sha256=geQ495HBA_pn0VBUBQNZZ1rG-POQ2_L97atsi_LUcPk,4824
 paddleseg/models/lraspp.py,sha256=kyRLw1S6BvJzouGf3WkMCeWkYbrsyIJsqMN-41Pjx7k,6385
+paddleseg/models/maskformer.py,sha256=ybTAQ0woa66WEPTE7BQg15eD_qrk8cq51ficCsDWqvc,26677
 paddleseg/models/mla_transformer.py,sha256=w-cWnEBUkbYYnrYVTcN9A5lUuy71GkTuwSGBfGC1U8Q,7555
 paddleseg/models/mobileseg.py,sha256=MJW5dffvq5kBBubpLZ9LvUuMR9SdpcPiSu4B1it3F2w,11412
 paddleseg/models/mscale_ocrnet.py,sha256=kQptTydqCJUsUyYEOit8ZBSbAl7z6znRSLGlZIcZTQQ,13220
 paddleseg/models/ocrnet.py,sha256=WuWTnVEHgE_U3zivDSH3iqBjf-tvjFSmnCqxGP7PY8E,9290
 paddleseg/models/pfpnnet.py,sha256=mQa_doC_SdDBcKlGQcAURUHgt2eFR8Wl0yCAba6ydLQ,7847
 paddleseg/models/pointrend.py,sha256=1MlH-4_w0XBCiYee6rZCKReAIBY-cqNpRPyktRJyJB0,35884
 paddleseg/models/portraitnet.py,sha256=H-r35x40Ez0uIkZPh_bVy-J_TUjGDbXxktXFiPPEYkc,7230
 paddleseg/models/pp_liteseg.py,sha256=Rc6-7Ik18t08FM35A9zWQTjkFm1HqP6z5iVdo77_zkw,10544
+paddleseg/models/pp_mobileseg.py,sha256=f6KczJZzzUcAqOz16PBOOPCB_4G2teieBWK2Lf9dDT8,4180
 paddleseg/models/pphumanseg_lite.py,sha256=pqlz9YNQTqVFCyTIzHENR-uG9kCIdxLmq5BTQh0RZx8,7874
 paddleseg/models/pspnet.py,sha256=iGYQpd2SJJXZ-va_301uQvJct5xF03epYUInKAvLAmE,6018
 paddleseg/models/rtformer.py,sha256=Bf_q3o1xaLZGGkMJGZTph3Xhtp23xdM8KaJLI1EBSzg,27207
 paddleseg/models/segformer.py,sha256=67qMASFMueJh-z-75pa1znYxp9gjzSsdPYS3EDdKy3E,4345
 paddleseg/models/segmenter.py,sha256=yPg5tt9cwbpwbcmDxsxjL-8TgL2mJliBrPjIQ03HYMc,9075
 paddleseg/models/segnet.py,sha256=d7kZFfnzcKtfVJ7glfCiXu4P6qGKk1FzkbxD8kroRkM,5218
+paddleseg/models/segnext.py,sha256=20S4ubwbCaph2PpBAO1PwPsxOlYV_3uPJfJ8NkRFLHY,5976
 paddleseg/models/setr.py,sha256=bgcoWjVywfCujAZeZaR10TrOrAHNbOKsbjFS1ZRZYzQ,16651
 paddleseg/models/sfnet.py,sha256=-uCE3on4xbvKNV9rQqUOI_CYosiK2Qosiz-1Jpqx1xU,8712
 paddleseg/models/sinet.py,sha256=Yctt4EdHJZ3ulmmuv2lSWdsxKJvQPDWDuavRumu2n3Q,13815
 paddleseg/models/stdcseg.py,sha256=0SXQwtrZ_hUc7vbjn6zZSkTnwJwSCGsYBHTANcqcD1o,8141
 paddleseg/models/topformer.py,sha256=WnWB-IUehK5p1UFsiWPlOA-IOoQNO1I0YzzZ8V5P3yI,5497
 paddleseg/models/u2net.py,sha256=-XutqT8nX8knA_iOl0RtGWfQz1rgk0q1rrHf91kRMP4,17553
 paddleseg/models/unet.py,sha256=wBaOpnIiPe8_I1v3ryB3wyT8SKMDCgrJqRhfHfOzsyk,5293
 paddleseg/models/unet_3plus.py,sha256=Bl44kPmPamjmCBtF5vtP8iaVwhEn4TInVnnJV33rqcU,14231
 paddleseg/models/unet_plusplus.py,sha256=Gpvgb_4mSw4JldtznsRRkDlRSM1glQEKkav2JEwwcfw,8424
 paddleseg/models/upernet.py,sha256=lHs0oj4VRYsWt7mvH_ya8rfQkXI-h4Imhsp3WQFppVQ,6397
-paddleseg/models/backbones/__init__.py,sha256=ZyjTCYXzzIvoSjAgfiyVoFoaXlrDUnyaehAa4G6njcM,993
+paddleseg/models/upernet_cae.py,sha256=4vqds68vBTJg_E12BIxjtHfIuUxOBsmgHkTMv7PYbPQ,12497
+paddleseg/models/upernet_vit_adapter.py,sha256=VftV25Y1ZS75AFK568UXWwMctl_rQKhL_Fpupsg55lM,10256
+paddleseg/models/backbones/__init__.py,sha256=KYmrn4vgwMIYXcIJSRftzH4WFwHTEg3_Zp1moLix3AM,1111
+paddleseg/models/backbones/cae.py,sha256=k6F0ctuFphI9EPY2888GDAcZBPHEBNlq_OyIWT6pQdo,21707
 paddleseg/models/backbones/ghostnet.py,sha256=WJ92NcFqyUzExjAxHt_wNvCSwugdxm6qD30Po5NqAEg,11130
+paddleseg/models/backbones/hrformer.py,sha256=egVZ-Lm8CtIjfGK1YggcHabNrCxFRo-MB2fyNEA9Gmc,42666
 paddleseg/models/backbones/hrnet.py,sha256=yzmkcTk3xZi76ygGnxLzDAi8vAdXAPizrgiSGel10iI,29516
 paddleseg/models/backbones/lite_hrnet.py,sha256=tKKiI5ZliJFRkyaq5PLogQxaxFouQO6vBMQQfYOh_Ag,35901
 paddleseg/models/backbones/mix_transformer.py,sha256=DOxWbJdrJAHPTkKlMFYfz-sAJm1V32KxzYOjw4IPIAk,18867
 paddleseg/models/backbones/mobilenetv2.py,sha256=9li6BwfxavV_Q5Ok6rAfEKoEQuS7HB_0zA8k-FLQxik,7995
-paddleseg/models/backbones/mobilenetv3.py,sha256=d18Ad9Jes-Mh0wGWCjcsEin-bh_HpN8W7U3nIb24-eo,15488
+paddleseg/models/backbones/mobilenetv3.py,sha256=bxPsF6ttpalJ-bFHHUw1-RPNCpjZfwFQaXhs0KChv_I,15489
+paddleseg/models/backbones/mscan.py,sha256=N0kKlWXjtwF2D2DSELkP1M_c3sZYoPmeCcZSUAqB8ys,12784
 paddleseg/models/backbones/resnet_vd.py,sha256=cgHklpBXcKi5zMVon7uO1ZrIfi_2xt01IU5dNG3k6no,13290
 paddleseg/models/backbones/shufflenetv2.py,sha256=3WgdBDQEqVCTPVZhChEZsOC6e8CSIYT-6buyU-ZmRRE,9977
 paddleseg/models/backbones/stdcnet.py,sha256=55GPn87lOl5-qilINrqYbv79aexSJhQhWIo43v5vMGg,12282
-paddleseg/models/backbones/swin_transformer.py,sha256=Y4yf1OhQDSqBco3vIIRel1W6Tw7DKCBwIg6tcZUVYNY,27803
+paddleseg/models/backbones/strideformer.py,sha256=knS5xkd5O9_sjNCPr0cKQwu8uVoLjXriPtapXOtM0Bg,26915
+paddleseg/models/backbones/swin_transformer.py,sha256=02NbeUQRBaSvc0DgbNw_m4utJozZHgn6bPkM72vOHEQ,33211
 paddleseg/models/backbones/top_transformer.py,sha256=Mc2HKga4HvxiH06HWXVyoD3eGtUyeHcBFlcXP_kq0jc,22573
-paddleseg/models/backbones/transformer_utils.py,sha256=m89l1WMynDs5EpYImS3XjbopekY5-rBeVnoVNbEc5pk,2577
-paddleseg/models/backbones/uhrnet.py,sha256=aHInqLXj5fqPN6q0RDMoUQ_ewnlTcGRdg9G_kJ8p470,33695
+paddleseg/models/backbones/transformer_utils.py,sha256=FI8MyPz0c0gf5dO67D9x34MDGgL_iHDFPMyhKifYwSI,2592
+paddleseg/models/backbones/uhrnet.py,sha256=eTT7CBrOVXZlWydt61_OCFgOp3RBZazAQWJOS6tIxd4,33695
 paddleseg/models/backbones/vision_transformer.py,sha256=pQ9NoiVhbk4K6bhAjAiNSMlyy2R1YrZqSmn_wRFlLww,12251
+paddleseg/models/backbones/vit_adapter.py,sha256=vN8XnL0kBriHvvAdxCCZtSALFoPoQkW-_b1WVOy2eAA,15122
 paddleseg/models/backbones/xception_deeplab.py,sha256=vssm5yUdvra5Yyn7XgOB4NK4jx2L8wLoZrk1xGPiCps,14070
-paddleseg/models/layers/__init__.py,sha256=OSMdtlaXUK0gDSyrQUcfmBptThuOYId1vgds4tOL8v0,1137
+paddleseg/models/layers/__init__.py,sha256=7DFhHqooCVB82Iq5Fhx5yh1WyQpZNaWpueZklgQ7hdQ,1187
 paddleseg/models/layers/activation.py,sha256=2Yh7P626s8X398NavRBDuSy8wdP3jCvZiFPFNjt6bCQ,2678
 paddleseg/models/layers/attention.py,sha256=Bped8PitmW0wavRVe3RZx2pHR-zziIsmI_Cy2_NoR_U,9657
-paddleseg/models/layers/layer_libs.py,sha256=5ig0fX6rQ8hGCxM-Y2iYBuw9lETI0TGXQu7csNGoKmw,10384
+paddleseg/models/layers/layer_libs.py,sha256=qWhhk-7liNVn_EmYLJG4zaaf3xtbdxex2X2VJk_xkSQ,12232
+paddleseg/models/layers/ms_deformable_attention.py,sha256=fSQLLBioPWyH8MI1v7dOZGNyJzhSEkum4yP9NMOnNdg,7267
+paddleseg/models/layers/nmf_2d.py,sha256=Tjnfo21dHje_lhX_iFeF9pTnkpn2_vK6ef820y0Y4WI,5241
 paddleseg/models/layers/nonlocal2d.py,sha256=o1CB19g-gI_BKZ6-9Tvcz9_zV48y55AQvAMRUB7FD88,5921
 paddleseg/models/layers/polaried_self_attention.py,sha256=nnsxORrnzY3_jbY-xMQJHlBNNutGIfAEBqwmzjseYCg,4453
 paddleseg/models/layers/pyramid_pool.py,sha256=XIxK1l6vPmbbxZ9NN4ueazy8NAWtHL8PzrRjqqFYm1A,6736
 paddleseg/models/layers/tensor_fusion.py,sha256=fubz8xIRJJhPK6so99jRYFEA_P3RgByJ-ZyxpAiX7zE,10360
 paddleseg/models/layers/tensor_fusion_helper.py,sha256=psRWqliiCKxrA8wi0bxhPbzpn_SUa50__7ZvYo15yoY,4216
+paddleseg/models/layers/vit_adapter_layers.py,sha256=lwwaqeVG294Aztke8Iu3MCB0wCjYS2GJVXjFFDl_uf8,14758
 paddleseg/models/layers/wrap_functions.py,sha256=GMmMh9Kk92m9qfG3q_XljV2MkHrVV5fkpP-yw0syiGs,2094
-paddleseg/models/losses/__init__.py,sha256=2dffTMMDbMucpmoitYDWqixdMwz1DijkTjqBJQXWbgc,1728
-paddleseg/models/losses/binary_cross_entropy_loss.py,sha256=J4Es0AHyqUhgo98vGWvv2ecbM3c8eYIMNdBJbQGF8nk,8292
+paddleseg/models/losses/__init__.py,sha256=QMw9Ga3YSnL2p_q9RQngIaFyBG6haVCE2rla7vgQqZs,1772
+paddleseg/models/losses/binary_cross_entropy_loss.py,sha256=EkLawqKzTfhkMumTut2apav_3AREg3K9CMzYNZeIfao,8360
 paddleseg/models/losses/bootstrapped_cross_entropy.py,sha256=eMN5Ac_NTivJr32f2m0KDck6Br2lUO4od5AUHSmpjZY,2734
-paddleseg/models/losses/cross_entropy_loss.py,sha256=4LhFxoJFk-0NK_z473mO4X6r02HbNN2enPRvtCVy30s,9168
+paddleseg/models/losses/cross_entropy_loss.py,sha256=0P1f_1SGh_5Js6wEVa6imO2p-I7jGTb6WES4ZUHBW6o,9472
 paddleseg/models/losses/decoupledsegnet_relax_boundary_loss.py,sha256=_UfR8m_UMAFm-TmipQWyvvlV5ge7ePqLE9yFU3osbbM,5170
 paddleseg/models/losses/detail_aggregate_loss.py,sha256=XiAqzDCcyKysJlrWYPvZIsUHJJ_cVblDqfokprkhcKk,5697
 paddleseg/models/losses/dice_loss.py,sha256=cxhtFrgBhHTQgiPheZrhs2fdoIHFT-icCFyxqkrqfII,2825
 paddleseg/models/losses/edge_attention_loss.py,sha256=mDAXzvIYUPiEn4o4fvfw8E6GWX9wIzA4CJAujdfaW8Y,3100
-paddleseg/models/losses/focal_loss.py,sha256=eq8hxfWrwU8ynMa7iqZK6ABGnH6925ha3dWu3TPAVKg,4825
+paddleseg/models/losses/focal_loss.py,sha256=q5ibZYIpQ6cahMxc4sic7TYnIZsYFLjGs__SgvUX4PM,4893
 paddleseg/models/losses/gscnn_dual_task_loss.py,sha256=BA96scHZZqKWjV_R5x3SoUSyp-RCM8DFySTnOPtBXm8,5172
 paddleseg/models/losses/kl_loss.py,sha256=8fldUm5cyQAlq8JlkpCzPWnCxdydD6AfspIJrVcfZJw,3176
 paddleseg/models/losses/l1_loss.py,sha256=sMJKnpy5zTZkCNgKPWrnnfIeQ6D3bN2Z6BvXrOW34us,4222
 paddleseg/models/losses/lovasz_loss.py,sha256=UVVcQAsZD4urTf5Lq9wjswjQPPqJgmEnl9Tx-npRyh8,8058
+paddleseg/models/losses/maskformer_loss.py,sha256=ZjrgLzxR9FRxJtnKQmXWzt8E1iyHQKLDRJbuWLNn7Hk,18810
 paddleseg/models/losses/mean_square_error_loss.py,sha256=pzdEGXh2JHOIrQVgHcNZSauU7zrpvps5aZYaE6iOJtc,2752
 paddleseg/models/losses/mixed_loss.py,sha256=iQjJKx50LJVelm-ofiWTzOOrkTPgs0BoiWp0x5aWJRo,1984
 paddleseg/models/losses/ohem_cross_entropy_loss.py,sha256=LrpUytflA7VKHZMLwsmzLQ9GaBd_Gd5mgdx8QA9TY60,3894
 paddleseg/models/losses/ohem_edge_attention_loss.py,sha256=cCBLVFXK-kgspjwAmyPS0qH3xG0BqUcjIhSIeLeMJ5g,4633
 paddleseg/models/losses/pixel_contrast_cross_entropy_loss.py,sha256=7dF_x9h4xZub7mw7gnVAtrccf9aEzqFj8kocjA0fK_w,7994
 paddleseg/models/losses/point_cross_entropy_loss.py,sha256=CdVqEOhp0fnbVEOuqwR0s14Yobh-LmNgTBLb6FrcaVM,6446
 paddleseg/models/losses/rmi_loss.py,sha256=QdZEjW4PsD4_xo6e0rRBzXIFMucrTMInzLq8faCywcU,10038
 paddleseg/models/losses/semantic_connectivity_loss.py,sha256=Vzd03HkdIdrdqc3DtqoyFKCSJgHjSM8V_-PEQzrifTI,7540
 paddleseg/models/losses/semantic_encode_cross_entropy_loss.py,sha256=VjHHtTmYJ6GdIAxS2rLAf6jAsinjZo-5YbJHX_ZMm4s,1698
+paddleseg/optimizers/__init__.py,sha256=45GWKdW1txKNvjYpCwqqg6NTC_y4-g9hW2WHOTr5g8g,634
+paddleseg/optimizers/custom_optimizers.py,sha256=9GMM7kRPQV9DZoxkmoIqHISNaRRU6I8QZ6Ou5lqkN3o,8756
+paddleseg/optimizers/optimizer.py,sha256=jWu3o_Mduo9BIajXhUSaLWg0L4qty8cD9VesZts8nkU,10265
 paddleseg/transforms/__init__.py,sha256=4lGYMUffXc5fuHbTSyke_KWpQ6ttspHyRHw3L6CHq7k,661
-paddleseg/transforms/functional.py,sha256=lwQm8DsClu4mbKrLPzioRPgu3vz2pyIWrNPxhPfziWs,5468
-paddleseg/transforms/transforms.py,sha256=1_emsjPpuww2I6hIJk6nIaauX13DG51YASOQEf6Nr40,44145
-paddleseg/utils/__init__.py,sha256=JCfOZS4H-BTwGp_e8rhiI1yhPi4MkDkBPZJqjDQn0eY,827
+paddleseg/transforms/functional.py,sha256=VWswt0uL61KFxpGt1xmPkI5qfBM14gikBX96c7tY2mQ,5587
+paddleseg/transforms/transforms.py,sha256=V6MKeQH9qfhlBUxirC01sPE4QYGPbwpf_376c08jgGM,47887
+paddleseg/utils/__init__.py,sha256=FS-sEqfMRJSE5oXWaP74mLIKHa_CShIb5z3QE_E8l7o,825
 paddleseg/utils/config_check.py,sha256=7IwhdcD_qom14IuumCiK8CakcE7W5W4v2Ha65aU1wJ8,2399
-paddleseg/utils/download.py,sha256=isTT9fXdAvW5dh9id8Q3JZCmfBMd6-j60-EpKsclOkI,5614
-paddleseg/utils/ema.py,sha256=aDkT19buxzqmRFIrsv5-hbQzxd3OjzTApm7x2y5xjcg,3489
+paddleseg/utils/download.py,sha256=l5AOowznzj8UrBrQKtckL4aeBkvEXMHBiAz2MNsLSRw,6038
+paddleseg/utils/ema.py,sha256=0ZZHesekGKwCFGmO3F6OguI03-GhST7kiJg2Gi6F2lM,1648
 paddleseg/utils/logger.py,sha256=7CXyqqr5ZqCuEUK2becft4Om1kL6dg6DquTkQ4hI7t4,1395
-paddleseg/utils/metrics.py,sha256=6uj7V4JNxr9po5UDXXaIsvE1n5Kc-aitcAAe8bx-kbk,8426
+paddleseg/utils/metrics.py,sha256=ZL3Qa59XlJS_y2W_JBmzDkotX62mHUS2qqxgLGZrz1A,8426
 paddleseg/utils/op_flops_funs.py,sha256=6NeCGyYbvpT9ksLBnzOFyboxtur7t54jaUmY6sSMvcY,776
 paddleseg/utils/progbar.py,sha256=zZ3JnPqZ0r-Zw-7McfRqD-hAe3cRDNTRcRzmDNeKFKM,8137
 paddleseg/utils/timer.py,sha256=ySWrrIEVuNgiG1y_MCeWQ11elHSrYAU11ZivST9mVpk,1615
 paddleseg/utils/train_profiler.py,sha256=alo5jHteBDRCp6Tc5rMJmMxt2wfCAgEsCPLgIO7EN6c,4455
-paddleseg/utils/utils.py,sha256=LrEQM3auwi4yMLvagnrr0Iv0wgIGGYmJ5Vy8dvIY6Qc,6779
+paddleseg/utils/utils.py,sha256=pHcq5YRfhg-CPnI7loakHzo_FNtdzW4HtnKavkp1afU,10523
 paddleseg/utils/visualize.py,sha256=3bB6LUMm0bR3VtOuQVlytyCHpAq71Q3m27E9brBoQ0U,4750
 paddleseg/utils/env/__init__.py,sha256=DLKHWM1ZH2ma12gG77FsBy8l7-l0YDrt_bLtk8QHRoo,665
 paddleseg/utils/env/seg_env.py,sha256=gIcOIU8IzzcgK9ybTl561NDYbaXnPZfT2GLKv6skbtA,1909
 paddleseg/utils/env/sys_env.py,sha256=5-RfRQtl80VK1Lo00IBsnVd8w52-p1vWBK-7bLKu5V8,4370
-paddleseg-2.7.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-paddleseg-2.7.0.dist-info/METADATA,sha256=YjhP_aBxWQpFnT4DoE3HxgfZ7gGuuEIlwbrMnsWWRXw,1265
-paddleseg-2.7.0.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
-paddleseg-2.7.0.dist-info/top_level.txt,sha256=mugXAz2EYkb-2b94IVzBJF7N3IsM6Ut1WJ_p-P0dfL0,10
-paddleseg-2.7.0.dist-info/RECORD,,
+paddleseg-2.8.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+paddleseg-2.8.0.dist-info/METADATA,sha256=b39g_25fULRiaDWzQokzvn09AAPD6PzjR4F417fBdTQ,1272
+paddleseg-2.8.0.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
+paddleseg-2.8.0.dist-info/top_level.txt,sha256=mugXAz2EYkb-2b94IVzBJF7N3IsM6Ut1WJ_p-P0dfL0,10
+paddleseg-2.8.0.dist-info/RECORD,,
```

