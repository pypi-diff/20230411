# Comparing `tmp/rec_pangu-0.3.4-py3-none-any.whl.zip` & `tmp/rec_pangu-0.3.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,69 +1,78 @@
-Zip file size: 92668 bytes, number of entries: 67
--rw-r--r--  2.0 unx      199 b- defN 23-Mar-23 15:17 rec_pangu/__init__.py
--rw-r--r--  2.0 unx     5787 b- defN 23-Mar-21 07:25 rec_pangu/benchmark_trainer.py
+Zip file size: 106585 bytes, number of entries: 76
+-rw-r--r--  2.0 unx      199 b- defN 23-Mar-24 04:44 rec_pangu/__init__.py
+-rw-r--r--  2.0 unx     5837 b- defN 23-Apr-03 08:03 rec_pangu/benchmark_trainer.py
 -rw-r--r--  2.0 unx     9722 b- defN 23-Mar-18 15:33 rec_pangu/gpt_ranktrainer.py
--rw-r--r--  2.0 unx    14852 b- defN 23-Mar-21 07:28 rec_pangu/model_pipeline.py
+-rw-r--r--  2.0 unx    14630 b- defN 23-Apr-03 08:03 rec_pangu/model_pipeline.py
 -rw-r--r--  2.0 unx     5448 b- defN 23-Mar-18 15:32 rec_pangu/old_ranktrainer.py
--rw-r--r--  2.0 unx    18576 b- defN 23-Mar-21 07:55 rec_pangu/trainer.py
+-rw-r--r--  2.0 unx    18650 b- defN 23-Apr-03 08:03 rec_pangu/trainer.py
 -rw-r--r--  2.0 unx      404 b- defN 23-Mar-05 10:13 rec_pangu/dataset/__init__.py
--rw-r--r--  2.0 unx     7625 b- defN 23-Mar-21 07:58 rec_pangu/dataset/base_dataset.py
--rw-r--r--  2.0 unx     3776 b- defN 22-Dec-21 14:07 rec_pangu/dataset/graph_dataset.py
--rw-r--r--  2.0 unx     4043 b- defN 23-Mar-21 07:58 rec_pangu/dataset/multi_task_dataset.py
--rw-r--r--  2.0 unx     3746 b- defN 23-Mar-16 16:31 rec_pangu/dataset/process_data.py
--rw-r--r--  2.0 unx     5594 b- defN 23-Mar-21 06:12 rec_pangu/dataset/sequence_dataset.py
+-rw-r--r--  2.0 unx     4954 b- defN 23-Apr-03 08:02 rec_pangu/dataset/base_dataset.py
+-rw-r--r--  2.0 unx     3776 b- defN 23-Apr-03 08:02 rec_pangu/dataset/graph_dataset.py
+-rw-r--r--  2.0 unx     2812 b- defN 23-Apr-03 08:02 rec_pangu/dataset/multi_task_dataset.py
+-rw-r--r--  2.0 unx     3986 b- defN 23-Apr-03 08:02 rec_pangu/dataset/process_data.py
+-rw-r--r--  2.0 unx     5635 b- defN 23-Apr-03 08:02 rec_pangu/dataset/sequence_dataset.py
 -rw-r--r--  2.0 unx      117 b- defN 22-Jul-28 02:44 rec_pangu/models/__init__.py
--rw-r--r--  2.0 unx     7622 b- defN 23-Mar-21 07:59 rec_pangu/models/base_model.py
--rw-r--r--  2.0 unx     8063 b- defN 23-Mar-18 16:28 rec_pangu/models/utils.py
+-rw-r--r--  2.0 unx     9351 b- defN 23-Apr-03 08:03 rec_pangu/models/base_model.py
+-rw-r--r--  2.0 unx     8027 b- defN 23-Apr-03 08:03 rec_pangu/models/utils.py
 -rw-r--r--  2.0 unx      949 b- defN 22-Jul-28 02:44 rec_pangu/models/layers/LGConv.py
--rw-r--r--  2.0 unx      337 b- defN 23-Mar-05 07:15 rec_pangu/models/layers/__init__.py
+-rw-r--r--  2.0 unx      456 b- defN 23-Apr-03 07:29 rec_pangu/models/layers/__init__.py
 -rw-r--r--  2.0 unx     1876 b- defN 23-Mar-20 16:44 rec_pangu/models/layers/activation.py
--rw-r--r--  2.0 unx     4605 b- defN 23-Mar-21 06:13 rec_pangu/models/layers/attention.py
--rw-r--r--  2.0 unx     5678 b- defN 23-Mar-21 08:20 rec_pangu/models/layers/deep.py
--rw-r--r--  2.0 unx     4557 b- defN 23-Mar-20 16:22 rec_pangu/models/layers/embedding.py
--rw-r--r--  2.0 unx     6158 b- defN 23-Mar-16 16:15 rec_pangu/models/layers/graph.py
--rw-r--r--  2.0 unx    11626 b- defN 22-Jul-28 02:44 rec_pangu/models/layers/interaction.py
--rw-r--r--  2.0 unx    15731 b- defN 23-Mar-21 06:35 rec_pangu/models/layers/multi_interest.py
--rw-r--r--  2.0 unx     1088 b- defN 22-Jul-28 02:44 rec_pangu/models/layers/sequence.py
--rw-r--r--  2.0 unx      851 b- defN 22-Jul-28 02:44 rec_pangu/models/layers/shallow.py
+-rw-r--r--  2.0 unx     4606 b- defN 23-Apr-03 08:02 rec_pangu/models/layers/attention.py
+-rw-r--r--  2.0 unx     7468 b- defN 23-Apr-03 08:02 rec_pangu/models/layers/conv.py
+-rw-r--r--  2.0 unx     3943 b- defN 23-Apr-03 08:02 rec_pangu/models/layers/deep.py
+-rw-r--r--  2.0 unx     2932 b- defN 23-Apr-03 08:02 rec_pangu/models/layers/embedding.py
+-rw-r--r--  2.0 unx     6164 b- defN 23-Apr-03 08:02 rec_pangu/models/layers/graph.py
+-rw-r--r--  2.0 unx    12947 b- defN 23-Apr-03 08:02 rec_pangu/models/layers/interaction.py
+-rw-r--r--  2.0 unx     8463 b- defN 23-Apr-03 08:02 rec_pangu/models/layers/multi_interest.py
+-rw-r--r--  2.0 unx     5266 b- defN 23-Apr-03 08:19 rec_pangu/models/layers/sequence.py
+-rw-r--r--  2.0 unx      852 b- defN 23-Apr-03 08:02 rec_pangu/models/layers/shallow.py
+-rw-r--r--  2.0 unx     9674 b- defN 23-Apr-03 08:02 rec_pangu/models/layers/trainformer.py
 -rw-r--r--  2.0 unx      272 b- defN 22-Jul-28 02:44 rec_pangu/models/multi_task/__init__.py
--rw-r--r--  2.0 unx     3946 b- defN 23-Mar-21 08:03 rec_pangu/models/multi_task/aitm.py
+-rw-r--r--  2.0 unx     3948 b- defN 23-Apr-03 08:02 rec_pangu/models/multi_task/aitm.py
 -rw-r--r--  2.0 unx     2609 b- defN 23-Mar-21 08:03 rec_pangu/models/multi_task/essm.py
 -rw-r--r--  2.0 unx     6268 b- defN 23-Mar-21 07:35 rec_pangu/models/multi_task/mlmmoe.py
 -rw-r--r--  2.0 unx     5723 b- defN 23-Mar-21 07:34 rec_pangu/models/multi_task/mmoe.py
 -rw-r--r--  2.0 unx     4599 b- defN 23-Mar-21 07:34 rec_pangu/models/multi_task/omoe.py
 -rw-r--r--  2.0 unx     4105 b- defN 23-Mar-21 07:34 rec_pangu/models/multi_task/sharebottom.py
--rw-r--r--  2.0 unx      423 b- defN 23-Mar-21 07:19 rec_pangu/models/ranking/__init__.py
--rw-r--r--  2.0 unx     2581 b- defN 23-Mar-21 08:03 rec_pangu/models/ranking/afm.py
--rw-r--r--  2.0 unx     4447 b- defN 23-Mar-21 08:03 rec_pangu/models/ranking/afn.py
--rw-r--r--  2.0 unx     5219 b- defN 23-Mar-21 08:03 rec_pangu/models/ranking/aoanet.py
--rw-r--r--  2.0 unx     3663 b- defN 23-Mar-21 08:03 rec_pangu/models/ranking/autoint.py
--rw-r--r--  2.0 unx     4392 b- defN 23-Mar-20 17:01 rec_pangu/models/ranking/ccpm.py
+-rw-r--r--  2.0 unx      452 b- defN 23-Apr-02 09:11 rec_pangu/models/ranking/__init__.py
+-rw-r--r--  2.0 unx     2590 b- defN 23-Apr-03 08:02 rec_pangu/models/ranking/afm.py
+-rw-r--r--  2.0 unx     4454 b- defN 23-Apr-03 08:02 rec_pangu/models/ranking/afn.py
+-rw-r--r--  2.0 unx     5225 b- defN 23-Apr-03 08:02 rec_pangu/models/ranking/aoanet.py
+-rw-r--r--  2.0 unx     3669 b- defN 23-Apr-03 08:02 rec_pangu/models/ranking/autoint.py
+-rw-r--r--  2.0 unx     4402 b- defN 23-Apr-03 08:02 rec_pangu/models/ranking/ccpm.py
 -rw-r--r--  2.0 unx     2604 b- defN 23-Mar-20 17:01 rec_pangu/models/ranking/dcn.py
--rw-r--r--  2.0 unx     2588 b- defN 23-Mar-21 08:03 rec_pangu/models/ranking/deepfm.py
+-rw-r--r--  2.0 unx     2593 b- defN 23-Apr-03 08:02 rec_pangu/models/ranking/deepfm.py
 -rw-r--r--  2.0 unx     3146 b- defN 23-Mar-21 08:03 rec_pangu/models/ranking/fibinet.py
 -rw-r--r--  2.0 unx     1894 b- defN 23-Mar-21 07:29 rec_pangu/models/ranking/fm.py
 -rw-r--r--  2.0 unx     1920 b- defN 22-Jul-28 02:44 rec_pangu/models/ranking/lightgcn.py
 -rw-r--r--  2.0 unx     1668 b- defN 23-Mar-20 17:01 rec_pangu/models/ranking/lr.py
+-rw-r--r--  2.0 unx     3797 b- defN 23-Apr-02 09:21 rec_pangu/models/ranking/masknet.py
 -rw-r--r--  2.0 unx     2875 b- defN 23-Mar-21 08:03 rec_pangu/models/ranking/nfm.py
 -rw-r--r--  2.0 unx     2785 b- defN 23-Mar-21 08:03 rec_pangu/models/ranking/wdl.py
 -rw-r--r--  2.0 unx     3162 b- defN 23-Mar-21 08:03 rec_pangu/models/ranking/xdeepfm.py
--rw-r--r--  2.0 unx      301 b- defN 23-Mar-16 16:20 rec_pangu/models/sequence/__init__.py
--rw-r--r--  2.0 unx     9004 b- defN 23-Mar-23 14:36 rec_pangu/models/sequence/cmi.py
--rw-r--r--  2.0 unx     4755 b- defN 23-Mar-23 14:36 rec_pangu/models/sequence/comirec.py
--rw-r--r--  2.0 unx     2421 b- defN 23-Mar-23 14:37 rec_pangu/models/sequence/mind.py
--rw-r--r--  2.0 unx     2974 b- defN 23-Mar-23 14:37 rec_pangu/models/sequence/narm.py
--rw-r--r--  2.0 unx     8049 b- defN 23-Mar-23 15:17 rec_pangu/models/sequence/re4.py
--rw-r--r--  2.0 unx      113 b- defN 23-Mar-21 06:18 rec_pangu/models/sequence/sasrec.py
--rw-r--r--  2.0 unx     3013 b- defN 23-Mar-23 14:38 rec_pangu/models/sequence/srgnn.py
--rw-r--r--  2.0 unx     1579 b- defN 23-Mar-23 14:38 rec_pangu/models/sequence/yotubednn.py
+-rw-r--r--  2.0 unx      437 b- defN 23-Apr-03 08:24 rec_pangu/models/sequence/__init__.py
+-rw-r--r--  2.0 unx     9009 b- defN 23-Apr-03 07:38 rec_pangu/models/sequence/cmi.py
+-rw-r--r--  2.0 unx     4764 b- defN 23-Apr-03 07:38 rec_pangu/models/sequence/comirec.py
+-rw-r--r--  2.0 unx     3807 b- defN 23-Apr-03 07:38 rec_pangu/models/sequence/gcsan.py
+-rw-r--r--  2.0 unx     2425 b- defN 23-Apr-03 08:02 rec_pangu/models/sequence/mind.py
+-rw-r--r--  2.0 unx     2977 b- defN 23-Apr-03 08:02 rec_pangu/models/sequence/narm.py
+-rw-r--r--  2.0 unx     2146 b- defN 23-Apr-03 07:37 rec_pangu/models/sequence/nextitnet.py
+-rw-r--r--  2.0 unx     3601 b- defN 23-Mar-28 08:15 rec_pangu/models/sequence/niser.py
+-rw-r--r--  2.0 unx     8056 b- defN 23-Apr-03 08:02 rec_pangu/models/sequence/re4.py
+-rw-r--r--  2.0 unx     2780 b- defN 23-Apr-03 08:02 rec_pangu/models/sequence/sasrec.py
+-rw-r--r--  2.0 unx     3014 b- defN 23-Apr-03 08:02 rec_pangu/models/sequence/srgnn.py
+-rw-r--r--  2.0 unx     1779 b- defN 23-Apr-03 08:24 rec_pangu/models/sequence/stamp.py
+-rw-r--r--  2.0 unx     1587 b- defN 23-Apr-03 08:02 rec_pangu/models/sequence/yotubednn.py
+-rw-r--r--  2.0 unx      115 b- defN 23-Apr-11 01:35 rec_pangu/serving/__init__.py
+-rw-r--r--  2.0 unx     2152 b- defN 23-Apr-11 01:56 rec_pangu/serving/ranking_server.py
 -rw-r--r--  2.0 unx      301 b- defN 23-Mar-05 10:33 rec_pangu/utils/__init__.py
--rw-r--r--  2.0 unx     1453 b- defN 22-Aug-11 06:31 rec_pangu/utils/check_version.py
--rw-r--r--  2.0 unx     8700 b- defN 23-Mar-23 14:52 rec_pangu/utils/evaluate.py
+-rw-r--r--  2.0 unx     1509 b- defN 23-Apr-03 08:03 rec_pangu/utils/check_version.py
+-rw-r--r--  2.0 unx     8756 b- defN 23-Apr-03 08:03 rec_pangu/utils/evaluate.py
 -rw-r--r--  2.0 unx     1565 b- defN 23-Mar-20 16:15 rec_pangu/utils/gpu_utils.py
--rw-r--r--  2.0 unx      897 b- defN 23-Mar-20 16:16 rec_pangu/utils/json_utils.py
--rw-r--r--  2.0 unx     1058 b- defN 23-Mar-23 15:20 rec_pangu-0.3.4.dist-info/LICENSE
--rw-r--r--  2.0 unx    12483 b- defN 23-Mar-23 15:20 rec_pangu-0.3.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-23 15:20 rec_pangu-0.3.4.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-Mar-23 15:20 rec_pangu-0.3.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     5956 b- defN 23-Mar-23 15:20 rec_pangu-0.3.4.dist-info/RECORD
-67 files, 284643 bytes uncompressed, 83150 bytes compressed:  70.8%
+-rw-r--r--  2.0 unx      668 b- defN 23-Apr-03 08:03 rec_pangu/utils/json_utils.py
+-rw-r--r--  2.0 unx     1058 b- defN 23-Apr-11 02:09 rec_pangu-0.3.5.dist-info/LICENSE
+-rw-r--r--  2.0 unx    14306 b- defN 23-Apr-11 02:09 rec_pangu-0.3.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-11 02:09 rec_pangu-0.3.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 23-Apr-11 02:09 rec_pangu-0.3.5.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     6776 b- defN 23-Apr-11 02:09 rec_pangu-0.3.5.dist-info/RECORD
+76 files, 317594 bytes uncompressed, 95767 bytes compressed:  69.8%
```

## zipnote {}

```diff
@@ -51,14 +51,17 @@
 
 Filename: rec_pangu/models/layers/activation.py
 Comment: 
 
 Filename: rec_pangu/models/layers/attention.py
 Comment: 
 
+Filename: rec_pangu/models/layers/conv.py
+Comment: 
+
 Filename: rec_pangu/models/layers/deep.py
 Comment: 
 
 Filename: rec_pangu/models/layers/embedding.py
 Comment: 
 
 Filename: rec_pangu/models/layers/graph.py
@@ -72,14 +75,17 @@
 
 Filename: rec_pangu/models/layers/sequence.py
 Comment: 
 
 Filename: rec_pangu/models/layers/shallow.py
 Comment: 
 
+Filename: rec_pangu/models/layers/trainformer.py
+Comment: 
+
 Filename: rec_pangu/models/multi_task/__init__.py
 Comment: 
 
 Filename: rec_pangu/models/multi_task/aitm.py
 Comment: 
 
 Filename: rec_pangu/models/multi_task/essm.py
@@ -129,14 +135,17 @@
 
 Filename: rec_pangu/models/ranking/lightgcn.py
 Comment: 
 
 Filename: rec_pangu/models/ranking/lr.py
 Comment: 
 
+Filename: rec_pangu/models/ranking/masknet.py
+Comment: 
+
 Filename: rec_pangu/models/ranking/nfm.py
 Comment: 
 
 Filename: rec_pangu/models/ranking/wdl.py
 Comment: 
 
 Filename: rec_pangu/models/ranking/xdeepfm.py
@@ -147,32 +156,50 @@
 
 Filename: rec_pangu/models/sequence/cmi.py
 Comment: 
 
 Filename: rec_pangu/models/sequence/comirec.py
 Comment: 
 
+Filename: rec_pangu/models/sequence/gcsan.py
+Comment: 
+
 Filename: rec_pangu/models/sequence/mind.py
 Comment: 
 
 Filename: rec_pangu/models/sequence/narm.py
 Comment: 
 
+Filename: rec_pangu/models/sequence/nextitnet.py
+Comment: 
+
+Filename: rec_pangu/models/sequence/niser.py
+Comment: 
+
 Filename: rec_pangu/models/sequence/re4.py
 Comment: 
 
 Filename: rec_pangu/models/sequence/sasrec.py
 Comment: 
 
 Filename: rec_pangu/models/sequence/srgnn.py
 Comment: 
 
+Filename: rec_pangu/models/sequence/stamp.py
+Comment: 
+
 Filename: rec_pangu/models/sequence/yotubednn.py
 Comment: 
 
+Filename: rec_pangu/serving/__init__.py
+Comment: 
+
+Filename: rec_pangu/serving/ranking_server.py
+Comment: 
+
 Filename: rec_pangu/utils/__init__.py
 Comment: 
 
 Filename: rec_pangu/utils/check_version.py
 Comment: 
 
 Filename: rec_pangu/utils/evaluate.py
@@ -180,23 +207,23 @@
 
 Filename: rec_pangu/utils/gpu_utils.py
 Comment: 
 
 Filename: rec_pangu/utils/json_utils.py
 Comment: 
 
-Filename: rec_pangu-0.3.4.dist-info/LICENSE
+Filename: rec_pangu-0.3.5.dist-info/LICENSE
 Comment: 
 
-Filename: rec_pangu-0.3.4.dist-info/METADATA
+Filename: rec_pangu-0.3.5.dist-info/METADATA
 Comment: 
 
-Filename: rec_pangu-0.3.4.dist-info/WHEEL
+Filename: rec_pangu-0.3.5.dist-info/WHEEL
 Comment: 
 
-Filename: rec_pangu-0.3.4.dist-info/top_level.txt
+Filename: rec_pangu-0.3.5.dist-info/top_level.txt
 Comment: 
 
-Filename: rec_pangu-0.3.4.dist-info/RECORD
+Filename: rec_pangu-0.3.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## rec_pangu/__init__.py

```diff
@@ -1,9 +1,9 @@
 # -*- ecoding: utf-8 -*-
 # @ModuleName: __init__
 # @Author: wk
 # @Email: 306178200@qq.com
 # @Time: 2022/6/10 8:20 PM
 from .utils import check_version
 
-__version__ = '0.3.4'
+__version__ = '0.3.5'
 check_version(__version__)
```

## rec_pangu/benchmark_trainer.py

```diff
@@ -10,25 +10,26 @@
 import time
 import os
 from rec_pangu.trainer import RankTrainer
 from rec_pangu.models.ranking import *
 from rec_pangu.models.multi_task import *
 from loguru import logger
 
+
 class BenchmarkTrainer:
     """
     BenchmarkTrainer is used to train models and store training logs in a pandas dataframe.
     """
 
     def __init__(
-        self,
-        num_task: int = 1,
-        model_list: Optional[List[str]] = None,
-        benchmark_res_path: Optional[str] = None,
-        ckpt_root: str = './benchmark_ckpt'
+            self,
+            num_task: int = 1,
+            model_list: Optional[List[str]] = None,
+            benchmark_res_path: Optional[str] = None,
+            ckpt_root: str = './benchmark_ckpt'
     ) -> None:
         """
         Args:
             num_task: The number of tasks being trained on.
             model_list: The list of models to train.
             benchmark_res_path: The filepath to store training logs.
             ckpt_root: The directory to store the trained models' checkpoints.
@@ -36,22 +37,22 @@
         self.num_task = num_task
         self.model_list = model_list
         self.benchmark_res_df = pd.DataFrame()
         self.benchmark_res_path = benchmark_res_path
         self.ckpt_root = ckpt_root
 
     def run(
-        self,
-        train_loader: DataLoader,
-        enc_dict: Dict[str, int],
-        valid_loader: Optional[DataLoader] = None,
-        test_loader: Optional[DataLoader] = None,
-        epoch: int = 10,
-        lr: float = 1e-3,
-        device: torch.device = torch.device('cpu')
+            self,
+            train_loader: DataLoader,
+            enc_dict: Dict[str, int],
+            valid_loader: Optional[DataLoader] = None,
+            test_loader: Optional[DataLoader] = None,
+            epoch: int = 10,
+            lr: float = 1e-3,
+            device: torch.device = torch.device('cpu')
     ) -> None:
         """Train and evaluate models on given data loaders and store logs.
 
         Args:
             train_loader: The data loader for training data.
             enc_dict: Dictionary of word embeddings.
             valid_loader: The data loader for validation data.
@@ -132,10 +133,7 @@
 #             logger.info(f'Model {model_name} Training Log :{log_dict}')
 #             log_dict.update(valid_metric)
 #             log_dict.update(test_metric)
 #             self.benchmark_res_df = self.benchmark_res_df.append(log_dict, ignore_index=True)
 #             self.benchmark_res_df.to_csv(self.benhcmark_res_path, index=False)
 #
 #
-
-
-
```

## rec_pangu/model_pipeline.py

```diff
@@ -40,15 +40,15 @@
     """
     model.train()
     max_iter = int(train_loader.dataset.__len__() / train_loader.batch_size)
     if num_task == 1:
         pred_list = []
         label_list = []
         start_time = time.time()
-        for idx,data in enumerate(train_loader):
+        for idx, data in enumerate(train_loader):
 
             for key in data.keys():
                 data[key] = data[key].to(device)
 
             output = model(data)
             pred = output['pred']
             loss = output['loss']
@@ -59,71 +59,78 @@
 
             pred_list.extend(pred.squeeze(-1).cpu().detach().numpy())
             label_list.extend(data['label'].squeeze(-1).cpu().detach().numpy())
 
             auc = round(roc_auc_score(label_list[-1000:], pred_list[-1000:]), 4)
 
             if use_wandb:
-                wandb.log({'train_loss':loss.item(),
-                           'train_auc':auc})
+                wandb.log({'train_loss': loss.item(),
+                           'train_auc': auc})
 
             iter_time = time.time() - start_time
-            remaining_time = round(((iter_time / (idx+1)) * (max_iter - idx + 1)) / 60, 2)
+            remaining_time = round(((iter_time / (idx + 1)) * (max_iter - idx + 1)) / 60, 2)
 
             if idx % log_rounds == 0 and device.type != 'cpu':
-                logger.info(f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)} AUC:{auc} GPU Mem:{get_gpu_usage(device)}')
+                logger.info(
+                    f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)} AUC:{auc} GPU Mem:{get_gpu_usage(device)}')
             elif idx % log_rounds == 0:
-                logger.info(f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)} AUC:{auc}')
+                logger.info(
+                    f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)} AUC:{auc}')
         res_dict = dict()
         for metric in metric_list:
             assert metric in ['roc_auc_score', 'log_loss'], 'metric :{} not supported! metric must be in {}'.format(
                 metric, ['roc_auc_score', 'log_loss'])
-            if metric =='log_loss':
-                res_dict[f'train_{metric}'] = round(log_loss(label_list,pred_list, eps=1e-7),4)
+            if metric == 'log_loss':
+                res_dict[f'train_{metric}'] = round(log_loss(label_list, pred_list, eps=1e-7), 4)
             else:
-                res_dict[f'train_{metric}'] = round(eval(metric)(label_list,pred_list),4)
+                res_dict[f'train_{metric}'] = round(eval(metric)(label_list, pred_list), 4)
         return res_dict
     else:
         multi_task_pred_list = [[] for _ in range(num_task)]
         multi_task_label_list = [[] for _ in range(num_task)]
         start_time = time.time()
-        for idx,data in enumerate(train_loader):
+        for idx, data in enumerate(train_loader):
 
             for key in data.keys():
                 data[key] = data[key].to(device)
 
             output = model(data)
             loss = output['loss']
 
             loss.backward()
             optimizer.step()
             model.zero_grad()
             for i in range(num_task):
                 multi_task_pred_list[i].extend(list(output[f'task{i + 1}_pred'].squeeze(-1).cpu().detach().numpy()))
                 multi_task_label_list[i].extend(list(data[f'task{i + 1}_label'].squeeze(-1).cpu().detach().numpy()))
             if use_wandb:
-                wandb.log({'train_loss':loss.item()})
+                wandb.log({'train_loss': loss.item()})
             iter_time = time.time() - start_time
             remaining_time = round(((iter_time / (idx + 1)) * (max_iter - idx + 1)) / 60, 2)
-            if idx % log_rounds ==0 and device.type != 'cpu':
-                logger.info(f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)} GPU Mem:{get_gpu_usage(device)}')
-            elif idx % log_rounds ==0:
-                logger.info(f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)}')
+            if idx % log_rounds == 0 and device.type != 'cpu':
+                logger.info(
+                    f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)} GPU Mem:{get_gpu_usage(device)}')
+            elif idx % log_rounds == 0:
+                logger.info(
+                    f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)}')
 
         res_dict = dict()
         for i in range(num_task):
             for metric in metric_list:
                 assert metric in ['roc_auc_score', 'log_loss'], 'metric :{} not supported! metric must be in {}'.format(
                     metric, ['roc_auc_score', 'log_loss'])
                 if metric == 'log_loss':
-                    res_dict[f'train_task{i+1}_{metric}'] = round(log_loss(multi_task_label_list[i], multi_task_pred_list[i], eps=1e-7),4)
+                    res_dict[f'train_task{i + 1}_{metric}'] = round(
+                        log_loss(multi_task_label_list[i], multi_task_pred_list[i], eps=1e-7), 4)
                 else:
-                    res_dict[f'train_task{i+1}_{metric}'] = round(eval(metric)(multi_task_label_list[i], multi_task_pred_list[i]),4)
+                    res_dict[f'train_task{i + 1}_{metric}'] = round(
+                        eval(metric)(multi_task_label_list[i], multi_task_pred_list[i]), 4)
         return res_dict
 
+
 def test_model(model: torch.nn.Module,
                test_loader: torch.utils.data.DataLoader,
                device: torch.device,
                metric_list: List[str] = ['roc_auc_score', 'log_loss'],
                num_task: int = 1) -> dict:
     """
     Evaluate the performance of a given model on the test set.
@@ -162,19 +169,20 @@
             label_list.extend(data['label'].squeeze(-1).cpu().detach().numpy())
 
         # Initialize the results dictionary
         res_dict = dict()
 
         # Compute evaluation metric for each supported metric
         for metric in metric_list:
-            assert metric in ['roc_auc_score','log_loss'], f"Unsupported metric: {metric}. Supported metrics are ['roc_auc_score','log_loss']."
+            assert metric in ['roc_auc_score',
+                              'log_loss'], f"Unsupported metric: {metric}. Supported metrics are ['roc_auc_score','log_loss']."
             if metric == 'log_loss':
-                res_dict[metric] = round(log_loss(label_list, pred_list, eps=1e-7),4)
+                res_dict[metric] = round(log_loss(label_list, pred_list, eps=1e-7), 4)
             else:
-                res_dict[metric] = round(eval(metric)(label_list, pred_list),4)
+                res_dict[metric] = round(eval(metric)(label_list, pred_list), 4)
 
         return res_dict
 
     else:
         multi_task_pred_list = [[] for _ in range(num_task)]
         multi_task_label_list = [[] for _ in range(num_task)]
 
@@ -195,22 +203,26 @@
 
         # Initialize the results dictionary
         res_dict = dict()
 
         # Compute the evaluation metric for each task and each supported metric
         for i in range(num_task):
             for metric in metric_list:
-                assert metric in ['roc_auc_score', 'log_loss'], f"Unsupported metric: {metric}. Supported metrics are ['roc_auc_score','log_loss']."
+                assert metric in ['roc_auc_score',
+                                  'log_loss'], f"Unsupported metric: {metric}. Supported metrics are ['roc_auc_score','log_loss']."
                 if metric == 'log_loss':
-                    res_dict[f'test_task{i + 1}_{metric}'] = round(log_loss(multi_task_label_list[i], multi_task_pred_list[i], eps=1e-7),4)
+                    res_dict[f'test_task{i + 1}_{metric}'] = round(
+                        log_loss(multi_task_label_list[i], multi_task_pred_list[i], eps=1e-7), 4)
                 else:
-                    res_dict[f'test_task{i + 1}_{metric}'] = round(eval(metric)(multi_task_label_list[i], multi_task_pred_list[i]),4)
+                    res_dict[f'test_task{i + 1}_{metric}'] = round(
+                        eval(metric)(multi_task_label_list[i], multi_task_pred_list[i]), 4)
 
         return res_dict
 
+
 def train_sequence_model(model: torch.nn.Module,
                          train_loader: torch.utils.data.DataLoader,
                          optimizer: torch.optim.Optimizer,
                          device: torch.device,
                          use_wandb: bool = False,
                          log_rounds: int = 100):
     """
@@ -255,17 +267,20 @@
 
         # Calculate time for iteration and remaining time
         iter_time = time.time() - start_time
         remaining_time = round(((iter_time / (idx + 1)) * (max_iter - idx + 1)) / 60, 2)
 
         # Log progress
         if idx % log_rounds == 0 and device.type != 'cpu':
-            logger.info(f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)} GPU Mem:{get_gpu_usage(device)}')
+            logger.info(
+                f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)} GPU Mem:{get_gpu_usage(device)}')
         elif idx % log_rounds == 0:
-            logger.info(f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)} ')
+            logger.info(
+                f'Iter {idx}/{max_iter} Remaining time:{remaining_time} min Loss:{round(float(loss.detach().cpu().numpy()), 4)} ')
+
 
 def test_sequence_model(model: torch.nn.Module,
                         test_loader: torch.utils.data.DataLoader,
                         device: torch.device,
                         topk_list: List[int] = [20, 50, 100],
                         use_wandb: bool = False) -> dict:
     """
@@ -298,33 +313,19 @@
 
     # Log metrics using Weights & Biases
     if use_wandb:
         wandb.log(metric_dict)
 
     return metric_dict
 
-# def test_sequence_model(model, test_loader, device, topk_list=[20,50,100],use_wandb=False):
-#     model.eval()
-#     test_gd = test_loader.dataset.get_test_gd()
-#     preds = get_recall_predict(model, test_loader, device, topN=200)
-#
-#     metric_dict = dict()
-#     for i, k in enumerate(topk_list):
-#         temp_metric_dict = evaluate_recall(preds, test_gd, k)
-#         logger.info(temp_metric_dict)
-#         metric_dict.update(temp_metric_dict)
-#
-#     if use_wandb:
-#         wandb.log(metric_dict)
-#     return metric_dict
 
 def train_graph_model(model, train_dataset, optimizer, device, batch_size=1024):
     model.train()
     epoch_loss = 0
-    pbar = tqdm(range(train_dataset.__len__()//batch_size))
+    pbar = tqdm(range(train_dataset.__len__() // batch_size))
     for _ in pbar:
 
         data = train_dataset.sample(batch_size)
 
         for key in data.keys():
             data[key] = data[key].to(device)
 
@@ -335,14 +336,15 @@
         optimizer.step()
         model.zero_grad()
 
         epoch_loss += loss.item()
         pbar.set_description("Loss {}".format(round(epoch_loss, 4)))
     return epoch_loss
 
+
 def test_graph_model(model, train_gd, test_gd, hidden_size, topN=50):
     model.eval()
     output = model(None, is_training=False)
     user_embs = output['user_emb'].detach().cpu().numpy()
     item_embs = output['item_emb'].detach().cpu().numpy()
 
     test_user_list = list(test_gd.keys())
@@ -357,8 +359,7 @@
         batch_user_emb = user_embs[user_ids, :]
         D, I = faiss_index.search(batch_user_emb, 1000)
 
         for i, iid_list in enumerate(user_ids):  # 每个用户的label列表，此处item_id为一个二维list，验证和测试是多label的
             train_items = train_gd.get(user_ids[i], [])
             preds[user_ids[i]] = [x for x in list(I[i, :]) if x not in train_items]
     return evaluate_recall(preds, test_gd, topN=topN)
-
```

## rec_pangu/trainer.py

```diff
@@ -4,23 +4,25 @@
 # @Email: 306178200@qq.com
 # @Time: 2022/6/10 7:40 PM
 """
 Model Trainer
 """
 import os
 import torch
-from .model_pipeline import train_model, test_model, train_graph_model, test_graph_model, train_sequence_model, test_sequence_model
+from .model_pipeline import train_model, test_model, train_graph_model, test_graph_model, train_sequence_model, \
+    test_sequence_model
 from .utils import beautify_json
-from .dataset import BaseDataset,MultiTaskDataset
+from .dataset import BaseDataset, MultiTaskDataset
 from loguru import logger
 import torch.utils.data as D
 import wandb
 from typing import List, Optional
 import pandas as pd
 
+
 class RankTrainer:
     """
     A class for training ranking models with single or multiple tasks.
 
     Attributes:
         num_task (int): The number of tasks for the model.
         wandb_config (dict): Configuration for Weights and Biases integration.
@@ -188,15 +190,15 @@
                     data[key] = data[key].to(device)
                 output = model(data, is_training=False)
                 for i in range(self.num_task):
                     multi_task_pred_list[i].extend(list(output[f'task{i + 1}_pred'].squeeze(-1).cpu().detach().numpy()))
             return multi_task_pred_list
 
     def predict_dataframe(self, model, test_df, enc_dict: dict, schema: dict,
-                          device:torch.device = torch.device('cpu'), batch_size: int = 1024):
+                          device: torch.device = torch.device('cpu'), batch_size: int = 1024):
         """
         Make predictions for the data in the given DataFrame using the model, encoding dictionary, and schema.
 
         Args:
             model (nn.Module): The model to make predictions with.
             test_df (pd.DataFrame): The DataFrame containing the test data.
             enc_dict (dict): The encoding dictionary.
@@ -210,18 +212,20 @@
         if schema['task_type'] == 'ranking':
             test_dataset = BaseDataset(schema, test_df, enc_dict=enc_dict)
         elif schema['task_type'] == 'multitask':
             test_dataset = MultiTaskDataset(schema, test_df, enc_dict=enc_dict)
         test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
         return self.predict_dataloader(model, test_loader, device=device)
 
+
 class SequenceTrainer:
     """
     Sequence Trainer class for training and evaluating sequence models.
     """
+
     def __init__(self, wandb_config: Optional[dict] = None,
                  model_ckpt_dir: str = './model_ckpt'):
         """
         Initializes the SequenceTrainer class.
 
         Args:
             wandb_config (Optional[dict], optional): Wandb configuration dictionary. Defaults to None.
@@ -366,30 +370,31 @@
             model_str (str): String to add to the model file name.
         """
         os.makedirs(model_ckpt_dir, exist_ok=True, mode=0o777)
         save_dict = {'model': model.state_dict()}
         torch.save(save_dict, os.path.join(model_ckpt_dir, f'model_{model_str}.pth'))
         logger.info(f'Model Saved to {model_ckpt_dir}')
 
+
 class GraphTrainer:
     def __init__(self):
         logger.info("Graph Trainer")
 
-    def fit(self,model,train_dataset,epoch,lr,device=torch.device('cpu'),batch_size=1024):
+    def fit(self, model, train_dataset, epoch, lr, device=torch.device('cpu'), batch_size=1024):
         optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
 
-        for i in range(1, epoch+1):
-            epoch_loss = train_graph_model(model=model,train_dataset=train_dataset,optimizer=optimizer,device=device,batch_size=batch_size)
+        for i in range(1, epoch + 1):
+            epoch_loss = train_graph_model(model=model, train_dataset=train_dataset, optimizer=optimizer, device=device,
+                                           batch_size=batch_size)
             logger.info(f"Epoch:{i}/{epoch} Train Loss:{epoch_loss}")
 
     def save_model(self, model, model_ckpt_dir):
         os.makedirs(model_ckpt_dir, exist_ok=True, mode=0o777)
         save_dict = {'model': model.state_dict()}
-        torch.save(save_dict,os.path.join(model_ckpt_dir, 'model.pth'))
+        torch.save(save_dict, os.path.join(model_ckpt_dir, 'model.pth'))
 
-    def evaluate_model(self, model, train_dataset,test_dataset,hidden_size,topN=50):
+    def evaluate_model(self, model, train_dataset, test_dataset, hidden_size, topN=50):
         train_gd = train_dataset.generate_test_gd()
         test_gd = test_dataset.generate_test_gd()
-        test_metric = test_graph_model(model,train_gd=train_gd,test_gd=test_gd,hidden_size=hidden_size,topN=topN)
+        test_metric = test_graph_model(model, train_gd=train_gd, test_gd=test_gd, hidden_size=hidden_size, topN=topN)
         logger.info(f"Test Metric:{beautify_json(test_metric)}")
         return test_metric
-
```

## rec_pangu/dataset/base_dataset.py

```diff
@@ -127,79 +127,7 @@
         """
         Returns the length of the dataset.
 
         Returns:
             The length of the dataset.
         """
         return len(self.df)
-# from typing import Dict, List
-# import pandas as pd
-# import numpy as np
-# import torch
-# from torch.utils.data import Dataset
-# from collections import defaultdict
-#
-#
-# class BaseDataset(Dataset):
-#     def __init__(self,
-#                  config: dict,
-#                  df: pd.DataFrame,
-#                  enc_dict: Dict[str, dict] = None):
-#         self.config = config
-#         self.df = df
-#         self.enc_dict = enc_dict
-#         self.df = self.df.rename(columns={self.config['label_col']: 'label'})
-#         self.dense_cols = list(set(self.config['dense_cols']))
-#         self.sparse_cols = list(set(self.config['sparse_cols']))
-#         self.feature_name = self.dense_cols+self.sparse_cols
-#
-#         #数据编码
-#         if self.enc_dict == None:
-#             self.get_enc_dict()
-#         self.enc_data()
-#
-#     def get_enc_dict(self):
-#         #计算enc_dict
-#         self.enc_dict = dict(zip( list(self.dense_cols+self.sparse_cols),[dict() for _ in range(len(self.dense_cols+self.sparse_cols))]))
-#         for f in self.sparse_cols:
-#             self.df[f] = self.df[f].astype('str')
-#             map_dict = dict(zip(sorted(self.df[f].unique()), range(self.df[f].nunique())))
-#             self.enc_dict[f] = map_dict
-#             self.enc_dict[f]['vocab_size'] = self.df[f].nunique()
-#
-#         for f in self.dense_cols:
-#             self.enc_dict[f]['min'] = self.df[f].min()
-#             self.enc_dict[f]['max'] = self.df[f].max()
-#
-#         return self.enc_dict
-#
-#     def enc_dense_data(self,col):
-#         return (self.df[col] - self.enc_dict[col]['min']) / (self.enc_dict[col]['max'] - self.enc_dict[col]['min'] + 1e-5)
-#
-#     def enc_sparse_data(self,col):
-#         return self.df[col].apply(lambda x : self.enc_dict[col].get(x,self.enc_dict[col]['vocab_size']))
-#
-#     def enc_data(self):
-#         #使用enc_dict对数据进行编码
-#         self.enc_data = defaultdict(np.array)
-#
-#         for col in self.dense_cols:
-#             self.enc_data[col] = torch.Tensor(np.array(self.enc_dense_data(col)))
-#         for col in self.sparse_cols:
-#             self.enc_data[col] = torch.Tensor(np.array(self.enc_sparse_data(col))).long()
-#
-#     def __getitem__(self, index):
-#         data = defaultdict(np.array)
-#         for col in self.dense_cols:
-#             data[col] = self.enc_data[col][index]
-#         for col in self.sparse_cols:
-#             data[col] = self.enc_data[col][index]
-#         if 'label' in self.df.columns:
-#             data['label'] = torch.Tensor([self.df['label'].iloc[index]]).squeeze(-1)
-#         return data
-#
-#     def __len__(self):
-#         return len(self.df)
-#
-#
-#
-#
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## rec_pangu/dataset/graph_dataset.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,14 @@
 from dgl import DGLGraph
 import numpy as np
 import torch
 from torch.utils.data import Dataset
 import random
 
+
 class GeneralGraphDataset(Dataset):
     def __init__(self, df, num_user, num_item, phase='train'):
         self.df = df
         self.n_item = self.df['item_id'].nunique()
         self.phase = phase
         self.num_user = num_user
         self.num_item = num_item
@@ -97,8 +98,7 @@
         return data
 
     def __len__(self):
         if self.phase == 'train':
             return len(self.df)
         else:
             return self.df['user_id'].nunique()
-
```

## rec_pangu/dataset/multi_task_dataset.py

```diff
@@ -76,39 +76,7 @@
         """
         Gets the length of the dataset.
 
         Returns:
             The length of the dataset.
         """
         return len(self.df)
-
-
-# class MultiTaskDataset(BaseDataset):
-#     def __init__(self,config,df,enc_dict=None):
-#         self.config = config
-#         self.df = df
-#         self.enc_dict = enc_dict
-#         for idx, col in enumerate(self.config['label_col']):
-#             self.df = self.df.rename(columns={col :f'task{idx + 1}_label'})
-#         self.dense_cols = list(set(self.config['dense_cols']))
-#         self.sparse_cols = list(set(self.config['sparse_cols']))
-#         self.feature_name = self.dense_cols+self.sparse_cols+['label']
-#
-#         #数据编码
-#         if self.enc_dict == None:
-#             self.get_enc_dict()
-#         self.enc_data()
-#
-#     def __getitem__(self, index):
-#         data = defaultdict(np.array)
-#         for col in self.dense_cols:
-#             data[col] = self.enc_data[col][index]
-#         for col in self.sparse_cols:
-#             data[col] = self.enc_data[col][index]
-#         for idx, col in enumerate(self.config['label_col']):
-#             if f'task{idx + 1}_label' in self.df.columns:
-#                 data[f'task{idx + 1}_label'] = torch.Tensor([self.df[f'task{idx + 1}_label'].iloc[index]]).squeeze(-1)
-#         return data
-#
-#     def __len__(self):
-#         return len(self.df)
-#
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## rec_pangu/dataset/process_data.py

```diff
@@ -1,68 +1,75 @@
 # -*- ecoding: utf-8 -*-
 # @ModuleName: process_data
 # @Author: wk
 # @Email: 306178200@qq.com
 # @Time: 2022/6/10 7:40 PM
-
 from .base_dataset import BaseDataset
 from .multi_task_dataset import MultiTaskDataset
-from .sequence_dataset import SequenceDataset,seq_collate
+from .sequence_dataset import SequenceDataset
 import torch.utils.data as D
 
-def get_base_dataloader(train_df, valid_df, test_df, schema, batch_size = 512*3):
 
-    train_dataset = BaseDataset(schema,train_df)
+def get_base_dataloader(train_df, valid_df, test_df, schema, batch_size=512 * 3):
+    train_dataset = BaseDataset(schema, train_df)
     enc_dict = train_dataset.get_enc_dict()
-    valid_dataset = BaseDataset(schema, valid_df,enc_dict=enc_dict)
-    test_dataset = BaseDataset(schema, test_df,enc_dict=enc_dict)
+    valid_dataset = BaseDataset(schema, valid_df, enc_dict=enc_dict)
+    test_dataset = BaseDataset(schema, test_df, enc_dict=enc_dict)
 
-    train_loader = D.DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=0, pin_memory=True)
-    valid_loader = D.DataLoader(valid_dataset,batch_size=batch_size,shuffle=False,num_workers=0, pin_memory=True)
-    test_loader = D.DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=0, pin_memory=True)
+    train_loader = D.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)
+    valid_loader = D.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)
+    test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)
 
-    return train_loader,valid_loader,test_loader, enc_dict
+    return train_loader, valid_loader, test_loader, enc_dict
 
-def get_multi_task_dataloader(train_df, valid_df, test_df, schema, batch_size = 512*3):
 
-    train_dataset = MultiTaskDataset(schema,train_df)
+def get_multi_task_dataloader(train_df, valid_df, test_df, schema, batch_size=512 * 3):
+    train_dataset = MultiTaskDataset(schema, train_df)
     enc_dict = train_dataset.get_enc_dict()
-    valid_dataset = MultiTaskDataset(schema, valid_df,enc_dict=enc_dict)
-    test_dataset = MultiTaskDataset(schema, test_df,enc_dict=enc_dict)
+    valid_dataset = MultiTaskDataset(schema, valid_df, enc_dict=enc_dict)
+    test_dataset = MultiTaskDataset(schema, test_df, enc_dict=enc_dict)
 
-    train_loader = D.DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=4, pin_memory=True)
-    valid_loader = D.DataLoader(valid_dataset,batch_size=batch_size,shuffle=False,num_workers=0, pin_memory=True)
-    test_loader = D.DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=0, pin_memory=True)
+    train_loader = D.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
+    valid_loader = D.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)
+    test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)
 
-    return train_loader,valid_loader,test_loader, enc_dict
+    return train_loader, valid_loader, test_loader, enc_dict
 
-def get_sequence_dataloader(train_df, valid_df, test_df, schema, batch_size = 512*3):
+
+def get_sequence_dataloader(train_df, valid_df, test_df, schema, batch_size=512 * 3):
     train_dataset = SequenceDataset(schema, df=train_df, phase='train')
     enc_dict = train_dataset.get_enc_dict()
     valid_dataset = SequenceDataset(schema, df=valid_df, enc_dict=enc_dict, phase='test')
     test_dataset = SequenceDataset(schema, df=test_df, enc_dict=enc_dict, phase='test')
 
-    train_loader = D.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)
-    valid_loader = D.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)
-    test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)
+    train_loader = D.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
+                                num_workers=0, pin_memory=True, drop_last=True)
+    valid_loader = D.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,
+                                num_workers=0, pin_memory=True, drop_last=True)
+    test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,
+                               num_workers=0, pin_memory=True, drop_last=True)
 
     return train_loader, valid_loader, test_loader, enc_dict
 
-def get_dataloader(train_df, valid_df, test_df, schema, batch_size=512*3):
-    if schema['task_type']=='ranking':
+
+def get_dataloader(train_df, valid_df, test_df, schema, batch_size=512 * 3):
+    if schema['task_type'] == 'ranking':
         return get_base_dataloader(train_df, valid_df, test_df, schema, batch_size=batch_size)
-    elif schema['task_type']=='multitask':
+    elif schema['task_type'] == 'multitask':
         return get_multi_task_dataloader(train_df, valid_df, test_df, schema, batch_size=batch_size)
-    elif schema['task_type']=='sequence':
+    elif schema['task_type'] == 'sequence':
         return get_sequence_dataloader(train_df, valid_df, test_df, schema, batch_size=batch_size)
     else:
         raise Exception(f"""task_type:{schema['task_type']} must be in ['ranking','multitask','sequence']""")
 
-def get_single_dataloader(test_df, schema, enc_dict, batch_size = 512, num_workers=0):
+
+def get_single_dataloader(test_df, schema, enc_dict, batch_size=512, num_workers=0):
     if isinstance(schema['label_col'], list):
-        test_dataset = MultiTaskDataset(schema, test_df,enc_dict=enc_dict)
-        test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
+        test_dataset = MultiTaskDataset(schema, test_df, enc_dict=enc_dict)
+        test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,
+                                   pin_memory=True)
         return test_loader
     else:
-        test_dataset = BaseDataset(schema, test_df,enc_dict=enc_dict)
-        test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
-        return test_loader
+        test_dataset = BaseDataset(schema, test_df, enc_dict=enc_dict)
+        test_loader = D.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,
+                                   pin_memory=True)
+        return test_loader
```

## rec_pangu/dataset/sequence_dataset.py

```diff
@@ -3,57 +3,58 @@
 # @Author: wk
 # @Email: 306178200@qq.com
 # @Time: 2023/3/3 14:07
 import torch
 from torch.utils.data import Dataset
 import random
 
+
 class SequenceDataset(Dataset):
-    def __init__(self,config,df,enc_dict=None,phase='train'):
+    def __init__(self, config, df, enc_dict=None, phase='train'):
         self.config = config
         self.df = df
         self.enc_dict = enc_dict
         self.max_length = self.config['max_length']
         self.user_col = self.config['user_col']
         self.item_col = self.config['item_col']
         self.time_col = self.config.get('time_col', None)
         self.cate_cols = self.config.get('cate_cols', [])
 
         if self.time_col:
             self.df = self.df.sort_values(by=[self.user_col, self.time_col])
 
-        if self.enc_dict==None:
+        if self.enc_dict == None:
             self.get_enc_dict()
         self.enc_data()
 
         self.user2item = self.df.groupby(self.user_col)[self.item_col].apply(list).to_dict()
         for col in self.cate_cols:
-            setattr(self,f'user2{col}',self.df.groupby(self.user_col)[col].apply(list).to_dict())
+            setattr(self, f'user2{col}', self.df.groupby(self.user_col)[col].apply(list).to_dict())
 
         self.user_list = self.df[self.user_col].unique()
         self.phase = phase
 
     def get_enc_dict(self):
-        #计算enc_dict
-        if self.enc_dict==None:
-            sparse_cols = [self.item_col]+self.cate_cols
-            self.enc_dict = dict(zip( list(sparse_cols),[dict() for _ in range(len(sparse_cols))]))
-            for f in [self.item_col]+self.cate_cols:
+        # 计算enc_dict
+        if self.enc_dict == None:
+            sparse_cols = [self.item_col] + self.cate_cols
+            self.enc_dict = dict(zip(list(sparse_cols), [dict() for _ in range(len(sparse_cols))]))
+            for f in [self.item_col] + self.cate_cols:
                 self.df[f] = self.df[f].astype('str')
-                map_dict = dict(zip(sorted(self.df[f].unique()), range(1,1+self.df[f].nunique())))
+                map_dict = dict(zip(sorted(self.df[f].unique()), range(1, 1 + self.df[f].nunique())))
                 self.enc_dict[f] = map_dict
-                self.enc_dict[f]['vocab_size'] = self.df[f].nunique()+1
+                self.enc_dict[f]['vocab_size'] = self.df[f].nunique() + 1
         else:
             return self.enc_dict
 
     def enc_data(self):
         sparse_cols = [self.item_col] + self.cate_cols
         for f in sparse_cols:
             self.df[f] = self.df[f].astype('str')
-            self.df[f] = self.df[f].apply(lambda x:self.enc_dict[f].get(x,0))
+            self.df[f] = self.df[f].apply(lambda x: self.enc_dict[f].get(x, 0))
 
     def __getitem__(self, index):
         user_id = self.user_list[index]
         item_list = self.user2item[user_id]
         hist_item_list = []
         hist_mask_list = []
         if self.phase == 'train':
@@ -61,68 +62,69 @@
             k = random.choice(range(4, len(item_list)))  # 从[4,len(item_list))中随机选择一个index
             item_id = item_list[k]  # 该index对应的item加入item_id_list
 
             if k >= self.max_length:  # 选取seq_len个物品
                 hist_item_list.append(item_list[k - self.max_length: k])
                 hist_mask_list.append([1.0] * self.max_length)
                 for col in self.cate_cols:
-                    cate_seq = getattr(self,f'user2{col}')[user_id]
+                    cate_seq = getattr(self, f'user2{col}')[user_id]
                     setattr(self, f'hist_{col}_list', cate_seq[k - self.max_length: k])
             else:
                 hist_item_list.append(item_list[:k] + [0] * (self.max_length - k))
                 hist_mask_list.append([1.0] * k + [0.0] * (self.max_length - k))
                 for col in self.cate_cols:
-                    cate_seq = getattr(self,f'user2{col}')[user_id]
+                    cate_seq = getattr(self, f'user2{col}')[user_id]
                     setattr(self, f'hist_{col}_list', cate_seq[:k] + [0] * (self.max_length - k))
             data = {
-                'hist_item_list':torch.Tensor(hist_item_list).squeeze(0).long(),
-                'hist_mask_list':torch.Tensor(hist_mask_list).squeeze(0).long(),
-                'target_item':torch.Tensor([item_id]).long()
+                'hist_item_list': torch.Tensor(hist_item_list).squeeze(0).long(),
+                'hist_mask_list': torch.Tensor(hist_mask_list).squeeze(0).long(),
+                'target_item': torch.Tensor([item_id]).long()
             }
 
             for col in self.cate_cols:
-                data.update({f'hist_{col}_list':torch.Tensor(getattr(self,f'hist_{col}_list')).squeeze(0).long()})
+                data.update({f'hist_{col}_list': torch.Tensor(getattr(self, f'hist_{col}_list')).squeeze(0).long()})
         else:
             k = int(0.8 * len(item_list))
             if k >= self.max_length:  # 选取seq_len个物品
                 hist_item_list.append(item_list[k - self.max_length: k])
                 hist_mask_list.append([1.0] * self.max_length)
                 for col in self.cate_cols:
-                    cate_seq = getattr(self,f'user2{col}')[user_id]
+                    cate_seq = getattr(self, f'user2{col}')[user_id]
                     setattr(self, f'hist_{col}_list', cate_seq[k - self.max_length: k])
             else:
                 hist_item_list.append(item_list[:k] + [0] * (self.max_length - k))
                 hist_mask_list.append([1.0] * k + [0.0] * (self.max_length - k))
                 for col in self.cate_cols:
-                    cate_seq = getattr(self,f'user2{col}')[user_id]
+                    cate_seq = getattr(self, f'user2{col}')[user_id]
                     setattr(self, f'hist_{col}_list', cate_seq[:k] + [0] * (self.max_length - k))
             data = {
                 'user': user_id,
                 'hist_item_list': torch.Tensor(hist_item_list).squeeze(0).long(),
                 'hist_mask_list': torch.Tensor(hist_mask_list).squeeze(0).long(),
             }
             for col in self.cate_cols:
-                data.update({f'hist_{col}_list':torch.Tensor(getattr(self,f'hist_{col}_list')).squeeze(0).long()})
+                data.update({f'hist_{col}_list': torch.Tensor(getattr(self, f'hist_{col}_list')).squeeze(0).long()})
         return data
 
     def __len__(self):
         return len(self.user_list)
 
     def get_test_gd(self):
         self.test_gd = dict()
         for user in self.user2item:
             item_list = self.user2item[user]
             test_item_index = int(0.8 * len(item_list))
             self.test_gd[str(user)] = item_list[test_item_index:]
         return self.test_gd
 
+
 def seq_collate(batch):
-    hist_item = torch.rand(len(batch),batch[0][0].shape[0])
-    hist_mask = torch.rand(len(batch),batch[0][0].shape[0])
+    hist_item = torch.rand(len(batch), batch[0][0].shape[0])
+    hist_mask = torch.rand(len(batch), batch[0][0].shape[0])
     item_list = []
     for i in range(len(batch)):
-        hist_item[i,:] = batch[i][0]
+        hist_item[i, :] = batch[i][0]
         hist_mask[i, :] = batch[i][1]
         item_list.append(batch[i][2])
     hist_item = hist_item.long()
     hist_mask = hist_mask.long()
-    return hist_item,hist_mask,item_list
+    return hist_item, hist_mask, item_list
```

## rec_pangu/models/base_model.py

```diff
@@ -6,14 +6,15 @@
 import torch
 from torch import nn
 from torch.nn.init import xavier_normal_, constant_
 import numpy as np
 from .layers import EmbeddingLayer
 from loguru import logger
 
+
 class BaseModel(nn.Module):
     def __init__(self, enc_dict: dict, embedding_dim: int) -> None:
         """
         A base class for a neural network model.
 
         Args:
             enc_dict (dict): A dictionary containing the encoding details.
@@ -47,27 +48,32 @@
             pretrained_dict (dict): A pre-trained embedding dictionary.
             trainable (bool, optional): Training flag. Defaults to True.
 
         Raises:
             AssertionError: If the column name is not in the encoding dictionary.
                               If the pre-trained embedding dimension is not equal to the model embedding dimension.
         """
-        assert col_name in self.enc_dict.keys(), "Pretrained Embedding Col: {} must be in the {}".format(col_name, self.enc_dict.keys())
+        assert col_name in self.enc_dict.keys(), "Pretrained Embedding Col: {} must be in the {}".format(col_name,
+                                                                                                         self.enc_dict.keys())
         pretrained_emb_dim = len(list(pretrained_dict.values())[0])
-        assert self.embedding_dim == pretrained_emb_dim, "Pretrained Embedding Dim:{} must be equal to Model Embedding Dim:{}".format(pretrained_emb_dim, self.embedding_dim)
+        assert self.embedding_dim == pretrained_emb_dim, "Pretrained Embedding Dim:{} must be equal to Model Embedding Dim:{}".format(
+            pretrained_emb_dim, self.embedding_dim)
         pretrained_emb = np.random.rand(self.enc_dict[col_name]['vocab_size'], pretrained_emb_dim)
         for k, v in self.enc_dict[col_name].items():
             if k == 'vocab_size':
                 continue
             pretrained_emb[v, :] = pretrained_dict.get(k, np.random.rand(pretrained_emb_dim))
 
         embeddings = torch.from_numpy(pretrained_emb).float()
         embedding_matrix = torch.nn.Parameter(embeddings)
         self.embedding_layer.set_weights(col_name=col_name, embedding_matrix=embedding_matrix, trainable=trainable)
-        logger.info('Successfully Set The Pretrained Embedding Weights for the column:{} With Trainable={}'.format(col_name, trainable))
+        logger.info(
+            'Successfully Set The Pretrained Embedding Weights for the column:{} With Trainable={}'.format(col_name,
+                                                                                                           trainable))
+
 
 class SequenceBaseModel(nn.Module):
     """
     Base sequence model for recommendation tasks.
 
     Attributes:
     enc_dict (dict): A dictionary mapping categorical variable names to their respective encoding dictionaries.
@@ -84,17 +90,19 @@
 
         self.enc_dict = enc_dict
         self.config = config
         self.embedding_dim = self.config['embedding_dim']
         self.max_length = self.config['max_length']
         self.device = self.config['device']
 
-        self.item_emb = nn.Embedding(self.enc_dict[self.config['item_col']]['vocab_size'], self.embedding_dim, padding_idx=0)
+        self.item_emb = nn.Embedding(self.enc_dict[self.config['item_col']]['vocab_size'], self.embedding_dim,
+                                     padding_idx=0)
         for col in self.config['cate_cols']:
-            setattr(self,f'{col}_emb',nn.Embedding(self.enc_dict[col]['vocab_size'], self.embedding_dim, padding_idx=0))
+            setattr(self, f'{col}_emb',
+                    nn.Embedding(self.enc_dict[col]['vocab_size'], self.embedding_dim, padding_idx=0))
 
         self.loss_fun = nn.CrossEntropyLoss()
 
     def calculate_loss(self, user_emb: torch.Tensor, pos_item: torch.Tensor) -> torch.Tensor:
         """
         Calculates the loss for the model given a user embedding and positive item.
 
@@ -128,33 +136,62 @@
     def output_items(self) -> torch.Tensor:
         """
         Returns the item embedding layer weight.
 
         Returns:
         The tensor representing the item embedding layer weight.
         """
-        self.eval()
         return self.item_emb.weight
 
+    def get_attention_mask(self, attention_mask: torch.Tensor) -> torch.Tensor:
+        """
+        Generate left-to-right uni-directional attention mask for multi-head attention.
+
+        Args:
+        attention_mask: a tensor used in multi-head attention with shape (batch_size,
+        seq_len), containing values of either 0 or 1. 0 indicates padding of a sequence
+        and 1 indicates the actual content of the sequence.
+
+        Return:
+        extended_attention_mask: a tensor with shape (batch_size, 1, seq_len, seq_len).
+        An attention mask tensor with float values of -1e6 added to masked positions
+        and 0 to unmasked positions.
+        """
+        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # reshape to (batch_size, 1, 1, seq_len)
+
+        max_len = attention_mask.size(-1)
+        attn_shape = (1, max_len, max_len)
+        subsequent_mask = torch.triu(torch.ones(attn_shape, dtype=torch.uint8),
+                                     diagonal=1)  # create a matrix of upper triangle
+
+        subsequent_mask = (subsequent_mask == 0).unsqueeze(1).type_as(
+            attention_mask)  # reshape and convert to attention_mask type
+
+        extended_attention_mask = extended_attention_mask * subsequent_mask  # apply mask
+
+        extended_attention_mask = (
+                                              1.0 - extended_attention_mask) * -1e6  # replace masked positions with -1e6 and unmasked positions with 0
+
+        return extended_attention_mask
+
     def _init_weights(self, module: nn.Module):
         """
         Initializes the weight value for the given module.
 
         Args:
         module (nn.Module): The module whose weights need to be initialized.
         """
         if isinstance(module, nn.Embedding):
-            xavier_normal_(module.weight.data)
+            torch.nn.init.kaiming_normal_(module.weight.data)
         elif isinstance(module, nn.Linear):
-             xavier_normal_(module.weight.data)
-             if module.bias is not None:
-                constant_(module.bias.data, 0)
+            torch.nn.init.kaiming_normal_(module.weight.data)
+
 
 class GraphBaseModel(nn.Module):
-    def __int__(self,num_user,num_item,embedding_dim):
+    def __int__(self, num_user, num_item, embedding_dim):
         super(GraphBaseModel, self).__init__()
         self.embedding_dim = embedding_dim
         self.num_user = num_item
         self.num_item = num_item
 
         self.user_emb_layer = nn.Embedding(self.num_user, self.embedding_dim)
         self.item_emb_layer = nn.Embedding(self.num_item, self.embedding_dim)
@@ -174,12 +211,13 @@
         mf_loss = nn.LogSigmoid()(pos_scores - neg_scores).mean()
         mf_loss = -1 * mf_loss
 
         regularizer = (torch.norm(users) ** 2 + torch.norm(pos_items) ** 2 + torch.norm(neg_items) ** 2) / 2
         emb_loss = self.lmbd * regularizer / users.shape[0]
 
         return mf_loss + emb_loss
+
     def get_ego_embedding(self):
         user_emb = self.user_emb_layer.weight
         item_emb = self.item_emb_layer.weight
 
         return torch.cat([user_emb, item_emb], 0)
```

## rec_pangu/models/utils.py

```diff
@@ -8,56 +8,59 @@
 import os
 import numpy as np
 import torch
 from torch import nn
 import random
 from typing import Dict, List, Tuple, Union
 
+
 def seed_everything(seed: int = 1029) -> None:
     """Set the random seed for reproducibility.
 
     Args:
         seed (int, optional): The random seed. Defaults to 1029.
     """
     random.seed(seed)
     os.environ["PYTHONHASHSEED"] = str(seed)
     np.random.seed(seed)
     torch.manual_seed(seed)
     torch.cuda.manual_seed(seed)
     torch.backends.cudnn.deterministic = True
 
+
 def set_device(gpu: int = -1) -> torch.device:
     """Set the device to use for computation.
 
     Args:
         gpu (int, optional): GPU index to use if available. Defaults to -1 (use CPU).
 
     Returns:
         torch.device: The device to use for computation.
     """
     if gpu >= 0 and torch.cuda.is_available():
-        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu)
-        device = torch.device("cuda")
+        device = torch.device(f"cuda:{gpu}")
     else:
         device = torch.device("cpu")
     return device
 
+
 def set_optimizer(optimizer: str) -> torch.optim.Optimizer:
     """Set the optimizer for training.
 
     Args:
         optimizer (str): Name of the optimizer.
 
     Returns:
         torch.optim.Optimizer: The optimizer object.
     """
     if optimizer.lower() == "adam":
         optimizer = "Adam"
     return getattr(torch.optim, optimizer)
 
+
 def set_loss(loss: str) -> str:
     """Set the loss function for training.
 
     Args:
         loss (str): Name of the loss function.
 
     Returns:
@@ -65,14 +68,15 @@
     """
     if loss in ["bce", "binary_crossentropy", "binary_cross_entropy"]:
         loss = "binary_cross_entropy"
     else:
         raise NotImplementedError(f"loss={loss} is not supported.")
     return loss
 
+
 def set_regularizer(reg: Union[float, str]) -> List[Tuple[int, float]]:
     """Set the regularizer for the model.
 
     Args:
         reg (Union[float, str]): Regularizer value or string.
 
     Returns:
@@ -91,14 +95,15 @@
                 reg_pair.append((2, float(l2_reg)))
             else:
                 raise NotImplementedError
         except:
             raise NotImplementedError(f"regularizer={reg} is not supported.")
     return reg_pair
 
+
 def set_activation(activation: str) -> nn.Module:
     """Set the activation function for the model.
 
     Args:
         activation (str): Name of the activation function.
 
     Returns:
@@ -109,14 +114,15 @@
     elif activation.lower() == "sigmoid":
         return nn.Sigmoid()
     elif activation.lower() == "tanh":
         return nn.Tanh()
     else:
         return getattr(nn, activation)()
 
+
 def get_linear_input(enc_dict: Dict, data: Dict) -> torch.Tensor:
     """Get the input tensor for linear layers.
 
     Args:
         enc_dict (Dict): Encoding dictionary.
         data (Dict): Data dictionary.
 
@@ -126,27 +132,29 @@
     res_data = []
     for col in enc_dict.keys():
         if 'min' in enc_dict[col].keys():
             res_data.append(data[col])
     res_data = torch.stack(res_data, axis=1)
     return res_data
 
+
 def get_dnn_input_dim(enc_dict: Dict, embedding_dim: int) -> int:
     """Get the input dimension for DNN layers.
 
     Args:
         enc_dict (Dict): Encoding dictionary.
         embedding_dim (int): Embedding dimension.
 
     Returns:
         int: The input dimension for DNN layers.
     """
     num_sparse, num_dense = get_feature_num(enc_dict)
     return num_sparse * embedding_dim + num_dense
 
+
 def get_feature_num(enc_dict: Dict) -> Tuple[int, int]:
     """Get the number of sparse and dense features.
 
     Args:
         enc_dict (Dict): Encoding dictionary.
 
     Returns:
@@ -157,14 +165,15 @@
     for col in enc_dict.keys():
         if 'min' in enc_dict[col].keys():
             num_dense += 1
         elif 'vocab_size' in enc_dict[col].keys():
             num_sparse += 1
     return num_sparse, num_dense
 
+
 def pad_sequence(seqs: List[torch.Tensor], max_len: int) -> torch.Tensor:
     """Pad sequences to the same length.
 
     Args:
         seqs (List[torch.Tensor]): List of sequences.
         max_len (int): Maximum length to pad.
 
@@ -179,14 +188,15 @@
             padded_seq = torch.cat([seq, padding])
         else:
             padded_seq = seq[:max_len]
         padded_seqs.append(padded_seq)
     padded_seqs = torch.stack(padded_seqs, dim=0)
     return padded_seqs
 
+
 def generate_graph(batch_data: Dict) -> Dict:
     """Generate session graph.
 
     Args:
     batch_data (Dict): Batch data dictionary.
 
     Returns:
```

## rec_pangu/models/layers/__init__.py

```diff
@@ -7,8 +7,10 @@
 from .activation import *
 from .shallow import *
 from .deep import *
 from .sequence import *
 from .attention import *
 from .interaction import *
 from .graph import *
-from .multi_interest import *
+from .conv import NextItNetLayer
+from .multi_interest import MultiInterestSelfAttention, CapsuleNetwork
+from .trainformer import TransformerEncoder
```

## rec_pangu/models/layers/attention.py

```diff
@@ -4,14 +4,15 @@
 # @Email: 306178200@qq.com
 # @Time: 2022/6/10 7:40 PM
 
 import torch
 from torch import nn
 import numpy as np
 
+
 class ScaledDotProductAttention(nn.Module):
     """ Scaled Dot-Product Attention """
 
     def __init__(self, dropout_rate=0.):
         super(ScaledDotProductAttention, self).__init__()
         self.dropout = None
         if dropout_rate > 0:
```

## rec_pangu/models/layers/deep.py

```diff
@@ -78,49 +78,7 @@
         Args:
             x: Input tensor with shape (batch_size, input_dim).
         Returns:
             Output tensor with shape (batch_size, output_dim) if output_dim is not None,
             otherwise tensor with shape (batch_size, hidden_units[-1]).
         """
         return self.net(x)
-
-# from torch import nn
-# from .activation import get_activation
-# class MLP_Layer(nn.Module):
-#     def __init__(self,
-#                  input_dim,
-#                  output_dim=None,
-#                  hidden_units=[],
-#                  hidden_activations="ReLU",
-#                  output_activation=None,
-#                  dropout_rates=0,
-#                  batch_norm=False,
-#                  use_bias=True):
-#         super(MLP_Layer, self).__init__()
-#         dense_layers = []
-#         if not isinstance(dropout_rates, list):
-#             dropout_rates = [dropout_rates] * len(hidden_units)
-#         if not isinstance(hidden_activations, list):
-#             hidden_activations = [hidden_activations] * len(hidden_units)
-#         hidden_activations = [get_activation(x) for x in hidden_activations]
-#         hidden_units = [input_dim] + hidden_units
-#         for idx in range(len(hidden_units) - 1):
-#             dense_layers.append(nn.Linear(hidden_units[idx], hidden_units[idx + 1], bias=use_bias))
-#             if batch_norm:
-#                 dense_layers.append(nn.BatchNorm1d(hidden_units[idx + 1]))
-#             if hidden_activations[idx]:
-#                 dense_layers.append(hidden_activations[idx])
-#             if dropout_rates[idx] > 0:
-#                 dense_layers.append(nn.Dropout(p=dropout_rates[idx]))
-#         if output_dim is not None:
-#             dense_layers.append(nn.Linear(hidden_units[-1], output_dim, bias=use_bias))
-#         if output_activation is not None:
-#             dense_layers.append(get_activation(output_activation))
-#         self.dnn = nn.Sequential(*dense_layers)  # * used to unpack list
-#
-#     def forward(self, inputs):
-#         return self.dnn(inputs)
-
-
-
-
-
```

## rec_pangu/models/layers/embedding.py

```diff
@@ -3,14 +3,15 @@
 # @Author: wk
 # @Email: 306178200@qq.com
 # @Time: 2022/6/10 7:40 PM
 from typing import Dict, Union, Optional
 import torch
 from torch import nn
 
+
 class EmbeddingLayer(nn.Module):
     def __init__(self,
                  enc_dict: Dict[str, Dict[str, Union[int, str]]],
                  embedding_dim: int) -> None:
         """
         Initialize EmbeddingLayer instance.
         Args:
@@ -41,15 +42,15 @@
             embedding_matrix: Embedding weight tensor for the column name
             trainable: Boolean indicating if the embedding layer should be trained
         """
         self.embedding_layer[col_name].weight = nn.Parameter(embedding_matrix)
         if not trainable:
             self.embedding_layer[col_name].weight.requires_grad = False
 
-    def forward(self, X: Dict[str,torch.Tensor], name: Optional[str] = None) -> torch.Tensor:
+    def forward(self, X: Dict[str, torch.Tensor], name: Optional[str] = None) -> torch.Tensor:
         """
         Compute the embeddings for a batch of input tensors.
         Args:
             X: Input tensor of shape [batch_size,feature_dim] where feature_dim is the number of features
             name: String indicating the column name
         Returns:
             feature_emb_list: Tensor of shape [batch_size, num_embeddings] containing embeddings for each input feature.
@@ -64,50 +65,7 @@
             if 'seq' in name:
                 inp = X[name].long()
                 fea = self.embedding_layer[name.replace('_seq', '')](inp)
             else:
                 inp = X[name].long().view(-1, 1)
                 fea = self.embedding_layer[name](inp)
             return fea
-
-# from torch import nn
-# import torch
-#
-# class EmbeddingLayer(nn.Module):
-#     def __init__(self,
-#                  enc_dict = None,
-#                  embedding_dim = None):
-#         super(EmbeddingLayer, self).__init__()
-#         self.enc_dict = enc_dict
-#         self.embedding_dim = embedding_dim
-#         self.embedding_layer = nn.ModuleDict()
-#
-#         self.emb_feature = []
-#
-#         for col in self.enc_dict.keys():
-#             if 'vocab_size' in self.enc_dict[col].keys():
-#                 self.emb_feature.append(col)
-#                 self.embedding_layer.update({col : nn.Embedding(
-#                     self.enc_dict[col]['vocab_size']+1,
-#                     self.embedding_dim,
-#                 )})
-#
-#     def set_weights(self, col_name, embedding_matrix, trainable=True):
-#         self.embedding_layer[col_name].weight = embedding_matrix
-#         if not trainable:
-#             self.embedding_layer[col_name].weight.requires_grad = False
-#
-#     def forward(self, X,name=None):
-#         if name == None:
-#             feature_emb_list = []
-#             for col in self.emb_feature:
-#                 inp = X[col].long().view(-1, 1)
-#                 feature_emb_list.append(self.embedding_layer[col](inp))
-#             return torch.stack(feature_emb_list,dim=1).squeeze(2)
-#         else:
-#             if 'seq' in name:
-#                 inp = X[name].long()
-#                 fea = self.embedding_layer[name.replace('_seq','')](inp)
-#             else:
-#                 inp = X[name].long().view(-1, 1)
-#                 fea = self.embedding_layer[name](inp)
-#             return fea
```

## rec_pangu/models/layers/graph.py

```diff
@@ -6,14 +6,15 @@
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from itertools import product
 import dgl.function as fn
 
+
 class FiGNN_Layer(nn.Module):
     def __init__(self,
                  num_fields,
                  embedding_dim,
                  gnn_layers=3,
                  reuse_graph_layer=False,
                  use_gru=True,
@@ -40,15 +41,15 @@
         src_emb = feature_emb[:, self.src_nodes, :]
         dst_emb = feature_emb[:, self.dst_nodes, :]
         concat_emb = torch.cat([src_emb, dst_emb], dim=-1)
         alpha = self.leaky_relu(self.W_attn(concat_emb))
         alpha = alpha.view(-1, self.num_fields, self.num_fields)
         mask = torch.eye(self.num_fields).to(self.device)
         alpha = alpha.masked_fill(mask.byte(), float('-inf'))
-        graph = F.softmax(alpha, dim=-1) # batch x field x field without self-loops
+        graph = F.softmax(alpha, dim=-1)  # batch x field x field without self-loops
         return graph
 
     def forward(self, feature_emb):
         g = self.build_graph_with_attention(feature_emb)
         h = feature_emb
         for i in range(self.gnn_layers):
             if self.reuse_graph_layer:
@@ -62,29 +63,31 @@
                 h = h.view(-1, self.num_fields, self.embedding_dim)
             else:
                 h = a + h
             if self.use_residual:
                 h += feature_emb
         return h
 
+
 class GraphLayer(nn.Module):
     def __init__(self, num_fields, embedding_dim):
         super(GraphLayer, self).__init__()
         self.W_in = torch.nn.Parameter(torch.Tensor(num_fields, embedding_dim, embedding_dim))
         self.W_out = torch.nn.Parameter(torch.Tensor(num_fields, embedding_dim, embedding_dim))
         nn.init.xavier_normal_(self.W_in)
         nn.init.xavier_normal_(self.W_out)
         self.bias_p = nn.Parameter(torch.zeros(embedding_dim))
 
     def forward(self, g, h):
-        h_out = torch.matmul(self.W_out, h.unsqueeze(-1)).squeeze(-1) # broadcast multiply
+        h_out = torch.matmul(self.W_out, h.unsqueeze(-1)).squeeze(-1)  # broadcast multiply
         aggr = torch.bmm(g, h_out)
         a = torch.matmul(self.W_in, aggr.unsqueeze(-1)).squeeze(-1) + self.bias_p
         return a
 
+
 class NGCFLayer(nn.Module):
     def __init__(self, in_size, out_size, dropout):
         super(NGCFLayer, self).__init__()
         self.in_size = in_size
         self.out_size = out_size
         self.dropout = dropout
 
@@ -163,8 +166,8 @@
         gh = self.lin_hh(hidden)  # gh: 3个U*v
         i_r, i_i, i_n = gi.chunk(3, -1)  # 分裂成3个W*a
         h_r, h_i, h_n = gh.chunk(3, -1)  # 分裂成3个U*v
         reset_gate = torch.sigmoid(i_r + h_r)  # 公式(2)
         input_gate = torch.sigmoid(i_i + h_i)  # 公式(3)
         new_gate = torch.tanh(i_n + reset_gate * h_n)  # 公式(4)
         hy = (1 - input_gate) * hidden + input_gate * new_gate  # 公式(5)
-        return hy
+        return hy
```

## rec_pangu/models/layers/interaction.py

```diff
@@ -4,14 +4,15 @@
 # @Email: 306178200@qq.com
 # @Time: 2022/6/10 7:40 PM
 
 import torch
 from torch import nn
 from itertools import combinations
 
+
 class InnerProductLayer(nn.Module):
     """ output: product_sum_pooling (bs x 1),
                 Bi_interaction_pooling (bs * dim),
                 inner_product (bs x f2/2),
                 elementwise_product (bs x f2/2 x emb_dim)
     """
 
@@ -216,33 +217,67 @@
                             out.append(self.fifth_order(p1, p2, p3, p4, p5))
         out = torch.cat(out, dim=-1)
         if self.bn is not None:
             out = self.bn(out)
         y = self.fc(out)
         return y
 
+
 class FM_Layer(nn.Module):
     def __init__(self, final_activation=None, use_bias=True):
         super(FM_Layer, self).__init__()
         self.inner_product_layer = InnerProductLayer(output="product_sum_pooling")
         self.final_activation = final_activation
 
     def forward(self, feature_emb_list):
         output = self.inner_product_layer(feature_emb_list)
         if self.final_activation is not None:
             output = self.final_activation(output)
         return output
 
+
 class SENET_Layer(nn.Module):
     def __init__(self, num_fields, reduction_ratio=3):
         super(SENET_Layer, self).__init__()
         reduced_size = max(1, int(num_fields / reduction_ratio))
         self.excitation = nn.Sequential(nn.Linear(num_fields, reduced_size, bias=False),
                                         nn.ReLU(),
                                         nn.Linear(reduced_size, num_fields, bias=False),
                                         nn.ReLU())
 
     def forward(self, feature_emb):
         Z = torch.mean(feature_emb, dim=-1, out=None)
         A = self.excitation(Z)
         V = feature_emb * A.unsqueeze(-1)
-        return V
+        return V
+
+
+class MaskBlock(torch.nn.Module):
+    def __init__(
+            self, input_dim: int, mask_input_dim: int, output_size: int, reduction_factor: float
+    ) -> None:
+        """
+       Initializes the MaskBlock module.
+
+       Args:
+           input_dim (int): The size of the input tensor.
+           mask_input_dim (int): The size of the mask tensor.
+           output_size (int): The size of the output tensor.
+           reduction_factor (float): The factor by which to reduce the size of the mask tensor.
+       """
+        super(MaskBlock, self).__init__()
+
+        self._input_layer_norm = torch.nn.LayerNorm(input_dim)
+        aggregation_size = int(mask_input_dim * reduction_factor)
+        self._mask_layer = torch.nn.Sequential(
+            torch.nn.Linear(mask_input_dim, aggregation_size),
+            torch.nn.ReLU(),
+            torch.nn.Linear(aggregation_size, input_dim),
+        )
+        self._hidden_layer = torch.nn.Linear(input_dim, output_size)
+        self._layer_norm = torch.nn.LayerNorm(output_size)
+
+    def forward(self, net: torch.Tensor, mask_input: torch.Tensor):
+        if self._input_layer_norm:
+            net = self._input_layer_norm(net)
+        hidden_layer_output = self._hidden_layer(net * self._mask_layer(mask_input))
+        return self._layer_norm(hidden_layer_output)
```

## rec_pangu/models/layers/multi_interest.py

```diff
@@ -3,14 +3,15 @@
 # @Author: wk
 # @Email: 306178200@qq.com
 # @Time: 2023/3/5 15:12
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
+
 class MultiInterestSelfAttention(nn.Module):
     def __init__(self, embedding_dim: int, num_attention_heads: int, d: int = None) -> None:
         super(MultiInterestSelfAttention, self).__init__()
         self.embedding_dim = embedding_dim
         self.num_attention_heads = num_attention_heads
         if d is None:
             self.d = self.embedding_dim * 4
@@ -49,247 +50,112 @@
         # Apply final attention pooling to get multi-interest embeddings
         multi_interest_emb = torch.matmul(A, sequence_embeddings)
 
         return multi_interest_emb
 
 
 class CapsuleNetwork(nn.Module):
-    """
-    Implements a Capsule Network that is capable of handling various types of bilinear
-    interactions between items in a sequence.
-
-    Args:
-    hidden_size: An integer representing the size of the hidden layer of the model.
-    seq_len: An integer representing the length of the input sequence.
-    bilinear_type: An integer representing the type of bilinear interaction between items.
-    interest_num: An integer representing the number of interest capsules in the model.
-    routing_times: An integer representing the number of dynamic routing iterations.
-    hard_readout: A Boolean indicating whether to use hard readout or not
-    relu_layer: A Boolean indicating whether to use a ReLU layer
-
-    Returns:
-    interest_capsule: The output interest capsule from the model
-    """
 
     def __init__(self, hidden_size: int, seq_len: int, bilinear_type: int = 2, interest_num: int = 4,
                  routing_times: int = 3, hard_readout: bool = True, relu_layer: bool = False) -> None:
-        super(CapsuleNetwork, self).__init__()
+        """
+        Implements a Capsule Network that is capable of handling various types of bilinear
+        interactions between items in a sequence.
 
-        self.hidden_size = hidden_size
-        self.seq_len = seq_len
+        Args:
+        hidden_size: An integer representing the size of the hidden layer of the model.
+        seq_len: An integer representing the length of the input sequence.
+        bilinear_type: An integer representing the type of bilinear interaction between items.
+        interest_num: An integer representing the number of interest capsules in the model.
+        routing_times: An integer representing the number of dynamic routing iterations.
+        hard_readout: A Boolean indicating whether to use hard readout or not
+        relu_layer: A Boolean indicating whether to use a ReLU layer
+
+        Returns:
+        interest_capsule: The output interest capsule from the model
+        """
+        super(CapsuleNetwork, self).__init__()
+        self.hidden_size = hidden_size  # h
+        self.seq_len = seq_len  # s
         self.bilinear_type = bilinear_type
         self.interest_num = interest_num
         self.routing_times = routing_times
         self.hard_readout = hard_readout
         self.relu_layer = relu_layer
-        self.stop_grad = True
-
-        if self.relu_layer:
-            self.relu = nn.Sequential(
-                nn.Linear(self.hidden_size, self.hidden_size, bias=False),
-                nn.ReLU()
-            )
-
-        if self.bilinear_type == 0:
-            # The MIND algorithm uses a single linear layer
+        self.stop_grad = False
+        self.relu = nn.Sequential(
+            nn.Linear(self.hidden_size, self.hidden_size, bias=False),
+            nn.ReLU()
+        )
+        if self.bilinear_type == 0:  # MIND
             self.linear = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
         elif self.bilinear_type == 1:
-            # Uses a linear layer to combine multiple interest capsules
             self.linear = nn.Linear(self.hidden_size, self.hidden_size * self.interest_num, bias=False)
         else:  # ComiRec_DR
-            # Uses learned attention weights for interactions between items in the sequence
             self.w = nn.Parameter(torch.Tensor(1, self.seq_len, self.interest_num * self.hidden_size, self.hidden_size))
 
-    def forward(self, item_eb: torch.Tensor, mask: torch.Tensor, device: torch.device) -> torch.Tensor:
-        """
-        Implements the forward pass of a Capsule Network model.
-
-        Args:
-        item_eb: A tensor of shape (batch_size, seq_len, hidden_size) containing the item embeddings for the input sequence.
-        mask: A tensor of shape (batch_size, seq_len) containing the attention mask for the input sequence.
-        device: The device to store the intermediate computations.
-
-        Returns:
-        interest_capsule: The output interest capsule from the model
-        """
-        if self.bilinear_type == 0:
-            # The MIND algorithm uses a single linear layer
-            item_eb_hat = self.linear(item_eb)  # [batch_size, seq_len, hidden_size]
-            item_eb_hat = item_eb_hat.repeat(1, 1, self.interest_num)  # [batch_size, seq_len, hidden_size*interest_num]
+    def forward(self, item_eb, mask, device):
+        if self.bilinear_type == 0:  # MIND
+            item_eb_hat = self.linear(item_eb)  # [b, s, h]
+            item_eb_hat = item_eb_hat.repeat(1, 1, self.interest_num)  # [b, s, h*in]
         elif self.bilinear_type == 1:
             item_eb_hat = self.linear(item_eb)
         else:  # ComiRec_DR
-            u = torch.unsqueeze(item_eb, dim=2)  # shape=(batch_size, seq_len, 1, embedding_dim)
+            u = torch.unsqueeze(item_eb, dim=2)  # shape=(batch_size, maxlen, 1, embedding_dim)
             item_eb_hat = torch.sum(self.w[:, :self.seq_len, :, :] * u,
-                                    -1)  # shape=(batch_size, seq_len, hidden_size*interest_num)
+                                    dim=3)  # shape=(batch_size, maxlen, hidden_size*interest_num)
 
-        item_eb_hat = item_eb_hat.view(-1, self.seq_len, self.interest_num, self.hidden_size)
-        item_eb_hat = item_eb_hat.permute(0, 2, 1, 3).contiguous()
-        item_eb_hat = item_eb_hat.view(-1, self.interest_num, self.seq_len,
-                                       self.hidden_size)  # [batch_size, num_interest, seq_len, hidden_size]
+        item_eb_hat = torch.reshape(item_eb_hat, (-1, self.seq_len, self.interest_num, self.hidden_size))
+        item_eb_hat = torch.transpose(item_eb_hat, 1, 2).contiguous()
+        item_eb_hat = torch.reshape(item_eb_hat, (-1, self.interest_num, self.seq_len, self.hidden_size))
 
-        # [batch_size, num_interest, seq_len, hidden_size]
-        if self.stop_grad:  # Clip signal for backpropagation, item_emb_hat is not included in gradient calculation
+        # [b, in, s, h]
+        if self.stop_grad:  # 截断反向传播，item_emb_hat不计入梯度计算中
             item_eb_hat_iter = item_eb_hat.detach()
         else:
             item_eb_hat_iter = item_eb_hat
 
-        if self.bilinear_type > 0:
-            # If using specific Capsule Network algorithm rather than ComiRec_DR, initialise capsule_weight to 0
+        # b的shape=(b, in, s)
+        if self.bilinear_type > 0:  # b初始化为0（一般的胶囊网络算法）
             capsule_weight = torch.zeros(item_eb_hat.shape[0], self.interest_num, self.seq_len, device=device,
                                          requires_grad=False)
-        else:  # Use Gaussian distribution to initialise b for the MIND algorithm
+        else:  # MIND使用高斯分布随机初始化b
             capsule_weight = torch.randn(item_eb_hat.shape[0], self.interest_num, self.seq_len, device=device,
                                          requires_grad=False)
 
-        for i in range(self.routing_times):  # Dynamic routing propagation 3 times
-            atten_mask = torch.unsqueeze(mask, 1).repeat(1, self.interest_num, 1)  # [batch_size, num_interest, seq_len]
+        for i in range(self.routing_times):  # 动态路由传播3次
+            atten_mask = torch.unsqueeze(mask, 1).repeat(1, self.interest_num, 1)  # [b, in, s]
             paddings = torch.zeros_like(atten_mask, dtype=torch.float)
 
-            # Calculates c, applies masking, final shape=[batch_size, num_interest, 1, seq_len]
+            # 计算c，进行mask，最后shape=[b, in, 1, s]
             capsule_softmax_weight = F.softmax(capsule_weight, dim=-1)
-            capsule_softmax_weight = torch.where(torch.eq(atten_mask, 0), paddings, capsule_softmax_weight)  # Mask
+            capsule_softmax_weight = torch.where(torch.eq(atten_mask, 0), paddings, capsule_softmax_weight)  # mask
             capsule_softmax_weight = torch.unsqueeze(capsule_softmax_weight, 2)
 
             if i < 2:
                 # s=c*u_hat , (batch_size, interest_num, 1, seq_len) * (batch_size, interest_num, seq_len, hidden_size)
-                interest_capsule = torch.matmul(capsule_softmax_weight, item_eb_hat_iter)
-                cap_norm = torch.sum(torch.square(interest_capsule), -1, True)
-                scalar_factor = cap_norm / (1 + cap_norm) / torch.sqrt(cap_norm + 1e-9)
-                interest_capsule = scalar_factor * interest_capsule
-
-                # Updating b
-                delta_weight = torch.matmul(item_eb_hat_iter, torch.transpose(interest_capsule, 2, 3).contiguous())
-                # u_hat*v, shape=(batch_size, interest_num, seq_len, 1)
-                delta_weight = delta_weight.view(-1, self.interest_num,
-                                                 self.seq_len)  # shape=(batch_size, interest_num, seq_len)
-                capsule_weight = capsule_weight + delta_weight  # Update b
+                interest_capsule = torch.matmul(capsule_softmax_weight,
+                                                item_eb_hat_iter)  # shape=(batch_size, interest_num, 1, hidden_size)
+                cap_norm = torch.sum(torch.square(interest_capsule), -1, True)  # shape=(batch_size, interest_num, 1, 1)
+                scalar_factor = cap_norm / (1 + cap_norm) / torch.sqrt(cap_norm + 1e-9)  # shape同上
+                interest_capsule = scalar_factor * interest_capsule  # squash(s)->v,shape=(batch_size, interest_num, 1, hidden_size)
+
+                # 更新b
+                delta_weight = torch.matmul(item_eb_hat_iter,  # shape=(batch_size, interest_num, seq_len, hidden_size)
+                                            torch.transpose(interest_capsule, 2, 3).contiguous()
+                                            # shape=(batch_size, interest_num, hidden_size, 1)
+                                            )  # u_hat*v, shape=(batch_size, interest_num, seq_len, 1)
+                delta_weight = torch.reshape(delta_weight, (
+                    -1, self.interest_num, self.seq_len))  # shape=(batch_size, interest_num, seq_len)
+                capsule_weight = capsule_weight + delta_weight  # 更新b
             else:
                 interest_capsule = torch.matmul(capsule_softmax_weight, item_eb_hat)
                 cap_norm = torch.sum(torch.square(interest_capsule), -1, True)
                 scalar_factor = cap_norm / (1 + cap_norm) / torch.sqrt(cap_norm + 1e-9)
                 interest_capsule = scalar_factor * interest_capsule
 
-        interest_capsule = interest_capsule.view(-1, self.interest_num, self.hidden_size)
+        interest_capsule = torch.reshape(interest_capsule, (-1, self.interest_num, self.hidden_size))
 
-        if self.relu_layer:  # MIND model uses relu_layer
+        if self.relu_layer:  # MIND模型使用book数据库时，使用relu_layer
             interest_capsule = self.relu(interest_capsule)
 
         return interest_capsule
-
-# class MultiInterest_SA(nn.Module):
-#     def __init__(self, embedding_dim, K, d=None):
-#         super(MultiInterest_SA, self).__init__()
-#         self.embedding_dim = embedding_dim
-#         self.K = K
-#         if d == None:
-#             self.d = self.embedding_dim*4
-#
-#         self.W1 = torch.nn.Parameter(torch.rand(self.embedding_dim, self.d), requires_grad=True)
-#         self.W2 = torch.nn.Parameter(torch.rand(self.d, self.K), requires_grad=True)
-#         self.W3 = torch.nn.Parameter(torch.rand(self.embedding_dim, self.embedding_dim), requires_grad=True)
-#
-#     def forward(self, seq_emb, mask = None):
-#         '''
-#         seq_emb : batch,seq,emb
-#         mask : batch,seq,1
-#         '''
-#         H = torch.einsum('bse, ed -> bsd', seq_emb, self.W1).tanh()
-#         if mask != None:
-#             A = torch.einsum('bsd, dk -> bsk', H, self.W2) + -1.e9 * (1 - mask.float())
-#             A = F.softmax(A, dim=1)
-#         else:
-#             A = F.softmax(torch.einsum('bsd, dk -> bsk', H, self.W2), dim=1)
-#         A = A.permute(0, 2, 1)
-#         multi_interest_emb = torch.matmul(A, seq_emb)
-#         return multi_interest_emb
-
-# class CapsuleNetwork(nn.Module):
-#
-#     def __init__(self, hidden_size, seq_len, bilinear_type=2, interest_num=4, routing_times=3, hard_readout=True,
-#                  relu_layer=False):
-#         super(CapsuleNetwork, self).__init__()
-#         self.hidden_size = hidden_size  # h
-#         self.seq_len = seq_len  # s
-#         self.bilinear_type = bilinear_type
-#         self.interest_num = interest_num
-#         self.routing_times = routing_times
-#         self.hard_readout = hard_readout
-#         self.relu_layer = relu_layer
-#         self.stop_grad = True
-#         self.relu = nn.Sequential(
-#             nn.Linear(self.hidden_size, self.hidden_size, bias=False),
-#             nn.ReLU()
-#         )
-#         if self.bilinear_type == 0:  # MIND
-#             self.linear = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
-#         elif self.bilinear_type == 1:
-#             self.linear = nn.Linear(self.hidden_size, self.hidden_size * self.interest_num, bias=False)
-#         else:  # ComiRec_DR
-#             self.w = nn.Parameter(torch.Tensor(1, self.seq_len, self.interest_num * self.hidden_size, self.hidden_size))
-#
-#     def forward(self, item_eb, mask, device):
-#         if self.bilinear_type == 0:  # MIND
-#             item_eb_hat = self.linear(item_eb)  # [b, s, h]
-#             item_eb_hat = item_eb_hat.repeat(1, 1, self.interest_num)  # [b, s, h*in]
-#         elif self.bilinear_type == 1:
-#             item_eb_hat = self.linear(item_eb)
-#         else:  # ComiRec_DR
-#             u = torch.unsqueeze(item_eb, dim=2)  # shape=(batch_size, maxlen, 1, embedding_dim)
-#             item_eb_hat = torch.sum(self.w[:, :self.seq_len, :, :] * u,
-#                                     dim=3)  # shape=(batch_size, maxlen, hidden_size*interest_num)
-#
-#         item_eb_hat = torch.reshape(item_eb_hat, (-1, self.seq_len, self.interest_num, self.hidden_size))
-#         item_eb_hat = torch.transpose(item_eb_hat, 1, 2).contiguous()
-#         item_eb_hat = torch.reshape(item_eb_hat, (-1, self.interest_num, self.seq_len, self.hidden_size))
-#
-#         # [b, in, s, h]
-#         if self.stop_grad:  # 截断反向传播，item_emb_hat不计入梯度计算中
-#             item_eb_hat_iter = item_eb_hat.detach()
-#         else:
-#             item_eb_hat_iter = item_eb_hat
-#
-#         # b的shape=(b, in, s)
-#         if self.bilinear_type > 0:  # b初始化为0（一般的胶囊网络算法）
-#             capsule_weight = torch.zeros(item_eb_hat.shape[0], self.interest_num, self.seq_len, device=device,
-#                                          requires_grad=False)
-#         else:  # MIND使用高斯分布随机初始化b
-#             capsule_weight = torch.randn(item_eb_hat.shape[0], self.interest_num, self.seq_len, device=device,
-#                                          requires_grad=False)
-#
-#         for i in range(self.routing_times):  # 动态路由传播3次
-#             atten_mask = torch.unsqueeze(mask, 1).repeat(1, self.interest_num, 1)  # [b, in, s]
-#             paddings = torch.zeros_like(atten_mask, dtype=torch.float)
-#
-#             # 计算c，进行mask，最后shape=[b, in, 1, s]
-#             capsule_softmax_weight = F.softmax(capsule_weight, dim=-1)
-#             capsule_softmax_weight = torch.where(torch.eq(atten_mask, 0), paddings, capsule_softmax_weight)  # mask
-#             capsule_softmax_weight = torch.unsqueeze(capsule_softmax_weight, 2)
-#
-#             if i < 2:
-#                 # s=c*u_hat , (batch_size, interest_num, 1, seq_len) * (batch_size, interest_num, seq_len, hidden_size)
-#                 interest_capsule = torch.matmul(capsule_softmax_weight,
-#                                                 item_eb_hat_iter)  # shape=(batch_size, interest_num, 1, hidden_size)
-#                 cap_norm = torch.sum(torch.square(interest_capsule), -1, True)  # shape=(batch_size, interest_num, 1, 1)
-#                 scalar_factor = cap_norm / (1 + cap_norm) / torch.sqrt(cap_norm + 1e-9)  # shape同上
-#                 interest_capsule = scalar_factor * interest_capsule  # squash(s)->v,shape=(batch_size, interest_num, 1, hidden_size)
-#
-#                 # 更新b
-#                 delta_weight = torch.matmul(item_eb_hat_iter,  # shape=(batch_size, interest_num, seq_len, hidden_size)
-#                                             torch.transpose(interest_capsule, 2, 3).contiguous()
-#                                             # shape=(batch_size, interest_num, hidden_size, 1)
-#                                             )  # u_hat*v, shape=(batch_size, interest_num, seq_len, 1)
-#                 delta_weight = torch.reshape(delta_weight, (
-#                 -1, self.interest_num, self.seq_len))  # shape=(batch_size, interest_num, seq_len)
-#                 capsule_weight = capsule_weight + delta_weight  # 更新b
-#             else:
-#                 interest_capsule = torch.matmul(capsule_softmax_weight, item_eb_hat)
-#                 cap_norm = torch.sum(torch.square(interest_capsule), -1, True)
-#                 scalar_factor = cap_norm / (1 + cap_norm) / torch.sqrt(cap_norm + 1e-9)
-#                 interest_capsule = scalar_factor * interest_capsule
-#
-#         interest_capsule = torch.reshape(interest_capsule, (-1, self.interest_num, self.hidden_size))
-#
-#         if self.relu_layer:  # MIND模型使用book数据库时，使用relu_layer
-#             interest_capsule = self.relu(interest_capsule)
-#
-#         return interest_capsule
```

## rec_pangu/models/layers/sequence.py

```diff
@@ -5,36 +5,135 @@
 # @Time: 2022/6/10 7:40 PM
 
 from torch import nn
 import torch
 
 
 class MaskedAveragePooling(nn.Module):
+    """
+    This module takes as input an embedding matrix,
+    applies masked pooling, i.e. ignores zero-padding,
+    and computes the average embedding vector for each input.
+    """
     def __init__(self):
         super(MaskedAveragePooling, self).__init__()
 
-    def forward(self, embedding_matrix):
+    def forward(self, embedding_matrix: torch.Tensor) -> torch.Tensor:
+        """
+        Computes the average embedding vector.
+
+        Args:
+            embedding_matrix (torch.Tensor): Input embedding of shape (batch_size, seq_len, hidden_size).
+
+        Returns:
+            torch.Tensor: A tensor of shape (batch_size, hidden_size) representing the averaged embedding vector.
+        """
         sum_pooling_matrix = torch.sum(embedding_matrix, dim=1)
         non_padding_length = (embedding_matrix != 0).sum(dim=1)
         embedding_vec = sum_pooling_matrix / (non_padding_length.float() + 1e-16)
         return embedding_vec
 
 
 class MaskedSumPooling(nn.Module):
+    """
+    This module takes as input an embedding matrix,
+    applies masked pooling, i.e. ignores zero-padding,
+    and computes the sum embedding vector for each input.
+    """
     def __init__(self):
         super(MaskedSumPooling, self).__init__()
 
-    def forward(self, embedding_matrix):
+    def forward(self, embedding_matrix: torch.Tensor) -> torch.Tensor:
+        """
+        Computes the sum embedding vector.
+
+        Args:
+            embedding_matrix (torch.Tensor): Input embedding of shape (batch_size, seq_len, hidden_size).
+
+        Returns:
+            torch.Tensor: A tensor of shape (batch_size, hidden_size) representing the summed embedding vector.
+        """
         # mask by zeros
         return torch.sum(embedding_matrix, dim=1)
 
 
 class KMaxPooling(nn.Module):
-    def __init__(self, k, dim):
+    """
+    This module takes as input an embedding matrix,
+    and returns the k-max pooling along the specified axis.
+    """
+    def __init__(self, k: int, dim: int):
         super(KMaxPooling, self).__init__()
         self.k = k
         self.dim = dim
 
-    def forward(self, X):
+    def forward(self, X: torch.Tensor) -> torch.Tensor:
+        """
+        Computes the k-max pooling.
+
+        Args:
+            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, hidden_size).
+
+        Returns:
+            torch.Tensor: A tensor of shape (batch_size, k, hidden_size) representing the k-max pooled embedding vector.
+        """
         index = X.topk(self.k, dim=self.dim)[1].sort(dim=self.dim)[0]
         output = X.gather(self.dim, index)
-        return output
+        return output
+
+
+class STAMPLayer(nn.Module):
+    def __init__(self, embedding_dim: int, feat_drop: float = 0.0):
+        """
+        Args:
+        embedding_dim(int): the input/output dimensions of the STAMPLayer
+        feat_drop(float): Dropout rate to be applied to the input features
+        """
+        super().__init__()
+        self.feat_drop = nn.Dropout(feat_drop) if feat_drop > 0 else None
+        self.fc_a = nn.Linear(embedding_dim, embedding_dim, bias=True)
+        self.fc_t = nn.Linear(embedding_dim, embedding_dim, bias=True)
+        self.attn_i = nn.Linear(embedding_dim, embedding_dim, bias=False)
+        self.attn_t = nn.Linear(embedding_dim, embedding_dim, bias=True)
+        self.attn_s = nn.Linear(embedding_dim, embedding_dim, bias=False)
+        self.attn_e = nn.Linear(embedding_dim, 1, bias=False)
+
+    def forward(self, emb_seqs: torch.Tensor, lens: torch.Tensor) -> torch.Tensor:
+        """
+        Applies the STAMP mechanism to a batch of input sequences
+
+        Args:
+        emb_seqs(torch.Tensor): Batch of input sequences [batch_size, max_len, embedding_dim]
+        lens(torch.Tensor): A tensor of actual sequence lengths for each sequence in the batch [batch_size]
+
+        Returns:
+        sr(torch.Tensor): Output scores of the STAMP mechanism [batch_size, embedding_dim]
+        """
+        if self.feat_drop is not None:
+            emb_seqs = self.feat_drop(emb_seqs)
+        batch_size, max_len, _ = emb_seqs.size()
+
+        # mask out padded inputs
+        mask = torch.arange(
+            max_len, device=lens.device
+        ).unsqueeze(0).expand(batch_size, max_len) >= lens.unsqueeze(-1)
+        emb_seqs = torch.masked_fill(emb_seqs, mask.unsqueeze(-1), 0)
+
+        # calculate mean sequence vector
+        ms = emb_seqs.sum(dim=1) / lens.unsqueeze(-1)  # [batch_size, embedding_dim]
+
+        # calculate target vector and attention weights
+        xt = emb_seqs[torch.arange(batch_size), lens - 1]  # [batch_size, embedding_dim]
+        ei = self.attn_i(emb_seqs)  # [batch_size, max_len, embedding_dim]
+        et = self.attn_t(xt).unsqueeze(1)  # [batch_size, 1, embedding_dim]
+        es = self.attn_s(ms).unsqueeze(1)  # [batch_size, 1, embedding_dim]
+        e = self.attn_e(torch.sigmoid(ei + et + es)).squeeze(-1)  # [batch_size, max_len]
+        alpha = torch.masked_fill(e, mask, 0)
+        alpha = alpha.unsqueeze(-1)  # [batch_size, max_len, 1]
+        ma = torch.sum(alpha * emb_seqs, dim=1)  # [batch_size, embedding_dim]
+
+        # calculate final output scores
+        ha = self.fc_a(ma)
+        ht = self.fc_t(xt)
+        sr = ha * ht  # [batch_size, embedding_dim]
+
+        return sr
```

## rec_pangu/models/layers/shallow.py

```diff
@@ -5,14 +5,15 @@
 # @Time: 2022/6/10 7:40 PM
 
 import torch
 from torch import nn
 from .embedding import EmbeddingLayer
 from ..utils import get_dnn_input_dim, get_linear_input
 
+
 # Wide部分
 class LR_Layer(nn.Module):
     def __init__(self, enc_dict):
         super(LR_Layer, self).__init__()
         self.enc_dict = enc_dict
         self.emb_layer = EmbeddingLayer(enc_dict=self.enc_dict, embedding_dim=1)
         self.dnn_input_dim = get_dnn_input_dim(self.enc_dict, 1)
```

## rec_pangu/models/multi_task/aitm.py

```diff
@@ -2,15 +2,15 @@
 # @ModuleName: aitm
 # @Author: wk
 # @Email: 306178200@qq.com
 # @Time: 2022/6/10 7:40 PM
 from typing import Dict, List
 import torch
 from torch import nn
-from ..layers import MLP,MultiHeadSelfAttention
+from ..layers import MLP, MultiHeadSelfAttention
 from ..utils import get_feature_num
 from ..base_model import BaseModel
 
 
 class AITM(BaseModel):
     def __init__(self,
                  embedding_dim: int = 32,
@@ -37,15 +37,15 @@
 
         self.click_layer = nn.Sequential(nn.Linear(tower_dims[-1], 1),
                                          nn.Sigmoid())
         self.conversion_layer = nn.Sequential(nn.Linear(tower_dims[-1], 1),
                                               nn.Sigmoid())
         self.apply(self._init_weights)
 
-    def forward(self, data,is_training=True):
+    def forward(self, data, is_training=True):
         """
         Perform forward propagation on the AITM model.
 
         Args:
             data (Dict[str, torch.Tensor]): The input data in the form of a dictionary containing the features and labels.
             is_training (bool): If True, compute the loss. Default is True.
```

## rec_pangu/models/ranking/__init__.py

```diff
@@ -11,8 +11,9 @@
 from .afn import AFN
 from .aoanet import AOANet
 from .autoint import AutoInt
 from .ccpm import CCPM
 from .lr import LR
 from .fm import FM
 from .xdeepfm import xDeepFM
-from .dcn import DCN
+from .dcn import DCN
+from .masknet import MaskNet
```

## rec_pangu/models/ranking/afm.py

```diff
@@ -4,23 +4,24 @@
 # @Email: 306178200@qq.com
 # @Time: 2022/6/10 7:40 PM
 import torch
 from ..layers import MLP, LR_Layer, SENET_Layer, BilinearInteractionLayer
 from ..utils import get_feature_num, get_linear_input
 from ..base_model import BaseModel
 
+
 # TODO: change the current code of AFM with the right version.
 
 class AFM(BaseModel):
     def __init__(self,
                  embedding_dim=32,
                  hidden_units=[64, 64, 64],
                  loss_fun='torch.nn.BCELoss()',
                  enc_dict=None):
-        super(AFM, self).__init__(enc_dict,embedding_dim)
+        super(AFM, self).__init__(enc_dict, embedding_dim)
 
         self.hidden_units = hidden_units
         self.loss_fun = eval(loss_fun)
         self.enc_dict = enc_dict
 
         self.num_sparse, self.num_dense = get_feature_num(self.enc_dict)
 
@@ -29,15 +30,15 @@
         self.bilinear_interaction = BilinearInteractionLayer(self.num_sparse, embedding_dim, 'field_interaction')
 
         input_dim = self.num_sparse * (self.num_sparse - 1) * self.embedding_dim + self.num_dense
         self.dnn = MLP(input_dim=input_dim, output_dim=1, hidden_units=self.hidden_units,
                        hidden_activations='relu', dropout_rates=0)
         self.apply(self._init_weights)
 
-    def forward(self, data,is_training=True):
+    def forward(self, data, is_training=True):
         """
         Perform forward propagation on the AFM model.
 
         Args:
             data (Dict[str, torch.Tensor]): The input data in the form of a dictionary containing the features and labels.
             is_training (bool): If True, compute the loss. Default is True.
 
@@ -49,18 +50,18 @@
         feature_emb = self.embedding_layer(data)
         senet_emb = self.senet_layer(feature_emb)
         bilinear_p = self.bilinear_interaction(feature_emb)
         bilinear_q = self.bilinear_interaction(senet_emb)
         comb_out = torch.flatten(torch.cat([bilinear_p, bilinear_q], dim=1), start_dim=1)
 
         dense_input = get_linear_input(self.enc_dict, data)
-        comb_out = torch.cat([comb_out, dense_input],dim=1)
+        comb_out = torch.cat([comb_out, dense_input], dim=1)
         y_pred += self.dnn(comb_out)
         y_pred = y_pred.sigmoid()
 
         # 输出
         if is_training:
-            loss = self.loss_fun(y_pred.squeeze(-1),data['label'])
-            output_dict = {'pred':y_pred,'loss':loss}
+            loss = self.loss_fun(y_pred.squeeze(-1), data['label'])
+            output_dict = {'pred': y_pred, 'loss': loss}
         else:
-            output_dict = {'pred':y_pred}
+            output_dict = {'pred': y_pred}
         return output_dict
```

## rec_pangu/models/ranking/afn.py

```diff
@@ -6,24 +6,25 @@
 from typing import Dict
 from torch import nn
 import torch
 from ..layers import EmbeddingLayer, MLP
 from ..utils import get_feature_num
 from ..base_model import BaseModel
 
+
 class AFN(BaseModel):
     def __init__(self,
                  embedding_dim=32,
                  dnn_hidden_units=[64, 64, 64],
-                 afn_hidden_units = [64, 64, 64],
-                 ensemble_dnn = True,
+                 afn_hidden_units=[64, 64, 64],
+                 ensemble_dnn=True,
                  loss_fun='torch.nn.BCELoss()',
-                 logarithmic_neurons = 5,
+                 logarithmic_neurons=5,
                  enc_dict=None):
-        super(AFN, self).__init__(enc_dict,embedding_dim)
+        super(AFN, self).__init__(enc_dict, embedding_dim)
         """
         AFN model.
 
         Args:
             embedding_dim (int): The size of the embedding vector. Default is 32.
             dnn_hidden_units (List[int]): The list of hidden units for the DNN. Default is [64, 64, 64].
             afn_hidden_units (List[int]): The list of hidden units for the AFN. Default is [64, 64, 64].
@@ -53,15 +54,15 @@
             self.embedding_layer2 = EmbeddingLayer(enc_dict=self.enc_dict, embedding_dim=self.embedding_dim)
             self.dnn = MLP(input_dim=embedding_dim * self.num_sparse,
                            output_dim=1,
                            hidden_units=dnn_hidden_units,
                            use_bias=True)
             self.fc = nn.Linear(2, 1)
 
-    def forward(self, data,is_training=True):
+    def forward(self, data, is_training=True):
         """
         Perform forward propagation on the AFN model.
 
         Args:
             data (Dict[str, torch.Tensor]): The input data in the form of a dictionary containing the features and labels.
             is_training (bool): If True, compute the loss. Default is True.
 
@@ -78,23 +79,23 @@
             dnn_out = self.dnn(feature_emb2.flatten(start_dim=1))
             y_pred = self.fc(torch.cat([afn_out, dnn_out], dim=-1))
         else:
             y_pred = afn_out
         y_pred = y_pred.sigmoid()
         # 输出
         if is_training:
-            loss = self.loss_fun(y_pred.squeeze(-1),data['label'])
-            output_dict = {'pred':y_pred,'loss':loss}
+            loss = self.loss_fun(y_pred.squeeze(-1), data['label'])
+            output_dict = {'pred': y_pred, 'loss': loss}
         else:
-            output_dict = {'pred':y_pred}
+            output_dict = {'pred': y_pred}
         return output_dict
 
     def logarithmic_net(self, feature_emb):
         feature_emb = torch.abs(feature_emb)
-        feature_emb = torch.clamp(feature_emb, min=1e-5) # ReLU with min 1e-5 (better than 1e-7 suggested in paper)
-        log_feature_emb = torch.log(feature_emb) # element-wise log
-        log_feature_emb = self.log_batch_norm(log_feature_emb) # batch_size * num_fields * embedding_dim
+        feature_emb = torch.clamp(feature_emb, min=1e-5)  # ReLU with min 1e-5 (better than 1e-7 suggested in paper)
+        log_feature_emb = torch.log(feature_emb)  # element-wise log
+        log_feature_emb = self.log_batch_norm(log_feature_emb)  # batch_size * num_fields * embedding_dim
         logarithmic_out = self.coefficient_W(log_feature_emb.transpose(2, 1)).transpose(1, 2)
-        cross_out = torch.exp(logarithmic_out) # element-wise exp
+        cross_out = torch.exp(logarithmic_out)  # element-wise exp
         cross_out = self.exp_batch_norm(cross_out)  # batch_size * logarithmic_neurons * embedding_dim
         concat_out = torch.flatten(cross_out, start_dim=1)
-        return concat_out
+        return concat_out
```

## rec_pangu/models/ranking/aoanet.py

```diff
@@ -6,14 +6,15 @@
 from typing import Dict, List
 from torch import nn
 import torch
 from ..layers import MLP
 from ..utils import get_feature_num, get_linear_input
 from ..base_model import BaseModel
 
+
 class AOANet(BaseModel):
     def __init__(self,
                  embedding_dim: int = 32,
                  dnn_hidden_units: List[int] = [64, 64, 64],
                  num_interaction_layers: int = 3,
                  num_subspaces: int = 4,
                  loss_fun: str = 'torch.nn.BCELoss()',
@@ -43,16 +44,15 @@
         self.gin = GeneralizedInteractionNet(num_interaction_layers,
                                              num_subspaces,
                                              self.num_sparse,
                                              self.embedding_dim)
         self.fc = nn.Linear(dnn_hidden_units[-1] + num_subspaces * self.embedding_dim, 1)
         self.apply(self._init_weights)
 
-
-    def forward(self, data,is_training=True):
+    def forward(self, data, is_training=True):
         """
         Perform forward propagation on the AoaNet model.
 
         Args:
             data (Dict[str, torch.Tensor]): The input data in the form of a dictionary containing the features and labels.
             is_training (bool): If True, compute the loss. Default is True.
 
@@ -65,18 +65,18 @@
         dnn_out = self.dnn(torch.cat([emb_flatten, dense_input], dim=1))
         interact_out = self.gin(feature_emb).flatten(start_dim=1)
         y_pred = self.fc(torch.cat([dnn_out, interact_out], dim=-1))
         y_pred = y_pred.sigmoid()
 
         # 输出
         if is_training:
-            loss = self.loss_fun(y_pred.squeeze(-1),data['label'])
-            output_dict = {'pred':y_pred,'loss':loss}
+            loss = self.loss_fun(y_pred.squeeze(-1), data['label'])
+            output_dict = {'pred': y_pred, 'loss': loss}
         else:
-            output_dict = {'pred':y_pred}
+            output_dict = {'pred': y_pred}
         return output_dict
 
 
 class GeneralizedInteractionNet(nn.Module):
     def __init__(self, num_layers, num_subspaces, num_fields, embedding_dim):
         super(GeneralizedInteractionNet, self).__init__()
         self.layers = nn.ModuleList([GeneralizedInteraction(num_fields if i == 0 else num_subspaces,
```

## rec_pangu/models/ranking/autoint.py

```diff
@@ -51,15 +51,15 @@
                                      attention_dim=attention_dim,
                                      num_heads=num_heads,
                                      align_to="output")
               for i in range(attention_layers)])
         self.fc = nn.Linear(self.num_sparse * attention_dim * num_heads, 1)
         self.apply(self._init_weights)
 
-    def forward(self, data,is_training=True):
+    def forward(self, data, is_training=True):
         """
         Perform forward propagation on the AutoInt model.
 
         Args:
             data (Dict[str, torch.Tensor]): The input data in the form of a dictionary containing the features and labels.
             is_training (bool): If True, compute the loss. Default is True.
 
@@ -76,12 +76,12 @@
             emb_flatten = feature_emb.flatten(start_dim=1)
             y_pred += self.dnn(torch.cat([emb_flatten, dense_input], dim=1))
         if self.lr_layer is not None:
             y_pred += self.lr_layer(data)
         y_pred = y_pred.sigmoid()
         # 输出
         if is_training:
-            loss = self.loss_fun(y_pred.squeeze(-1),data['label'])
-            output_dict = {'pred':y_pred,'loss':loss}
+            loss = self.loss_fun(y_pred.squeeze(-1), data['label'])
+            output_dict = {'pred': y_pred, 'loss': loss}
         else:
-            output_dict = {'pred':y_pred}
+            output_dict = {'pred': y_pred}
         return output_dict
```

## rec_pangu/models/ranking/ccpm.py

```diff
@@ -41,15 +41,15 @@
                                          channels=channels,
                                          kernel_heights=kernel_heights)
         conv_out_dim = 3 * embedding_dim * channels[-1]  # 3 is k-max-pooling size of the last layer
         self.fc = nn.Linear(conv_out_dim, 1)
 
         self.apply(self._init_weights)
 
-    def forward(self, data,is_training=True):
+    def forward(self, data, is_training=True):
         """
         Perform forward propagation on the CCPM model.
 
         Args:
             data (Dict[str, torch.Tensor]): The input data in the form of a dictionary containing the features and labels.
             is_training (bool): If True, compute the loss. Default is True.
 
@@ -62,30 +62,32 @@
         conv_out = self.conv_layer(conv_in)
         flatten_out = torch.flatten(conv_out, start_dim=1)
         y_pred = self.fc(flatten_out)
 
         y_pred = y_pred.sigmoid()
         # 输出
         if is_training:
-            loss = self.loss_fun(y_pred.squeeze(-1),data['label'])
-            output_dict = {'pred':y_pred,'loss':loss}
+            loss = self.loss_fun(y_pred.squeeze(-1), data['label'])
+            output_dict = {'pred': y_pred, 'loss': loss}
         else:
-            output_dict = {'pred':y_pred}
+            output_dict = {'pred': y_pred}
         return output_dict
 
+
 class CCPM_ConvLayer(nn.Module):
     """
     Input X: tensor of shape (batch_size, 1, num_fields, embedding_dim)
     """
+
     def __init__(self, num_fields, channels=[3], kernel_heights=[3], activation="Tanh"):
         super(CCPM_ConvLayer, self).__init__()
         if not isinstance(kernel_heights, list):
             kernel_heights = [kernel_heights] * len(channels)
         elif len(kernel_heights) != len(channels):
-            raise ValueError("channels={} and kernel_heights={} should have the same length."\
+            raise ValueError("channels={} and kernel_heights={} should have the same length." \
                              .format(channels, kernel_heights))
         module_list = []
         self.channels = [1] + channels
         layers = len(kernel_heights)
         for i in range(1, len(self.channels)):
             in_channels = self.channels[i - 1]
             out_channels = self.channels[i]
@@ -97,8 +99,8 @@
             else:
                 k = 3
             module_list.append(KMaxPooling(k, dim=2))
             module_list.append(get_activation(activation))
         self.conv_layer = nn.Sequential(*module_list)
 
     def forward(self, X):
-        return self.conv_layer(X)
+        return self.conv_layer(X)
```

## rec_pangu/models/ranking/deepfm.py

```diff
@@ -12,15 +12,15 @@
 
 class DeepFM(BaseModel):
     def __init__(self,
                  embedding_dim: int = 32,
                  hidden_units: List[int] = [64, 64, 64],
                  loss_fun: str = 'torch.nn.BCELoss()',
                  enc_dict: Dict[str, dict] = None):
-        super(DeepFM, self).__init__(enc_dict,embedding_dim)
+        super(DeepFM, self).__init__(enc_dict, embedding_dim)
         """
         DeepFM model.
 
         Args:
             embedding_dim (int): The size of the embedding vector. Default is 32.
             hidden_units (list[int]): The list of hidden units for the DNN. Default is [64, 64, 64].
             loss_fun (str): The loss function used for training. Default is 'torch.nn.BCELoss()'.
@@ -55,13 +55,12 @@
         # DNN
         emb_flatten = sparse_embedding.flatten(start_dim=1)
         dnn_input = torch.cat((emb_flatten, dense_input), dim=1)
         dnn_output = self.dnn(dnn_input)
 
         y_pred = torch.sigmoid(fm_out + dnn_output)
         if is_training:
-            loss = self.loss_fun(y_pred.squeeze(-1),data['label'])
-            output_dict = {'pred':y_pred,'loss':loss}
+            loss = self.loss_fun(y_pred.squeeze(-1), data['label'])
+            output_dict = {'pred': y_pred, 'loss': loss}
         else:
-            output_dict = {'pred':y_pred}
+            output_dict = {'pred': y_pred}
         return output_dict
-
```

## rec_pangu/models/sequence/__init__.py

```diff
@@ -5,8 +5,13 @@
 # @Time: 2023/3/5 15:08
 from .comirec import ComirecDR, ComirecSA
 from .mind import MIND
 from .yotubednn import YotubeDNN
 from .cmi import CMI
 from .re4 import Re4
 from .narm import NARM
-from .srgnn import SRGNN
+from .sasrec import SASRec
+from .srgnn import SRGNN
+from .gcsan import GCSAN
+from .niser import NISER
+from .nextitnet import NextItNet
+from .stamp import STAMP
```

## rec_pangu/models/sequence/cmi.py

```diff
@@ -6,14 +6,15 @@
 # <Improving Micro-video Recommendation via Contrastive Multiple Interests> SIGIR 2022
 from typing import Dict
 import torch
 from torch import nn
 import torch.nn.functional as F
 from rec_pangu.models.base_model import SequenceBaseModel
 
+
 class CMI(SequenceBaseModel):
     def __init__(self, enc_dict, config):
         super(CMI, self).__init__(enc_dict, config)
 
         self.hidden_size = self.config.get('hidden_size', 64)
         self.num_layers = self.config.get('num_layers', 2)
         self.dropout_prob = self.config.get('dropout_prob', 0)
@@ -64,15 +65,15 @@
 
             w = self.interest_embedding.weight.data.clone()
             w = F.normalize(w, dim=-1, p=2)
             self.interest_embedding.weight.copy_(w)
 
         item_seq = data['hist_item_list']
 
-        item_seq_len = torch.sum(data['hist_mask_list'],dim=-1)
+        item_seq_len = torch.sum(data['hist_mask_list'], dim=-1)
 
         batch_size, n_seq = item_seq.shape
         item_seq_emb = self.item_emb(item_seq)
         item_seq_emb = self.emb_dropout(item_seq_emb)
 
         psnl_interest = self.interest_embedding.weight.unsqueeze(0).repeat(batch_size, 1,
                                                                            1)  # bs * n_interest * embed_size
@@ -112,27 +113,27 @@
         # interest_mask = imp_probs  # 将 interest_mask 用于表示各个兴趣向量的重要程度
 
         psnl_interest = psnl_interest + full_psnl_emb.unsqueeze(1)
         psnl_interest = F.normalize(psnl_interest, p=2, dim=-1)
 
         if is_training:
             output_dict = {
-                'global_user_emb' : full_psnl_emb,
-                'user_emb' : psnl_interest,
+                'global_user_emb': full_psnl_emb,
+                'user_emb': psnl_interest,
                 'loss': self.calculate_cmi_loss(psnl_interest, data['target_item'].squeeze())
             }
         else:
             output_dict = {
                 'user_emb': psnl_interest,
             }
         return output_dict
 
     def get_neg_item(self, batch_size):
         n_item = self.item_emb.weight.shape[0]
-        return torch.randint(1, n_item-1, (batch_size,1)).squeeze()
+        return torch.randint(1, n_item - 1, (batch_size, 1)).squeeze()
 
     def calculate_cmi_loss(self, psnl_interest, pos_items):
         batch_size, n_interest, embed_size = psnl_interest.shape
 
         neg_items = self.get_neg_item(batch_size)
         neg_items = neg_items.to(psnl_interest.device)
 
@@ -162,16 +163,16 @@
         batch_size, n_interest, embed_size = user_interests.shape
         user_interests = user_interests.reshape(batch_size // 2, 2, n_interest, embed_size)
         user_interests_a = user_interests[:, 0].reshape(-1, embed_size)
         user_interests_b = user_interests[:, 1].reshape(-1, embed_size)
         user_interests_a = F.normalize(user_interests_a, p=2, dim=-1)
         user_interests_b = F.normalize(user_interests_b, p=2, dim=-1)
         sim_matrix = user_interests_a.matmul(user_interests_b.transpose(0, 1)) / self.temperature
-        loss = F.cross_entropy(sim_matrix, torch.arange(sim_matrix.shape[0],device=device)) + F.cross_entropy(
-            sim_matrix.transpose(0, 1), torch.arange(sim_matrix.shape[0],device=device))
+        loss = F.cross_entropy(sim_matrix, torch.arange(sim_matrix.shape[0], device=device)) + F.cross_entropy(
+            sim_matrix.transpose(0, 1), torch.arange(sim_matrix.shape[0], device=device))
         return loss
 
     def get_orth_loss(self, x):
         '''
         Args:
             x: batch_size * embed_size; Orthogonal embeddings
         Returns:
```

## rec_pangu/models/sequence/comirec.py

```diff
@@ -7,15 +7,15 @@
 import torch
 from rec_pangu.models.layers import MultiInterestSelfAttention, CapsuleNetwork
 from rec_pangu.models.base_model import SequenceBaseModel
 
 
 class ComirecSA(SequenceBaseModel):
 
-    def __init__(self, enc_dict,config):
+    def __init__(self, enc_dict, config):
         super(ComirecSA, self).__init__(enc_dict, config)
 
         self.multi_interest_sa = MultiInterestSelfAttention(embedding_dim=self.embedding_dim,
                                                             num_attention_heads=self.config['K'])
         self.apply(self._init_weights)
 
     def forward(self, data: Dict[str, torch.tensor], is_training: bool = True):
@@ -44,31 +44,32 @@
             cos_res = torch.bmm(multi_interest_emb, item_e.squeeze(1).unsqueeze(-1))
             k_index = torch.argmax(cos_res, dim=1)
 
             best_interest_emb = torch.rand(multi_interest_emb.shape[0], multi_interest_emb.shape[2]).to(self.device)
             for k in range(multi_interest_emb.shape[0]):
                 best_interest_emb[k, :] = multi_interest_emb[k, k_index[k], :]
 
-            loss = self.calculate_loss(best_interest_emb,item)
+            loss = self.calculate_loss(best_interest_emb, item)
             output_dict = {
-                'user_emb':multi_interest_emb,
-                'loss':loss,
+                'user_emb': multi_interest_emb,
+                'loss': loss,
             }
         else:
             seq_emb = self.item_emb(item_seq)  # Batch,Seq,Emb
             mask = mask.unsqueeze(-1).float()
             multi_interest_emb = self.multi_interest_sa(seq_emb, mask)  # Batch,K,Emb
             output_dict = {
                 'user_emb': multi_interest_emb,
             }
         return output_dict
 
+
 class ComirecDR(SequenceBaseModel):
 
-    def __init__(self, enc_dict,config):
+    def __init__(self, enc_dict, config):
         super(ComirecDR, self).__init__(enc_dict, config)
 
         self.capsule = CapsuleNetwork(self.embedding_dim, self.max_length,
                                       bilinear_type=2, interest_num=self.config['K'])
         self.apply(self._init_weights)
 
     def forward(self, data: Dict[str, torch.tensor], is_training: bool = True):
@@ -88,30 +89,28 @@
         mask = data['hist_mask_list']
 
         if is_training:
             item = data['target_item'].squeeze()
             seq_emb = self.item_emb(item_seq)  # Batch,Seq,Emb
             item_e = self.item_emb(item).squeeze(1)
 
-            multi_interest_emb = self.capsule(seq_emb, mask,self.device)  # Batch,K,Emb
+            multi_interest_emb = self.capsule(seq_emb, mask, self.device)  # Batch,K,Emb
 
             cos_res = torch.bmm(multi_interest_emb, item_e.squeeze(1).unsqueeze(-1))
             k_index = torch.argmax(cos_res, dim=1)
 
             best_interest_emb = torch.rand(multi_interest_emb.shape[0], multi_interest_emb.shape[2]).to(self.device)
             for k in range(multi_interest_emb.shape[0]):
                 best_interest_emb[k, :] = multi_interest_emb[k, k_index[k], :]
 
-            loss = self.calculate_loss(best_interest_emb,item)
+            loss = self.calculate_loss(best_interest_emb, item)
             output_dict = {
-                'user_emb':multi_interest_emb,
-                'loss':loss,
+                'user_emb': multi_interest_emb,
+                'loss': loss,
             }
         else:
             seq_emb = self.item_emb(item_seq)  # Batch,Seq,Emb
-            multi_interest_emb = self.capsule(seq_emb, mask,self.device)  # Batch,K,Emb
+            multi_interest_emb = self.capsule(seq_emb, mask, self.device)  # Batch,K,Emb
             output_dict = {
                 'user_emb': multi_interest_emb,
             }
         return output_dict
-
-
```

## rec_pangu/models/sequence/mind.py

```diff
@@ -4,17 +4,18 @@
 # @Email: 306178200@qq.com
 # @Time: 2023/3/5 15:08
 from typing import Dict
 import torch
 from rec_pangu.models.layers import CapsuleNetwork
 from rec_pangu.models.base_model import SequenceBaseModel
 
+
 class MIND(SequenceBaseModel):
-    def __init__(self, enc_dict,config):
-        super(MIND, self).__init__(enc_dict,config)
+    def __init__(self, enc_dict, config):
+        super(MIND, self).__init__(enc_dict, config)
 
         self.capsule = CapsuleNetwork(self.embedding_dim, self.max_length, bilinear_type=0,
                                       interest_num=self.config['K'])
         self.apply(self._init_weights)
 
     def forward(self, data: Dict[str, torch.tensor], is_training: bool = True):
         """
@@ -42,15 +43,15 @@
             cos_res = torch.bmm(multi_interest_emb, item_e.squeeze(1).unsqueeze(-1))
             k_index = torch.argmax(cos_res, dim=1)
 
             best_interest_emb = torch.rand(multi_interest_emb.shape[0], multi_interest_emb.shape[2]).to(self.device)
             for k in range(multi_interest_emb.shape[0]):
                 best_interest_emb[k, :] = multi_interest_emb[k, k_index[k], :]
 
-            loss = self.calculate_loss(best_interest_emb,item)
+            loss = self.calculate_loss(best_interest_emb, item)
             output_dict = {
                 'user_emb': multi_interest_emb,
                 'loss': loss,
             }
         else:
             seq_emb = self.item_emb(item_seq)  # Batch,Seq,Emb
             multi_interest_emb = self.capsule(seq_emb, mask, self.device)  # Batch,K,Emb
```

## rec_pangu/models/sequence/narm.py

```diff
@@ -10,18 +10,17 @@
 
 
 class NARM(SequenceBaseModel):
     def __init__(self, enc_dict, config):
         super(NARM, self).__init__(enc_dict, config)
 
         self.n_layers = self.config.get('n_layers', 2)
-        self.dropout_probs = self.config.get('dropout_probs',[0.1, 0.1])
+        self.dropout_probs = self.config.get('dropout_probs', [0.1, 0.1])
         self.hidden_size = self.config.get('hidden_size', 32)
 
-
         self.emb_dropout = nn.Dropout(self.dropout_probs[0])
         self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.n_layers, bias=False, batch_first=True)
         self.a_1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
         self.a_2 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
         self.v_t = nn.Linear(self.hidden_size, 1, bias=False)
         self.ct_dropout = nn.Dropout(self.dropout_probs[1])
         self.b = nn.Linear(2 * self.hidden_size, self.embedding_dim, bias=False)
@@ -38,15 +37,15 @@
             is_training (bool): a flag variable to set the mode of the model; default is True.
 
         Returns:
             dict: a dictionary with the user embeddings and model loss (if training) as keys and the corresponding 
             tensors as values.
         """
         item_seq = data['hist_item_list']
-        item_seq_len = torch.sum(data['hist_mask_list'],dim=-1)
+        item_seq_len = torch.sum(data['hist_mask_list'], dim=-1)
 
         item_seq_emb = self.item_emb(item_seq)
         item_seq_emb_dropout = self.emb_dropout(item_seq_emb)
         gru_out, _ = self.gru(item_seq_emb_dropout)
 
         # fetch the last hidden state of last timestamp
         c_global = ht = self.gather_indexes(gru_out, item_seq_len - 1)
@@ -61,15 +60,15 @@
         c_t = torch.cat([c_local, c_global], 1)
         c_t = self.ct_dropout(c_t)
         user_emb = self.b(c_t)
 
         if is_training:
             target_item = data['target_item'].squeeze()
             output_dict = {
-                'user_emb':user_emb,
-                'loss':self.calculate_loss(user_emb, target_item)
+                'user_emb': user_emb,
+                'loss': self.calculate_loss(user_emb, target_item)
             }
         else:
             output_dict = {
                 'user_emb': user_emb,
             }
         return output_dict
```

## rec_pangu/models/sequence/re4.py

```diff
@@ -7,14 +7,15 @@
 from typing import Dict
 import numpy as np
 import torch
 from torch import nn
 import torch.nn.functional as F
 from rec_pangu.models.base_model import SequenceBaseModel
 
+
 class Re4(SequenceBaseModel):
     def __init__(self, enc_dict, config):
         super(Re4, self).__init__(enc_dict, config)
 
         self.num_interests = self.config.get('K', 4)
         self.att_thre = self.config.get('att_thre', -1)
         self.t_cont = self.config.get('t_cont', 0.02)
@@ -51,15 +52,15 @@
             dict: a dictionary with the user embeddings and model loss (if training) as keys and the corresponding 
             tensors as values.
         """
         item_seq = data['hist_item_list']
         item_mask = data['hist_mask_list']
         dim0, dim1 = item_seq.shape
 
-        item_seq_len = torch.sum(item_mask,dim=-1)
+        item_seq_len = torch.sum(item_mask, dim=-1)
         item_seq = torch.reshape(item_seq, (1, dim0 * dim1))
         item_seq_emb = self.item_emb(item_seq)
         item_seq_emb = torch.reshape(item_seq_emb, (dim0, dim1, -1))
 
         proposals_weight = torch.matmul(self.W1_2,
                                         torch.tanh(torch.matmul(self.W1, torch.transpose(item_seq_emb, 1, 2))))
         proposals_weight_logits = proposals_weight.masked_fill(item_mask.unsqueeze(1).bool(), -1e9)
@@ -134,19 +135,18 @@
                 [dim0, self.proposal_num, dim1, -1])
             target_emb = item_seq_emb.unsqueeze(1).repeat(1, self.proposal_num, 1, 1)
             loss_construct = self.recons_mse_loss(recons_item, target_emb)
             loss_construct = loss_construct.masked_fill((positive_weight_idx == 0).unsqueeze(-1), 0.)
             loss_construct = loss_construct.masked_fill(item_mask.unsqueeze(-1).unsqueeze(1).bool(), 0.)
             loss_construct = torch.mean(loss_construct)
 
-            loss = loss + self.att_lambda*loss_attend + self.ct_lambda*loss_contrastive + self.cs_lambda*loss_construct
+            loss = loss + self.att_lambda * loss_attend + self.ct_lambda * loss_contrastive + self.cs_lambda * loss_construct
             output_dict = {
                 'user_emb': user_interests,
                 'loss': loss
             }
         else:
             output_dict = {
                 'user_emb': user_interests
             }
 
         return output_dict
-
```

## rec_pangu/models/sequence/sasrec.py

```diff
@@ -1,5 +1,69 @@
 # -*- ecoding: utf-8 -*-
 # @ModuleName: sasrec
 # @Author: wk
 # @Email: 306178200@qq.com
 # @Time: 2023/3/14 16:24
+from typing import Dict
+import torch
+from torch import nn
+from rec_pangu.models.layers import TransformerEncoder
+from rec_pangu.models.base_model import SequenceBaseModel
+
+
+class SASRec(SequenceBaseModel):
+    def __init__(self, enc_dict, config):
+        super(SASRec, self).__init__(enc_dict, config)
+
+        self.n_layers = config.get('n_layers', 2)
+        self.n_heads = config.get('n_heads', 4)
+        self.hidden_size = config.get('hidden_size', 64)  # same as embedding_size
+        self.inner_size = config.get('inner_size', 32)  # the dimensionality in feed-forward layer
+        self.hidden_dropout_prob = config.get('hidden_dropout_prob', 0.1)
+        self.attn_dropout_prob = config.get('attn_dropout_prob', 0.1)
+        self.hidden_act = config.get('hidden_act', 'gelu')
+        self.layer_norm_eps = config.get('layer_norm_eps', 0.001)
+
+        self.self_attention = TransformerEncoder(
+            n_layers=self.n_layers,
+            n_heads=self.n_heads,
+            hidden_size=self.hidden_size,
+            inner_size=self.inner_size,
+            hidden_dropout_prob=self.hidden_dropout_prob,
+            attn_dropout_prob=self.attn_dropout_prob,
+            hidden_act=self.hidden_act,
+            layer_norm_eps=self.layer_norm_eps
+        )
+
+        self.apply(self._init_weights)
+
+    def forward(self, data: Dict[str, torch.tensor], is_training: bool = True):
+        """
+        This method initializes the forward step to compute the user embeddings which will then be used for
+        recommendations.
+
+        Args:
+            data (dict): a dictionary with input features as keys and the corresponding tensors as values .
+            is_training (bool): a flag variable to set the mode of the model; default is True.
+
+        Returns:
+            dict: a dictionary with the user embeddings and model loss (if training) as keys and the corresponding
+            tensors as values.
+        """
+        item_seq_len = torch.sum(data['hist_mask_list'], dim=-1)
+        item_seq_emb = self.item_emb(data['hist_item_list'])
+        attention_mask = self.get_attention_mask(data['hist_mask_list'])
+        outputs = self.self_attention(item_seq_emb, attention_mask, output_all_encoded_layers=True)
+        output = outputs[-1]
+        user_emb = self.gather_indexes(output, item_seq_len - 1)
+
+        if is_training:
+            target_item = data['target_item'].squeeze()
+            output_dict = {
+                'user_emb': user_emb,
+                'loss': self.calculate_loss(user_emb, target_item)
+            }
+        else:
+            output_dict = {
+                'user_emb': user_emb,
+            }
+        return output_dict
```

## rec_pangu/models/sequence/srgnn.py

```diff
@@ -12,15 +12,15 @@
 
 
 class SRGNN(SequenceBaseModel):
     def __init__(self, enc_dict, config):
         super(SRGNN, self).__init__(enc_dict, config)
         # define layers and loss
 
-        self.step = self.config.get('step',1)
+        self.step = self.config.get('step', 1)
 
         self.gnncell = SRGNNCell(self.embedding_dim)
         self.linear_one = nn.Linear(self.embedding_dim, self.embedding_dim)
         self.linear_two = nn.Linear(self.embedding_dim, self.embedding_dim)
         self.linear_three = nn.Linear(self.embedding_dim, 1, bias=False)
         self.linear_transform = nn.Linear(self.embedding_dim * 2, self.embedding_dim)
```

## rec_pangu/models/sequence/yotubednn.py

```diff
@@ -3,17 +3,18 @@
 # @Author: wk
 # @Email: 306178200@qq.com
 # @Time: 2023/3/5 15:25
 from typing import Dict
 import torch
 from rec_pangu.models.base_model import SequenceBaseModel
 
+
 class YotubeDNN(SequenceBaseModel):
-    def __init__(self, enc_dict,config):
-        super(YotubeDNN, self).__init__(enc_dict,config)
+    def __init__(self, enc_dict, config):
+        super(YotubeDNN, self).__init__(enc_dict, config)
 
         self.apply(self._init_weights)
 
     def forward(self, data: Dict[str, torch.tensor], is_training: bool = True):
         """
         This method initializes the forward step to compute the user embeddings which will then be used for 
         recommendations.
@@ -27,21 +28,20 @@
             tensors as values.
         """
         item_seq = data['hist_item_list']
         mask = data['hist_mask_list']
 
         user_emb = self.item_emb(item_seq)
         mask = mask.unsqueeze(-1).float()
-        user_emb = torch.mean(user_emb*mask,dim=1)
+        user_emb = torch.mean(user_emb * mask, dim=1)
         if is_training:
             item = data['target_item'].squeeze()
             loss = self.calculate_loss(user_emb, item)
             output_dict = {
-                'user_emb':user_emb,
-                'loss':loss
+                'user_emb': user_emb,
+                'loss': loss
             }
         else:
             output_dict = {
-                'user_emb':user_emb
+                'user_emb': user_emb
             }
         return output_dict
-
```

## rec_pangu/utils/check_version.py

```diff
@@ -30,13 +30,15 @@
                 releases = j.get('releases', [])
                 for release in releases:
                     ver = parse(release)
                     if ver.is_prerelease or ver.is_postrelease:
                         continue
                     latest_version = max(latest_version, ver)
                 if latest_version > version:
-                    logger.warning('\nRec_Pangu version {0} detected. Your version is {1}.\nUse `pip install -U rec_pangu` to upgrade.'.format(latest_version,version))
+                    logger.warning(
+                        '\nRec_Pangu version {0} detected. Your version is {1}.\nUse `pip install -U rec_pangu` to upgrade.'.format(
+                            latest_version, version))
         except:
             print("Please check the latest version manually on https://pypi.org/project/rec_pangu/#history")
             return
 
-    Thread(target=check, args=(version,)).start()
+    Thread(target=check, args=(version,)).start()
```

## rec_pangu/utils/evaluate.py

```diff
@@ -45,15 +45,15 @@
             data[key] = data[key].to(device)
 
         # Get user embeddings for the given data.
         model.eval()
         user_embs = model(data, is_training=False)['user_emb']
         user_embs = user_embs.cpu().detach().numpy().astype('float32')
 
-        user_list = data['user'].detach().cpu().numpy()
+        user_list = data['user'].cpu().numpy()
 
         # Get the recommendations using Faiss index.
 
         if len(user_embs.shape) == 2:
 
             # Non-multi-interest model.
             user_embs = normalize(user_embs, norm='l2').astype('float32')
@@ -100,45 +100,47 @@
 
     total_recall = 0.0
     total_ndcg = 0.0
     total_hitrate = 0
 
     # Iterate over each user in the test data
     for user in test_gd.keys():
+        if user not in preds.keys():
+            continue
         recall = 0
         dcg = 0.0
         item_list = test_gd[user]
 
         # Iterate over each actual item in the test data
         for no, item_id in enumerate(item_list):
             if item_id in preds[user][:topN]:
                 # Increment recall for each correctly predicted item
                 recall += 1
                 # Calculate dcg
-                dcg += 1.0 / math.log(no+2, 2)
+                dcg += 1.0 / math.log(no + 2, 2)
 
             # Calculate idcg
             idcg = 0.0
             for no in range(recall):
-                idcg += 1.0 / math.log(no+2, 2)
+                idcg += 1.0 / math.log(no + 2, 2)
 
         # Calculate total recall, total ndcg, and total hitrate
         total_recall += recall * 1.0 / len(item_list)
         if recall > 0:
             total_ndcg += dcg / idcg
             total_hitrate += 1
 
     # Calculate overall recall, overall ndcg, and overall hitrate
     total = len(test_gd)
     recall = total_recall / total
     ndcg = total_ndcg / total
     hitrate = total_hitrate * 1.0 / total
 
     # Return a dictionary containing results
-    return {f'recall@{topN}': round(recall,4), f'ndcg@{topN}': round(ndcg,4), f'hitrate@{topN}': round(hitrate,4)}
+    return {f'recall@{topN}': round(recall, 4), f'ndcg@{topN}': round(ndcg, 4), f'hitrate@{topN}': round(hitrate, 4)}
 
 # def get_recall_predict(model, test_data, device, topN=20,):
 #     item_embs = model.output_items().cpu().detach().numpy()
 #     item_embs = normalize(item_embs, norm='l2').astype('float32')
 #     hidden_size = item_embs.shape[1]
 #     faiss_index = faiss.IndexFlatIP(hidden_size)
 #     faiss_index.add(item_embs)
```

## rec_pangu/utils/json_utils.py

```diff
@@ -15,12 +15,7 @@
         ori_dict: A dictionary to be beautified
 
     Returns:
         A string of formatted JSON with syntax highlighting
     """
     formatted_json = json.dumps(ori_dict, indent=4, ensure_ascii=False, sort_keys=True)
     return highlight(formatted_json, lexers.get_lexer_by_name('json'), formatters.TerminalFormatter())
-
-# def beautify_json(ori_dict):
-#     formatted_json = json.dumps(ori_dict, indent=4, ensure_ascii=False, sort_keys=True)
-#     return highlight(formatted_json, lexers.get_lexer_by_name('json'), formatters.TerminalFormatter())
-#
```

## Comparing `rec_pangu-0.3.4.dist-info/LICENSE` & `rec_pangu-0.3.5.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `rec_pangu-0.3.4.dist-info/METADATA` & `rec_pangu-0.3.5.dist-info/METADATA`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: rec-pangu
-Version: 0.3.4
+Version: 0.3.5
 Summary: Some Rank/Multi-task model implemented by Pytorch
 Home-page: https://github.com/HaSai666/rec_pangu
 Author: wk
 Author-email: 306178200@qq.com
 Keywords: rank,multi task,deep learning,pytorch,recsys,recommendation
 Platform: all
 Classifier: Intended Audience :: Developers
@@ -57,45 +57,73 @@
 cd rec_pangu
 pip install -e . --verbose
 
 #稳定版 
 pip install rec_pangu --upgrade
 ```
 ## 3.Rank模型
-这里目前支持以下Rank模型
-
-| 模型     | 论文                                                                                                                                                                                                                                                                                                             | 年份   | 相关资料 |
-|--------|------|------|------|
-| WDL    | [Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792)     | 2016 | TBD  |
-| DeepFM | [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](https://arxiv.org/pdf/1703.04247)     | 2017 | TBD  |
-| NFM | [Neural Factorization Machines for Sparse Predictive Analytics](https://arxiv.org/pdf/1708.05027.pdf)              | 2017 | TBD  |
-| FiBiNet | [FiBiNET: Combining Feature Importance and Bilinear Feature Interaction for Click-Through Rate](https://arxiv.org/pdf/1905.09433.pdf) | 2019 | TBD  |
-| AFM | [Attentional Factorization Machines](https://arxiv.org/pdf/1708.04617)                  | 2017 | TBD  |
-| AutoInt | [AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks](https://arxiv.org/pdf/1810.11921.pdf)              | 2018 | TBD  |
-| CCPM | [A Convolutional Click Prediction Model](http://www.shuwu.name/sw/Liu2015CCPM.pdf)    | 2015 | TBD  |
-| LR | /  | 2019 | TBD  |
-| FM | /  | 2019 | TBD  |
-| xDeepFM | [xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems](https://arxiv.org/pdf/1803.05170.pdf)     | 2018 | TBD  |
-| DCN | [Deep & Cross Network for Ad Click Predictions](https://arxiv.org/pdf/1708.05123.pdf) | 2019 | TBD  |
 
+| 模型      | 论文     | 年份   | 
+|---------|------|------|
+| WDL     | [Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792)     | 2016 | 
+| DeepFM  | [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](https://arxiv.org/pdf/1703.04247)     | 2017 | 
+| NFM     | [Neural Factorization Machines for Sparse Predictive Analytics](https://arxiv.org/pdf/1708.05027.pdf)              | 2017 | 
+| FiBiNet | [FiBiNET: Combining Feature Importance and Bilinear Feature Interaction for Click-Through Rate](https://arxiv.org/pdf/1905.09433.pdf) | 2019 | 
+| AFM     | [Attentional Factorization Machines](https://arxiv.org/pdf/1708.04617)                  | 2017 | 
+| AutoInt | [AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks](https://arxiv.org/pdf/1810.11921.pdf)              | 2018 | 
+| CCPM    | [A Convolutional Click Prediction Model](http://www.shuwu.name/sw/Liu2015CCPM.pdf)    | 2015 |
+| LR      | /  | 2019 | 
+| FM      | /  | 2019 | 
+| xDeepFM | [xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems](https://arxiv.org/pdf/1803.05170.pdf)     | 2018 |
+| DCN     | [Deep & Cross Network for Ad Click Predictions](https://arxiv.org/pdf/1708.05123.pdf) | 2019 | 
+| MaskNet | [MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask](https://arxiv.org/pdf/2102.07619.pdf) | 2021 | 
 ## 4.多任务模型
-目前支持以下多任务模型
 
-| 模型          | 论文                                                                                                                                          | 年份   | 相关资料 |
-|-------------|---------------------------------------------------------------------------------------------------------------------------------------------|------|------|
-| MMOE        | [Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 | TBD  |
-| ShareBottom | [Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 | TBD  |
-| ESSM        | [Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate](https://arxiv.org/pdf/1804.07931.pdf)      | 2018 | TBD  |
-| OMOE        | [Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 | TBD  |
-| MLMMOE      | /                                                                                                                                           | /    | TBD  |
-| AITM        | [Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising](https://arxiv.org/pdf/2105.08489.pdf)| 2019 | TBD  |
+| 模型          | 论文                                                                                                                                          | 年份   | 
+|-------------|---------------------------------------------------------------------------------------------------------------------------------------------|------|
+| MMOE        | [Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 |
+| ShareBottom | [Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 |
+| ESSM        | [Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate](https://arxiv.org/pdf/1804.07931.pdf)      | 2018 |
+| OMOE        | [Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 |
+| MLMMOE      | /                                                                                                                                           | /    |
+| AITM        | [Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising](https://arxiv.org/pdf/2105.08489.pdf)| 2019 |
+
+## 5.序列召回模型
+
+目前支持如下类型的序列召回模型:
+
+- 经典序列召回模型
+- 基于图的序列召回模型
+- 多兴趣序列召回模型
+
+
+| 模型        | 类型     | 论文                                                                                                                                     | 年份   | 
+|-----------|--------|-------------------------------------------------------------------------------------------------------------------------------------------|------|
+| YotubeDNN | 经典序列召回 |[Deep Neural Networks for YouTube Recommendations](https://dl.acm.org/doi/pdf/10.1145/2959100.2959190?utm_campaign=Weekly%20dose%20of%20Machine%20Learning&utm_medium=email&utm_source=Revue%20newsletter) | 2016 |
+| Gru4Rec   | 经典序列召回 |[Session-based recommendations with recurrent neural networks](https://arxiv.org/pdf/1511.06939) | 2015 |
+| Narm      | 经典序列召回 |[Neural Attentive Session-based Recommendation](https://arxiv.org/pdf/1711.04725) | 2017 |
+| NextItNet | 经典序列召回 |[A Simple Convolutional Generative Network for Next Item](https://arxiv.org/pdf/1808.05163) | 2019 |
+| ComirecSA | 多兴趣召回  |[Controllable multi-interest framework for recommendation](https://arxiv.org/pdf/2005.09347) | 2020 |
+| ComirecDR | 多兴趣召回  |[Controllable multi-interest framework for recommendation](https://arxiv.org/pdf/2005.09347) | 2020 |
+| Mind      | 多兴趣召回  |[Multi-Interest Network with Dynamic Routing for Recommendation at Tmall](https://arxiv.org/pdf/1904.08030) | 2019 |
+| Re4       | 多兴趣召回  |[Re4: Learning to Re-contrast, Re-attend, Re-construct for Multi-interest Recommendation](https://dl.acm.org/doi/10.1145/3485447.3512094) | 2022 |
+| CMI       | 多兴趣召回  |[mproving Micro-video Recommendation via Contrastive Multiple Interests](https://arxiv.org/pdf/2205.09593) | 2022 |
+| SRGNN     | 图序列召回  |[Session-based Recommendation with Graph Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/3804/3682) | 2019 |
+| GC-SAN    | 图序列召回  |[SGraph Contextualized Self-Attention Network for Session-based Recommendation](https://www.ijcai.org/proceedings/2019/0547.pdf) | 2019 |
+| NISER     | 图序列召回  |[NISER: Normalized Item and Session Representations to Handle Popularity Bias](https://arxiv.org/pdf/1909.04276) | 2019 |
+
+
 
-## 5.Demo
+## 6.图协同过滤模型
+
+TODO
+
+## 7.Demo
 我们的Rank和多任务模型所对外暴露的接口十分相似,同时我们这里也支持使用wandb来实时监测模型训练指标,我们下面会分别给出Rank,多任务模型,wandb的demo
-### 5.1 Rank Demo
+### 7.1 排序任务Demo
 
 ```python
 # 声明数据schema
 import torch
 from rec_pangu.dataset import get_dataloader
 from rec_pangu.models.ranking import WDL, DeepFM, NFM, FiBiNet, AFM, AFN, AOANet, AutoInt, CCPM, LR, FM, xDeepFM
 from rec_pangu.trainer import RankTrainer
@@ -131,15 +159,15 @@
     # 保存模型权重
     trainer.save_model(model, './model_ckpt')
     # 模型验证
     test_metric = trainer.evaluate_model(model, test_loader, device=device)
     print('Test metric:{}'.format(test_metric))
 
 ```
-### 5.2 多任务模型Demo
+### 7.2 多任务模型Demo
 
 ```python
 import torch
 from rec_pangu.dataset import get_dataloader
 from rec_pangu.models.multi_task import AITM, ShareBottom, ESSM, MMOE, OMOE, MLMMOE
 from rec_pangu.trainer import RankTrainer
 import pandas as pd
@@ -173,59 +201,60 @@
     trainer.fit(model, train_loader, valid_loader, epoch=5, lr=1e-3, device=device)
     #保存模型权重
     trainer.save_model(model, './model_ckpt')
     #模型验证
     test_metric = trainer.evaluate_model(model, test_loader, device=device)
     print('Test metric:{}'.format(test_metric))
 ```
-### 5.3 Wandb Demo
-
+### 7.3 序列召回Demo
 ```python
 import torch
 from rec_pangu.dataset import get_dataloader
-from rec_pangu.models.ranking import WDL, DeepFM, NFM, FiBiNet, AFM, AFN, AOANet, AutoInt, CCPM, LR, FM, xDeepFM, DCN
-from rec_pangu.trainer import RankTrainer
+from rec_pangu.models.sequence import ComirecSA,ComirecDR,MIND,CMI,Re4,NARM,YotubeDNN,SRGNN
+from rec_pangu.trainer import SequenceTrainer
+from rec_pangu.utils import set_device
 import pandas as pd
 
-df = pd.read_csv('sample_data/ranking_sample_data.csv')
-# 声明数据schema
-schema = {
-    "sparse_cols": ['user_id', 'item_id', 'item_type', 'dayofweek', 'is_workday', 'city', 'county',
-                    'town', 'village', 'lbs_city', 'lbs_district', 'hardware_platform', 'hardware_ischarging',
-                    'os_type', 'network_type', 'position'],
-    "dense_cols": ['item_expo_1d', 'item_expo_7d', 'item_expo_14d', 'item_expo_30d', 'item_clk_1d',
-                   'item_clk_7d', 'item_clk_14d', 'item_clk_30d', 'use_duration'],
-    "label_col": 'click',
-}
-# 只需要额外增加wandb_config即可
-wandb_config = {
-    'key': 'ca0a80eab60eff065b8c16ab3f41dec4783e60ae',
-    'project': 'pangu_ranking_example',
-    'name': 'exp_2',
-    'config': {
-        'embedding_dim': 16,
-        'hidden_units': [64, 32, 16]
+if __name__=='__main__':
+    #声明数据schema
+    schema = {
+        'user_col': 'user_id',
+        'item_col': 'item_id',
+        'cate_cols': ['genre'],
+        'max_length': 20,
+        'time_col': 'timestamp',
+        'task_type':'sequence'
+    }
+    # 模型配置
+    config = {
+        'embedding_dim': 64,
+        'lr': 0.001,
+        'K': 1,
+        'device':-1,
     }
-}
-# 准备数据,这里只选择了100条数据,所以没有切分数据集
-train_df = df[:80]
-valid_df = df[:90]
-test_df = df[:95]
-
-# 声明使用的device
-device = torch.device('cpu')
-# 获取dataloader
-train_loader, valid_loader, test_loader, enc_dict = get_dataloader(train_df, valid_df, test_df, schema, batch_size=512)
-# 声明模型,排序模型目前支持：WDL, DeepFM, NFM, FiBiNet, AFM, AFN, AOANet, AutoInt, CCPM, LR, FM, xDeepFM
-model = DeepFM(**wandb_config['config'], enc_dict=enc_dict)
-# 声明Trainer
-trainer = RankTrainer(num_task=1, wandb_config=wandb_config)
-# 训练模型
-trainer.fit(model, train_loader, valid_loader, epoch=500, lr=1e-3, device=device,
-            use_earlystopping=True, max_patience=5, monitor_metric='valid_roc_auc_score')
-# 保存模型权重
-# trainer.save_model(model, './model_ckpt')
-# 保存模型权重和enc_dict
-trainer.save_all(model, enc_dict, './model_ckpt')
-# 模型验证
-test_metric = trainer.evaluate_model(model, test_loader, device=device)
+    config['device'] = set_device(config['device'])
+    config.update(schema)
+
+    #样例数据
+    train_df = pd.read_csv('./sample_data/sample_train.csv')
+    valid_df = pd.read_csv('./sample_data/sample_valid.csv')
+    test_df = pd.read_csv('./sample_data/sample_test.csv')
+
+    #声明使用的device
+    device = torch.device('cpu')
+    #获取dataloader
+    train_loader, valid_loader, test_loader, enc_dict = get_dataloader(train_df, valid_df, test_df, schema, batch_size=50)
+    #声明模型,序列召回模型模型目前支持： ComirecSA,ComirecDR,MIND,CMI,Re4,NARM,YotubeDNN,SRGNN
+    model = ComirecSA(enc_dict=enc_dict,config=config)
+    #声明Trainer
+    trainer = SequenceTrainer(model_ckpt_dir='./model_ckpt')
+    #训练模型
+    trainer.fit(model, train_loader, valid_loader, epoch=500, lr=1e-3, device=device,log_rounds=10,
+                use_earlystoping=True, max_patience=5, monitor_metric='recall@20',)
+    #保存模型权重和enc_dict
+    trainer.save_all(model, enc_dict, './model_ckpt')
+    #模型验证
+    test_metric = trainer.evaluate_model(model, test_loader, device=device)
+
 ```
+### 7.4 图协调过滤Demo
+TODO
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: rec-pangu Version: 0.3.4 Summary: Some Rank/Multi-
+Metadata-Version: 2.1 Name: rec-pangu Version: 0.3.5 Summary: Some Rank/Multi-
 task model implemented by Pytorch Home-page: https://github.com/HaSai666/
 rec_pangu Author: wk Author-email: 306178200@qq.com Keywords: rank,multi
 task,deep learning,pytorch,recsys,recommendation Platform: all Classifier:
 Intended Audience :: Developers Classifier: Intended Audience :: Education
 Classifier: Intended Audience :: Science/Research Classifier: Operating System
 :: OS Independent Classifier: Programming Language :: Python :: 3 Classifier:
 Programming Language :: Python :: 3.7 Classifier: Programming Language ::
@@ -36,49 +36,77 @@
 å¤ä»»å¡æ¨¡åè¿è¡å®ç°ï¼å¹¶ä¸å¯¹å¤æä¾ç»ä¸è°ç¨çAPIæ¥å£ï¼æå¤§çéä½äºä½¿ç¨Rank/
 å¤ä»»å¡æ¨¡åçæ¶é´ææ¬ -
 è¯¥é¡¹ç®ä½¿ç¨äºpytorchæ¥å®ç°æä»¬çåç§æ¨¡åï¼ä»¥ä¾¿äºåå­¦æ¨èç³»ç»çäººå¯ä»¥æ´å¥½ççè§£ç®æ³çæ ¸å¿ææ³
 -
 ç±äºå·²ç»æäºå¾å¤ç±»ä¼¼çä¼ç§çå¼æºï¼æä»¬è¿éå¯¹é£äºååéç¨çæ¨¡ååèäºå·²æçå¼æºï¼ååæè°¢è¿äºå¼æºè´¡ç®èçè´¡ç®
 ## 2.å®è£ ```bash #ææ°ç git clone https://github.com/HaSai666/
 rec_pangu.git cd rec_pangu pip install -e . --verbose #ç¨³å®ç pip install
-rec_pangu --upgrade ``` ## 3.Rankæ¨¡å è¿éç®åæ¯æä»¥ä¸Rankæ¨¡å |
-æ¨¡å | è®ºæ | å¹´ä»½ | ç¸å³èµæ | |--------|------|------|------| | WDL
-| [Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/
-1606.07792) | 2016 | TBD | | DeepFM | [DeepFM: A Factorization-Machine based
-Neural Network for CTR Prediction](https://arxiv.org/pdf/1703.04247) | 2017 |
-TBD | | NFM | [Neural Factorization Machines for Sparse Predictive Analytics]
-(https://arxiv.org/pdf/1708.05027.pdf) | 2017 | TBD | | FiBiNet | [FiBiNET:
-Combining Feature Importance and Bilinear Feature Interaction for Click-Through
-Rate](https://arxiv.org/pdf/1905.09433.pdf) | 2019 | TBD | | AFM | [Attentional
-Factorization Machines](https://arxiv.org/pdf/1708.04617) | 2017 | TBD | |
-AutoInt | [AutoInt: Automatic Feature Interaction Learning via Self-Attentive
-Neural Networks](https://arxiv.org/pdf/1810.11921.pdf) | 2018 | TBD | | CCPM |
-[A Convolutional Click Prediction Model](http://www.shuwu.name/sw/
-Liu2015CCPM.pdf) | 2015 | TBD | | LR | / | 2019 | TBD | | FM | / | 2019 | TBD |
-| xDeepFM | [xDeepFM: Combining Explicit and Implicit Feature Interactions for
-Recommender Systems](https://arxiv.org/pdf/1803.05170.pdf) | 2018 | TBD | | DCN
-| [Deep & Cross Network for Ad Click Predictions](https://arxiv.org/pdf/
-1708.05123.pdf) | 2019 | TBD | ## 4.å¤ä»»å¡æ¨¡å
-ç®åæ¯æä»¥ä¸å¤ä»»å¡æ¨¡å | æ¨¡å | è®ºæ | å¹´ä»½ | ç¸å³èµæ |
-|-------------|----------------------------------------------------------------
------------------------------------------------------------------------------|-
------|------| | MMOE | [Modeling Task Relationships in Multi-task Learning with
-Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/pdf/10.1145/
-3219819.3220007) | 2018 | TBD | | ShareBottom | [Modeling Task Relationships in
-Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/
-pdf/10.1145/3219819.3220007) | 2018 | TBD | | ESSM | [Entire Space Multi-Task
-Model: An Effective Approach for Estimating Post-Click Conversion Rate](https:/
-/arxiv.org/pdf/1804.07931.pdf) | 2018 | TBD | | OMOE | [Modeling Task
+rec_pangu --upgrade ``` ## 3.Rankæ¨¡å | æ¨¡å | è®ºæ | å¹´ä»½ | |---------
+|------|------| | WDL | [Wide & Deep Learning for Recommender Systems](https://
+arxiv.org/pdf/1606.07792) | 2016 | | DeepFM | [DeepFM: A Factorization-Machine
+based Neural Network for CTR Prediction](https://arxiv.org/pdf/1703.04247) |
+2017 | | NFM | [Neural Factorization Machines for Sparse Predictive Analytics]
+(https://arxiv.org/pdf/1708.05027.pdf) | 2017 | | FiBiNet | [FiBiNET: Combining
+Feature Importance and Bilinear Feature Interaction for Click-Through Rate]
+(https://arxiv.org/pdf/1905.09433.pdf) | 2019 | | AFM | [Attentional
+Factorization Machines](https://arxiv.org/pdf/1708.04617) | 2017 | | AutoInt |
+[AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural
+Networks](https://arxiv.org/pdf/1810.11921.pdf) | 2018 | | CCPM | [A
+Convolutional Click Prediction Model](http://www.shuwu.name/sw/Liu2015CCPM.pdf)
+| 2015 | | LR | / | 2019 | | FM | / | 2019 | | xDeepFM | [xDeepFM: Combining
+Explicit and Implicit Feature Interactions for Recommender Systems](https://
+arxiv.org/pdf/1803.05170.pdf) | 2018 | | DCN | [Deep & Cross Network for Ad
+Click Predictions](https://arxiv.org/pdf/1708.05123.pdf) | 2019 | | MaskNet |
+[MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by
+Instance-Guided Mask](https://arxiv.org/pdf/2102.07619.pdf) | 2021 | ##
+4.å¤ä»»å¡æ¨¡å | æ¨¡å | è®ºæ | å¹´ä»½ | |-------------|-----------------
+-------------------------------------------------------------------------------
+---------------------------------------------|------| | MMOE | [Modeling Task
 Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https:
-//dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 | TBD | | MLMMOE | / | / |
-TBD | | AITM | [Modeling the Sequential Dependence among Audience Multi-step
-Conversions with Multi-task Learning in Targeted Display Advertising](https://
-arxiv.org/pdf/2105.08489.pdf)| 2019 | TBD | ## 5.Demo
+//dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 | | ShareBottom |
+[Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-
+Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 | | ESSM |
+[Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click
+Conversion Rate](https://arxiv.org/pdf/1804.07931.pdf) | 2018 | | OMOE |
+[Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-
+Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007) | 2018 | | MLMMOE
+| / | / | | AITM | [Modeling the Sequential Dependence among Audience Multi-
+step Conversions with Multi-task Learning in Targeted Display Advertising]
+(https://arxiv.org/pdf/2105.08489.pdf)| 2019 | ## 5.åºåå¬åæ¨¡å
+ç®åæ¯æå¦ä¸ç±»åçåºåå¬åæ¨¡å: - ç»å¸åºåå¬åæ¨¡å -
+åºäºå¾çåºåå¬åæ¨¡å - å¤å´è¶£åºåå¬åæ¨¡å | æ¨¡å | ç±»å
+| è®ºæ | å¹´ä»½ | |-----------|--------|-------------------------------------
+-------------------------------------------------------------------------------
+-----------------------|------| | YotubeDNN | ç»å¸åºåå¬å |[Deep Neural
+Networks for YouTube Recommendations](https://dl.acm.org/doi/pdf/10.1145/
+2959100.2959190?utm_campaign=Weekly%20dose%20of%20Machine%20Learning&utm_medium=email&utm_source=Revue%20newsletter)
+| 2016 | | Gru4Rec | ç»å¸åºåå¬å |[Session-based recommendations with
+recurrent neural networks](https://arxiv.org/pdf/1511.06939) | 2015 | | Narm |
+ç»å¸åºåå¬å |[Neural Attentive Session-based Recommendation](https://
+arxiv.org/pdf/1711.04725) | 2017 | | NextItNet | ç»å¸åºåå¬å |[A Simple
+Convolutional Generative Network for Next Item](https://arxiv.org/pdf/
+1808.05163) | 2019 | | ComirecSA | å¤å´è¶£å¬å |[Controllable multi-
+interest framework for recommendation](https://arxiv.org/pdf/2005.09347) | 2020
+| | ComirecDR | å¤å´è¶£å¬å |[Controllable multi-interest framework for
+recommendation](https://arxiv.org/pdf/2005.09347) | 2020 | | Mind |
+å¤å´è¶£å¬å |[Multi-Interest Network with Dynamic Routing for
+Recommendation at Tmall](https://arxiv.org/pdf/1904.08030) | 2019 | | Re4 |
+å¤å´è¶£å¬å |[Re4: Learning to Re-contrast, Re-attend, Re-construct for
+Multi-interest Recommendation](https://dl.acm.org/doi/10.1145/3485447.3512094)
+| 2022 | | CMI | å¤å´è¶£å¬å |[mproving Micro-video Recommendation via
+Contrastive Multiple Interests](https://arxiv.org/pdf/2205.09593) | 2022 | |
+SRGNN | å¾åºåå¬å |[Session-based Recommendation with Graph Neural
+Networks](https://ojs.aaai.org/index.php/AAAI/article/view/3804/3682) | 2019 |
+| GC-SAN | å¾åºåå¬å |[SGraph Contextualized Self-Attention Network for
+Session-based Recommendation](https://www.ijcai.org/proceedings/2019/0547.pdf)
+| 2019 | | NISER | å¾åºåå¬å |[NISER: Normalized Item and Session
+Representations to Handle Popularity Bias](https://arxiv.org/pdf/1909.04276) |
+2019 | ## 6.å¾ååè¿æ»¤æ¨¡å TODO ## 7.Demo
 æä»¬çRankåå¤ä»»å¡æ¨¡åæå¯¹å¤æ´é²çæ¥å£ååç¸ä¼¼,åæ¶æä»¬è¿éä¹æ¯æä½¿ç¨wandbæ¥å®æ¶çæµæ¨¡åè®­ç»ææ ,æä»¬ä¸é¢ä¼åå«ç»åºRank,å¤ä»»å¡æ¨¡å,wandbçdemo
-### 5.1 Rank Demo ```python # å£°ææ°æ®schema import torch from
+### 7.1 æåºä»»å¡Demo ```python # å£°ææ°æ®schema import torch from
 rec_pangu.dataset import get_dataloader from rec_pangu.models.ranking import
 WDL, DeepFM, NFM, FiBiNet, AFM, AFN, AOANet, AutoInt, CCPM, LR, FM, xDeepFM
 from rec_pangu.trainer import RankTrainer import pandas as pd if __name__ ==
 '__main__': df = pd.read_csv('sample_data/ranking_sample_data.csv') print
 (df.head()) # å£°ææ°æ®schema schema = { "sparse_cols": ['user_id',
 'item_id', 'item_type', 'dayofweek', 'is_workday', 'city', 'county', 'town',
 'village', 'lbs_city', 'lbs_district', 'hardware_platform',
@@ -92,15 +120,15 @@
 enc_dict = get_dataloader(train_df, valid_df, test_df, schema) #
 å£°ææ¨¡å,æåºæ¨¡åç®åæ¯æï¼WDL, DeepFM, NFM, FiBiNet, AFM, AFN,
 AOANet, AutoInt, CCPM, LR, FM, xDeepFM model = xDeepFM(enc_dict=enc_dict) #
 å£°æTrainer trainer = RankTrainer(num_task=1) # è®­ç»æ¨¡å trainer.fit
 (model, train_loader, valid_loader, epoch=5, lr=1e-3, device=device) #
 ä¿å­æ¨¡åæé trainer.save_model(model, './model_ckpt') # æ¨¡åéªè¯
 test_metric = trainer.evaluate_model(model, test_loader, device=device) print
-('Test metric:{}'.format(test_metric)) ``` ### 5.2 å¤ä»»å¡æ¨¡åDemo
+('Test metric:{}'.format(test_metric)) ``` ### 7.2 å¤ä»»å¡æ¨¡åDemo
 ```python import torch from rec_pangu.dataset import get_dataloader from
 rec_pangu.models.multi_task import AITM, ShareBottom, ESSM, MMOE, OMOE, MLMMOE
 from rec_pangu.trainer import RankTrainer import pandas as pd if __name__ ==
 '__main__': df = pd.read_csv('sample_data/multi_task_sample_data.csv') print
 (df.head()) #å£°ææ°æ®schema schema = { "sparse_cols": ['user_id',
 'item_id', 'item_type', 'dayofweek', 'is_workday', 'city', 'county', 'town',
 'village', 'lbs_city', 'lbs_district', 'hardware_platform',
@@ -114,35 +142,30 @@
 enc_dict = get_dataloader(train_df, valid_df, test_df, schema)
 #å£°ææ¨¡å,å¤ä»»å¡æ¨¡åç®åæ¯æï¼AITM,ShareBottom,ESSM,MMOE,OMOE,MLMMOE
 model = AITM(enc_dict=enc_dict) #å£°æTrainer trainer = RankTrainer
 (num_task=2) #è®­ç»æ¨¡å trainer.fit(model, train_loader, valid_loader,
 epoch=5, lr=1e-3, device=device) #ä¿å­æ¨¡åæé trainer.save_model(model,
 './model_ckpt') #æ¨¡åéªè¯ test_metric = trainer.evaluate_model(model,
 test_loader, device=device) print('Test metric:{}'.format(test_metric)) ``` ###
-5.3 Wandb Demo ```python import torch from rec_pangu.dataset import
-get_dataloader from rec_pangu.models.ranking import WDL, DeepFM, NFM, FiBiNet,
-AFM, AFN, AOANet, AutoInt, CCPM, LR, FM, xDeepFM, DCN from rec_pangu.trainer
-import RankTrainer import pandas as pd df = pd.read_csv('sample_data/
-ranking_sample_data.csv') # å£°ææ°æ®schema schema = { "sparse_cols":
-['user_id', 'item_id', 'item_type', 'dayofweek', 'is_workday', 'city',
-'county', 'town', 'village', 'lbs_city', 'lbs_district', 'hardware_platform',
-'hardware_ischarging', 'os_type', 'network_type', 'position'], "dense_cols":
-['item_expo_1d', 'item_expo_7d', 'item_expo_14d', 'item_expo_30d',
-'item_clk_1d', 'item_clk_7d', 'item_clk_14d', 'item_clk_30d', 'use_duration'],
-"label_col": 'click', } # åªéè¦é¢å¤å¢å wandb_configå³å¯ wandb_config
-= { 'key': 'ca0a80eab60eff065b8c16ab3f41dec4783e60ae', 'project':
-'pangu_ranking_example', 'name': 'exp_2', 'config': { 'embedding_dim': 16,
-'hidden_units': [64, 32, 16] } } #
-åå¤æ°æ®,è¿éåªéæ©äº100æ¡æ°æ®,æä»¥æ²¡æååæ°æ®é
-train_df = df[:80] valid_df = df[:90] test_df = df[:95] # å£°æä½¿ç¨çdevice
-device = torch.device('cpu') # è·ådataloader train_loader, valid_loader,
-test_loader, enc_dict = get_dataloader(train_df, valid_df, test_df, schema,
-batch_size=512) # å£°ææ¨¡å,æåºæ¨¡åç®åæ¯æï¼WDL, DeepFM, NFM,
-FiBiNet, AFM, AFN, AOANet, AutoInt, CCPM, LR, FM, xDeepFM model = DeepFM
-(**wandb_config['config'], enc_dict=enc_dict) # å£°æTrainer trainer =
-RankTrainer(num_task=1, wandb_config=wandb_config) # è®­ç»æ¨¡å trainer.fit
-(model, train_loader, valid_loader, epoch=500, lr=1e-3, device=device,
-use_earlystopping=True, max_patience=5, monitor_metric='valid_roc_auc_score') #
-ä¿å­æ¨¡åæé # trainer.save_model(model, './model_ckpt') #
-ä¿å­æ¨¡åæéåenc_dict trainer.save_all(model, enc_dict, './model_ckpt')
-# æ¨¡åéªè¯ test_metric = trainer.evaluate_model(model, test_loader,
-device=device) ```
+7.3 åºåå¬åDemo ```python import torch from rec_pangu.dataset import
+get_dataloader from rec_pangu.models.sequence import
+ComirecSA,ComirecDR,MIND,CMI,Re4,NARM,YotubeDNN,SRGNN from rec_pangu.trainer
+import SequenceTrainer from rec_pangu.utils import set_device import pandas as
+pd if __name__=='__main__': #å£°ææ°æ®schema schema = { 'user_col':
+'user_id', 'item_col': 'item_id', 'cate_cols': ['genre'], 'max_length': 20,
+'time_col': 'timestamp', 'task_type':'sequence' } # æ¨¡åéç½® config =
+{ 'embedding_dim': 64, 'lr': 0.001, 'K': 1, 'device':-1, } config['device'] =
+set_device(config['device']) config.update(schema) #æ ·ä¾æ°æ® train_df =
+pd.read_csv('./sample_data/sample_train.csv') valid_df = pd.read_csv('./
+sample_data/sample_valid.csv') test_df = pd.read_csv('./sample_data/
+sample_test.csv') #å£°æä½¿ç¨çdevice device = torch.device('cpu')
+#è·ådataloader train_loader, valid_loader, test_loader, enc_dict =
+get_dataloader(train_df, valid_df, test_df, schema, batch_size=50)
+#å£°ææ¨¡å,åºåå¬åæ¨¡åæ¨¡åç®åæ¯æï¼
+ComirecSA,ComirecDR,MIND,CMI,Re4,NARM,YotubeDNN,SRGNN model = ComirecSA
+(enc_dict=enc_dict,config=config) #å£°æTrainer trainer = SequenceTrainer
+(model_ckpt_dir='./model_ckpt') #è®­ç»æ¨¡å trainer.fit(model, train_loader,
+valid_loader, epoch=500, lr=1e-3, device=device,log_rounds=10,
+use_earlystoping=True, max_patience=5, monitor_metric='recall@20',)
+#ä¿å­æ¨¡åæéåenc_dict trainer.save_all(model, enc_dict, './
+model_ckpt') #æ¨¡åéªè¯ test_metric = trainer.evaluate_model(model,
+test_loader, device=device) ``` ### 7.4 å¾åè°è¿æ»¤Demo TODO
```

## Comparing `rec_pangu-0.3.4.dist-info/RECORD` & `rec_pangu-0.3.5.dist-info/RECORD`

 * *Files 20% similar despite different names*

```diff
@@ -1,67 +1,76 @@
-rec_pangu/__init__.py,sha256=9I-HAFrRW79RCNk_rnyIf2ztCyKydofrNRVYt-l20Mw,199
-rec_pangu/benchmark_trainer.py,sha256=NGWDyEbfAMhtmN4CDJxZ9HrFurAoFhsp_layh6GrTsI,5787
+rec_pangu/__init__.py,sha256=LOxIlNamZjW_p3zv0osfF999QZ57y3ckORHPx2gQLQg,199
+rec_pangu/benchmark_trainer.py,sha256=J5kGL93iq4O7TW1k60oJZksrYRW4OniJa-JoVjmmelk,5837
 rec_pangu/gpt_ranktrainer.py,sha256=cHLZu4OgbpqL9vVMqtR1pvqMeNpLtyO_EdaJHMjWq08,9722
-rec_pangu/model_pipeline.py,sha256=746nnPxe6cIrTnf1Z8txw3mfj2knN4Qq2c4FK6JWUL4,14852
+rec_pangu/model_pipeline.py,sha256=y2_EnjqjU0jY1VpVC6r9sUYePtdQlpqznSskCmxMDn4,14630
 rec_pangu/old_ranktrainer.py,sha256=lMwui-xDAxMvyuxJyp8XKCV3BtQGWl39zbIL_bj6LUM,5448
-rec_pangu/trainer.py,sha256=eCC3Au1YzAal-OrcbfGxujzmokkqmD93VOmooLXosD8,18576
+rec_pangu/trainer.py,sha256=8joVSeeZE8W7WLvLW5XwwM04SsM3b2egnQqfTFKH1f4,18650
 rec_pangu/dataset/__init__.py,sha256=o53SpwAQVsmLQp7CUWmIy7kikwH3ZBD-VBrW_5p2ToA,404
-rec_pangu/dataset/base_dataset.py,sha256=UtPUF-i8cHgAN__oMHoclvNbGtDiVlQWt6VUkb3Gas4,7625
-rec_pangu/dataset/graph_dataset.py,sha256=XpXhlBeJZdpEDgUsNG7UcOMK1JBbAYm8mOyOmBxeBBI,3776
-rec_pangu/dataset/multi_task_dataset.py,sha256=708IihQ1oxrH03rFaABJNc0PgqoEZd43LXtZoX-4bPo,4043
-rec_pangu/dataset/process_data.py,sha256=5Nv4XWmNRO-ODqqUYyTPGrZDJwzAZfT6ahtbHeiV3JE,3746
-rec_pangu/dataset/sequence_dataset.py,sha256=zI9f4DnF8MrlToyRTZzkEvK7ZPoD5rVhefylg-Ttz3o,5594
+rec_pangu/dataset/base_dataset.py,sha256=iImLln0AwCbDWizCuJzahz5eL1etRU7XP0Y2vklLzRs,4954
+rec_pangu/dataset/graph_dataset.py,sha256=Sz_PB9ibzf8X3zBncN8ozXY40k1Q5-ceRAWW0X5LMmk,3776
+rec_pangu/dataset/multi_task_dataset.py,sha256=XnQUTsvwWcYYWq52OtaBfpgfQ0jvLzaowaISNUhMjWg,2812
+rec_pangu/dataset/process_data.py,sha256=DRgqDeVwAym2PCZolwS-wHPBs9VU9p2mSlfaGxZUC3k,3986
+rec_pangu/dataset/sequence_dataset.py,sha256=Jr-SMzdqwquxh2W4yODeL7k5fmIjd07fgNLbQclnsYI,5635
 rec_pangu/models/__init__.py,sha256=ZTUYJ8guxLRAXkbK6TxxvRJw8Q153cUDA1XBcJwjqsI,117
-rec_pangu/models/base_model.py,sha256=piUz1sMSao5959PqB8gr7d-4n0FOaBIPKC3Nl_fQ8eg,7622
-rec_pangu/models/utils.py,sha256=XQb0urw9S83yHYaJMonA2B9XtMhjcUd7n2epu1qCDi4,8063
+rec_pangu/models/base_model.py,sha256=9VlZOAIaRUGuvKWfTDLiCXy0-t9jbv9bjAD7tOg3UFY,9351
+rec_pangu/models/utils.py,sha256=0_thQaedHWmVEfuJ7C1eqF4L-Ya0Wc7ouw8AxHqM3e0,8027
 rec_pangu/models/layers/LGConv.py,sha256=ZvqwTQ0zkmCK5XdwNZNChytFrCUtNT-AAFNqxp2WtLA,949
-rec_pangu/models/layers/__init__.py,sha256=tOYDlESC353Vt5_t0xOE-cM4xR2bAxa6mhbNs72_Eys,337
+rec_pangu/models/layers/__init__.py,sha256=5nIwNU6Rh0hFRypJV6wAbpQhVmRm3nxlP-c0P9sVEt0,456
 rec_pangu/models/layers/activation.py,sha256=vXcVU5Cvk-Xmoxzhv6A3sk-zjHY5z1Is488SlBhTjPY,1876
-rec_pangu/models/layers/attention.py,sha256=qHz-I9ZY-cw1fi281j0bgXdSyc7LL37lsYuV3z_gIbw,4605
-rec_pangu/models/layers/deep.py,sha256=g69NL8HMct1gO04yNJc4QD9gOHpBZNHJ8za-UX-cYzg,5678
-rec_pangu/models/layers/embedding.py,sha256=tmjmvbUjQNgBxP7HQUYLQfo85mU--E2-ytgHHI1k-f0,4557
-rec_pangu/models/layers/graph.py,sha256=sbSE4kJiOCIbRM_gWJcLH1w2k7oE3DvRsTzJeFuyPuk,6158
-rec_pangu/models/layers/interaction.py,sha256=lKsZ5pDobtuaNPA1n0LZV8QW70wQWQ2yZNkNEEhoeGY,11626
-rec_pangu/models/layers/multi_interest.py,sha256=zm7fUTIyVpSrGRSwJvQ71VBWhYS36g7FLB4tUKaRkhI,15731
-rec_pangu/models/layers/sequence.py,sha256=AmXo0Fbi2VFcveuMjrpAovMH4VTR5W8C1d-95DsFaRk,1088
-rec_pangu/models/layers/shallow.py,sha256=H1KJgJP9cDCeECRv2O8GgZa-HNxfVwGnJ12pQmUiRbU,851
+rec_pangu/models/layers/attention.py,sha256=Jp1XOZfXrScOfwCf8OkH3OXwGKoYYlK2EcnTBuII9BE,4606
+rec_pangu/models/layers/conv.py,sha256=qDdETiJLruwQuG9vOTpscWjwbZjjqn_xCMGywg08axU,7468
+rec_pangu/models/layers/deep.py,sha256=Oin-CzDL8wMb34ATQpBZ6_o1VHxPA7RuePPCSa231Ks,3943
+rec_pangu/models/layers/embedding.py,sha256=dxii_wTx7SxaJzDgmnvihpbYhIkU6Uog2xHQ688R2BE,2932
+rec_pangu/models/layers/graph.py,sha256=SMYmK87qrxNBWF-Zo3wYOPynnNNsEqq0Bjljc4Y_CY8,6164
+rec_pangu/models/layers/interaction.py,sha256=xJcrDH-cLRyVCTlhNJA-3KWUJEicwm1QEVyRZIelYN0,12947
+rec_pangu/models/layers/multi_interest.py,sha256=ueMpO6UC2OrbbN48WThokIdJ2FXKaTEU9FRq-lrJAZk,8463
+rec_pangu/models/layers/sequence.py,sha256=8YtL2WblpZkrjSHdiTIxQQVRZ7KUFfhkQVCk4NU8VwU,5266
+rec_pangu/models/layers/shallow.py,sha256=vrV2jZNjbzOT92aSnkIXkHoZendrh2i5Yn5hLWd_lIE,852
+rec_pangu/models/layers/trainformer.py,sha256=5W9A-QOuijjXdSD42rHuyOwtjziUW29OO2FslK8k3lo,9674
 rec_pangu/models/multi_task/__init__.py,sha256=0r9Q2nDklTxGFgD52RgkZIu-jV3y6ojH1Chxw5nZjpo,272
-rec_pangu/models/multi_task/aitm.py,sha256=rjEh74nPPrSkdWgq0vCtqYMJnf3gLoIm8KgeDyIIdyA,3946
+rec_pangu/models/multi_task/aitm.py,sha256=cvAz7lOxbA2Z9wJ6zektYlBmmbv_IonCeZF0Og0SWPE,3948
 rec_pangu/models/multi_task/essm.py,sha256=6_eYZ7zRREmoRgCFrdrauRFqpjor_YrnTGAeyBldpu4,2609
 rec_pangu/models/multi_task/mlmmoe.py,sha256=l_xtuGG2qAlqOrvsPbN3ptX-IGzibCh8rTQymBnqnT4,6268
 rec_pangu/models/multi_task/mmoe.py,sha256=KLm5iEvejz64xLzCjY2B7Tj1oKUR8t18s_p2GC83Hb4,5723
 rec_pangu/models/multi_task/omoe.py,sha256=3xDHlRu1Y0CBpqGGe2j_5HTvHcgRcz20voaa6lOnXdQ,4599
 rec_pangu/models/multi_task/sharebottom.py,sha256=rCVfj2GgoRvpbgQw5g3o4l4Nzx1E2wCZx69Dkpa6jkw,4105
-rec_pangu/models/ranking/__init__.py,sha256=Un9VcLC6CeRfai0hgt9oalYhdQWBFcVJtXd6D_p1Xwg,423
-rec_pangu/models/ranking/afm.py,sha256=VMCwbJjWawrCYeSWTGcQNcCpgbnwN_MRzTwgVm18Qlw,2581
-rec_pangu/models/ranking/afn.py,sha256=DmSTWbd2tJWhdi6nYgO485q63GKpNHxpbEHKfFe4QfA,4447
-rec_pangu/models/ranking/aoanet.py,sha256=M3ZMY2SNw9qfpcf_F4J7JrblU-_GsxE6ONOMQRRThJY,5219
-rec_pangu/models/ranking/autoint.py,sha256=QcHIfeAX_U64cxX2rSJC0h5V4kRNYXfrxY6lBL8SM0I,3663
-rec_pangu/models/ranking/ccpm.py,sha256=o7nJK7KXwRUKsSWco2uNhRoToqdig9no6TWvmP9mEFg,4392
+rec_pangu/models/ranking/__init__.py,sha256=mgaEFoyKmdRRfzc89rPGaMQp4ReMKuWRA1BMyyJtN68,452
+rec_pangu/models/ranking/afm.py,sha256=xuHAAb3X-vWfarsdPV4yYPbtEzOmwieqVQqPew332Ck,2590
+rec_pangu/models/ranking/afn.py,sha256=3Z7un8FsZnW29Qjq6aOXbG9EfncbF5KYnO7DkX214_k,4454
+rec_pangu/models/ranking/aoanet.py,sha256=jB0bHfcP_G1nbnm_iSv_QsVUg-FQCQJJ653RlUuofXc,5225
+rec_pangu/models/ranking/autoint.py,sha256=TkCFz3NwmCix7TcMdvwUmVbpxQE69Oks2gbTaPSQo-k,3669
+rec_pangu/models/ranking/ccpm.py,sha256=tZXN79xlB99itzyVfiiQgEKU3FSJxY_F5no-nKw-o88,4402
 rec_pangu/models/ranking/dcn.py,sha256=xJldzOrmzL_mm6_X8Zp7a2zvrW5pSdLj7IWxK9s2xYU,2604
-rec_pangu/models/ranking/deepfm.py,sha256=g7DLSLLKipI6ELhmjvVFRq7NuLUbkZZjju_TEzhl23A,2588
+rec_pangu/models/ranking/deepfm.py,sha256=5628mi3C_5ZZbuAp7m3jmD1dXbMqxiSSld6qQ7vJGAg,2593
 rec_pangu/models/ranking/fibinet.py,sha256=xbOEV5D1cR3NICbbZ7IsjdkdS9V_7XbdrTNj51uPvm8,3146
 rec_pangu/models/ranking/fm.py,sha256=-TDvov-WYN4lAHj9d5JIm7f4ueb0q0pN6cgsEtStMUI,1894
 rec_pangu/models/ranking/lightgcn.py,sha256=zjv5b7N95UxuZpFtzGTYg2IvoecS0Lkrim1-reE45QA,1920
 rec_pangu/models/ranking/lr.py,sha256=vBD4bOFc8pPHbN5nfN0ACU_k_fuwz36EmxjBYw6YuFg,1668
+rec_pangu/models/ranking/masknet.py,sha256=7hXGUhJLuPK2mZ1aeTtofdnyrkYzQiTHAmJrw6QV7sM,3797
 rec_pangu/models/ranking/nfm.py,sha256=4XoPxhhFkPWBofhqYlweClAXoQtrkgyf-CFOkgPYoIk,2875
 rec_pangu/models/ranking/wdl.py,sha256=it-m0RA3vWAYAv2dVlEkkpNHgJzW5BdVx7x8eQg_4Ao,2785
 rec_pangu/models/ranking/xdeepfm.py,sha256=Y72SLHIGUfXQbjdPUrmWkuCGOYAofm03mjxxo70hirU,3162
-rec_pangu/models/sequence/__init__.py,sha256=UAKVX_eQOQEG-5wNFOhzKARCU6uIGO3kM-t2woSSzHI,301
-rec_pangu/models/sequence/cmi.py,sha256=Ru0b5_CzYkClg2WnKN72hD3QvoFyn4bBq5MzCwK03T4,9004
-rec_pangu/models/sequence/comirec.py,sha256=ZvsJSrOACabcdphmAbObUxI5Wl_cyyU_8s-y3mrML6c,4755
-rec_pangu/models/sequence/mind.py,sha256=zWhrQi3ddb6AvQVeJMg19mSAN8-4rhzUG-4bX9F1e0E,2421
-rec_pangu/models/sequence/narm.py,sha256=R-z9R5q-ieS8szpdcLBcQhbumUNA1XXDYZxj1ox856A,2974
-rec_pangu/models/sequence/re4.py,sha256=HnzkCkOJQgkqX6foKbipX1-rPX9H5F3h5SMiMstn2S4,8049
-rec_pangu/models/sequence/sasrec.py,sha256=d2TnJHmHBF6xnBANcXNCE8TSS9spHO06pwY9PU8o7OY,113
-rec_pangu/models/sequence/srgnn.py,sha256=QTmUpWazh_jKtFkb1uenvCGIdHFDan3LGuM9trShHV8,3013
-rec_pangu/models/sequence/yotubednn.py,sha256=9uckfZN4kIImh0rshlM_xO7weeNtz6deLg3w-EcKCQM,1579
+rec_pangu/models/sequence/__init__.py,sha256=qvxm6FfhrkPmNnhPDKfMDyFLhak9mdjnVbE8ZDvKQvg,437
+rec_pangu/models/sequence/cmi.py,sha256=pyXRK02pY0Do2qPaXNhpg7OKsasCPRg_daP2TO8iGEw,9009
+rec_pangu/models/sequence/comirec.py,sha256=-wsYrhc2BlKiqlnGEymo7dG8tyoLwcEzpDE05pns2hI,4764
+rec_pangu/models/sequence/gcsan.py,sha256=_S85-gEEfeE-spWU1N2DetFKMOJzpf0iu9cTdODPiLk,3807
+rec_pangu/models/sequence/mind.py,sha256=6R_2HuKi5Y3n390GZnh72igbL-fodYqTyOqiHFtVENg,2425
+rec_pangu/models/sequence/narm.py,sha256=EjCKgo39qlFHxCgS4V87GHljxGnAbNWXY84Az3iORRc,2977
+rec_pangu/models/sequence/nextitnet.py,sha256=EW-1XkUXGoLB47MJ1-qKLYn0MbB8ueJQfwSkAbj_l7I,2146
+rec_pangu/models/sequence/niser.py,sha256=F7mRUZpDnbxus6ajMdZMgs7-dRW7uD4iEgbDHZyh9mU,3601
+rec_pangu/models/sequence/re4.py,sha256=YwXzssDsnqTuBn2A8ryYNzdQpPGGZ5XKyW1GxeEM_7M,8056
+rec_pangu/models/sequence/sasrec.py,sha256=OVfpDAXzgPRC71IrPySql4sTCXTO8O3rkB1VNYD1Di0,2780
+rec_pangu/models/sequence/srgnn.py,sha256=mowlvjcDwDTyoI6cTs4NyD48Cg61VpzLPCdn1FwG1Vs,3014
+rec_pangu/models/sequence/stamp.py,sha256=xCVDD4pD1KcUlhK92trpZCYmUlHqj6484i9BPEhJDy8,1779
+rec_pangu/models/sequence/yotubednn.py,sha256=44ZDo-Q9WdLeUJ7fixdR8_yeX1rULcSFbgJJtNcROCc,1587
+rec_pangu/serving/__init__.py,sha256=twv9kol32GZD16l-6Xu5hiaYDHK72UIA1GGJmwcJi2A,115
+rec_pangu/serving/ranking_server.py,sha256=1ZoxUeJyCFlBxebvPDrLH_LiSJK8YDg0D9AI5ejDDIo,2152
 rec_pangu/utils/__init__.py,sha256=dF7uxiBraZf82Do_VlkQqicD4WsNcguSgebsO6dyyag,301
-rec_pangu/utils/check_version.py,sha256=9f3WV-hGFZzQLAc5Aw1UPfy9QB8-QsrEDDMtJASSJcg,1453
-rec_pangu/utils/evaluate.py,sha256=FPUqbri3B4OO0EKEPCIpsN_gjUPmisBeeKt0YiWG1gQ,8700
+rec_pangu/utils/check_version.py,sha256=ONSdf0hw-hPTHjJpHldDz5HdlNHo4kqrm2icjDPkUtg,1509
+rec_pangu/utils/evaluate.py,sha256=-OabWaxaItSLheTzMV3vir05J0yJg6cAr6NPmaKiPFQ,8756
 rec_pangu/utils/gpu_utils.py,sha256=V5-yE_XaHjGELIElhr5J9MWNTeaJS1HiA37CwF3tvrM,1565
-rec_pangu/utils/json_utils.py,sha256=KoBDH8g0vFoyIBOFZU49ZJDtRAw7wbMd58X-XSNwAlw,897
-rec_pangu-0.3.4.dist-info/LICENSE,sha256=5t15gkJb51GDxbB-lVcG2ZMTYLHbccXypC-X_GK-kPw,1058
-rec_pangu-0.3.4.dist-info/METADATA,sha256=a3IbNFjOUj5AySKc8MPxGilA8f5ep_U3gokD5VlpveA,12483
-rec_pangu-0.3.4.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-rec_pangu-0.3.4.dist-info/top_level.txt,sha256=VQeGX3ukDrCpoYzPshLjJWRzJ6lzbymbmBqfQQCUOYE,10
-rec_pangu-0.3.4.dist-info/RECORD,,
+rec_pangu/utils/json_utils.py,sha256=2NbJgYTDACZXQIX11h5U4l0L5fHwyXNVdVksgWwiccI,668
+rec_pangu-0.3.5.dist-info/LICENSE,sha256=5t15gkJb51GDxbB-lVcG2ZMTYLHbccXypC-X_GK-kPw,1058
+rec_pangu-0.3.5.dist-info/METADATA,sha256=ahwTTgIfOSzUehUV4JWMw5C9GmsxsUSDGgTEcgijNSM,14306
+rec_pangu-0.3.5.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+rec_pangu-0.3.5.dist-info/top_level.txt,sha256=VQeGX3ukDrCpoYzPshLjJWRzJ6lzbymbmBqfQQCUOYE,10
+rec_pangu-0.3.5.dist-info/RECORD,,
```

