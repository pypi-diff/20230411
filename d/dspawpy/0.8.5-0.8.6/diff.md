# Comparing `tmp/dspawpy-0.8.5-py3-none-any.whl.zip` & `tmp/dspawpy-0.8.6-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,22 +1,22 @@
-Zip file size: 61331 bytes, number of entries: 20
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-11 06:25 dspawpy/__init__.py
--rw-r--r--  2.0 unx    26618 b- defN 23-Apr-04 10:00 dspawpy/plot.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-02 06:23 dspawpy/analysis/__init__.py
--rw-r--r--  2.0 unx    44264 b- defN 23-Apr-04 08:12 dspawpy/analysis/aimdtools.py
--rw-r--r--  2.0 unx    19597 b- defN 23-Apr-04 05:35 dspawpy/analysis/vacf.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 05:35 dspawpy/diffusion/__init__.py
--rw-r--r--  2.0 unx     9279 b- defN 23-Apr-04 08:12 dspawpy/diffusion/neb.py
--rw-r--r--  2.0 unx    46056 b- defN 23-Apr-04 12:56 dspawpy/diffusion/nebtools.py
--rw-r--r--  2.0 unx    10682 b- defN 23-Apr-04 05:35 dspawpy/diffusion/pathfinder.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 08:21 dspawpy/io/__init__.py
--rw-r--r--  2.0 unx    21023 b- defN 23-Apr-04 11:29 dspawpy/io/read.py
--rw-r--r--  2.0 unx     7677 b- defN 23-Apr-04 08:12 dspawpy/io/read_json.py
--rw-r--r--  2.0 unx    10030 b- defN 23-Apr-04 08:12 dspawpy/io/structure.py
--rw-r--r--  2.0 unx     9017 b- defN 23-Apr-04 08:12 dspawpy/io/utils.py
--rw-r--r--  2.0 unx    10741 b- defN 23-Apr-04 09:20 dspawpy/io/write.py
--rw-r--r--  2.0 unx     1739 b- defN 23-Apr-04 08:12 dspawpy/io/write_json.py
--rw-r--r--  2.0 unx      724 b- defN 23-Apr-04 12:57 dspawpy-0.8.5.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-04 12:57 dspawpy-0.8.5.dist-info/WHEEL
--rw-r--r--  2.0 unx        8 b- defN 23-Apr-04 12:57 dspawpy-0.8.5.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1580 b- defN 23-Apr-04 12:57 dspawpy-0.8.5.dist-info/RECORD
-20 files, 219127 bytes uncompressed, 58781 bytes compressed:  73.2%
+Zip file size: 61941 bytes, number of entries: 20
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-04 06:32 dspawpy/__init__.py
+-rw-rw-rw-  2.0 fat    27378 b- defN 23-Apr-06 03:08 dspawpy/plot.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-04 06:32 dspawpy/analysis/__init__.py
+-rw-rw-rw-  2.0 fat    47068 b- defN 23-Apr-11 01:30 dspawpy/analysis/aimdtools.py
+-rw-rw-rw-  2.0 fat    20152 b- defN 23-Apr-04 06:32 dspawpy/analysis/vacf.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-04 06:32 dspawpy/diffusion/__init__.py
+-rw-rw-rw-  2.0 fat     9574 b- defN 23-Apr-04 06:32 dspawpy/diffusion/neb.py
+-rw-rw-rw-  2.0 fat    47341 b- defN 23-Apr-06 03:08 dspawpy/diffusion/nebtools.py
+-rw-rw-rw-  2.0 fat    10966 b- defN 23-Apr-04 06:32 dspawpy/diffusion/pathfinder.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-04 08:25 dspawpy/io/__init__.py
+-rw-rw-rw-  2.0 fat    21646 b- defN 23-Apr-04 06:32 dspawpy/io/read.py
+-rw-rw-rw-  2.0 fat     7910 b- defN 23-Apr-04 06:32 dspawpy/io/read_json.py
+-rw-rw-rw-  2.0 fat    10331 b- defN 23-Apr-04 06:32 dspawpy/io/structure.py
+-rw-rw-rw-  2.0 fat     9407 b- defN 23-Apr-07 09:23 dspawpy/io/utils.py
+-rw-rw-rw-  2.0 fat    11049 b- defN 23-Apr-04 09:19 dspawpy/io/write.py
+-rw-rw-rw-  2.0 fat     1794 b- defN 23-Apr-04 06:32 dspawpy/io/write_json.py
+-rw-rw-rw-  2.0 fat      754 b- defN 23-Apr-11 01:32 dspawpy-0.8.6.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-11 01:32 dspawpy-0.8.6.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 23-Apr-11 01:32 dspawpy-0.8.6.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     1580 b- defN 23-Apr-11 01:32 dspawpy-0.8.6.dist-info/RECORD
+20 files, 227050 bytes uncompressed, 59391 bytes compressed:  73.8%
```

## zipnote {}

```diff
@@ -42,20 +42,20 @@
 
 Filename: dspawpy/io/write.py
 Comment: 
 
 Filename: dspawpy/io/write_json.py
 Comment: 
 
-Filename: dspawpy-0.8.5.dist-info/METADATA
+Filename: dspawpy-0.8.6.dist-info/METADATA
 Comment: 
 
-Filename: dspawpy-0.8.5.dist-info/WHEEL
+Filename: dspawpy-0.8.6.dist-info/WHEEL
 Comment: 
 
-Filename: dspawpy-0.8.5.dist-info/top_level.txt
+Filename: dspawpy-0.8.6.dist-info/top_level.txt
 Comment: 
 
-Filename: dspawpy-0.8.5.dist-info/RECORD
+Filename: dspawpy-0.8.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dspawpy/plot.py

 * *Ordering differences only*

```diff
@@ -1,760 +1,760 @@
-import os
-import h5py
-import json
-import numpy as np
-import pandas as pd
-import statsmodels.api as sm
-import matplotlib.pyplot as plt
-from scipy.interpolate import interp1d
-from dspawpy.io.read import load_h5
-
-
-def average_along_axis(
-    datafile = "potential.h5",
-    task: str = "potential",
-    axis=2,
-    smooth=False,
-    smooth_frac=0.8,
-    raw=False,
-    **kwargs,
-):
-    """绘制沿着某个轴向的物理量平均值曲线
-
-    Parameters
-    ----------
-    datafile : str or list or np.ndarray
-        h5或json文件路径或包含任意这些文件的文件夹，默认 'potential.h5'
-    task: str
-        任务类型，可以是 'rho', 'potential', 'elf', 'pcharge', 'boundcharge'
-    axis : int
-        沿着哪个轴向绘制势能曲线, 默认2
-    smooth : bool
-        是否平滑, 默认False
-    smooth_frac : float
-        平滑系数, 默认0.8
-    raw : bool
-        是否返回原始数据到csv文件
-    **kwargs : dict
-        其他参数, 传递给 matplotlib.pyplot.plot
-
-    Returns
-    -------
-    axes: matplotlib.axes._subplots.AxesSubplot
-        可传递给其他函数进行进一步处理
-
-    Examples
-    --------
-    >>> from dspawpy.plot import average_along_axis
-    # 读取 potential.h5 文件中的数据，绘图并保存原始绘图数据到csv文件
-    >>> average_along_axis(datafile='potential.h5', task='potential', axis=2, smooth=True, smooth_frac=0.8, raw=True)
-    """
-    assert task.lower() in [
-        "rho",
-        "potential",
-        "elf",
-        "pcharge",
-        "boundcharge",
-    ], "仅支持 rho, potential, elf, pcharge, boundcharge 任务类型"
-    # only for compatibility
-    if isinstance(datafile, list) or isinstance(datafile, np.ndarray):
-        ys = datafile  # expect np.ndarray or list
-
-    # search datafile in the given directory
-    elif os.path.isdir(datafile):
-        directory = datafile  # specified datafile is actually a directory
-        print("您指定了一个文件夹，正在查找相关h5或json文件...")
-        if os.path.exists(os.path.join(directory, f"{task}.h5")):
-            datafile = os.path.join(directory, f"{task}.h5")
-            print("Readingf {task}.h5...")
-        elif os.path.exists(os.path.join(directory, f"{task}.json")):
-            datafile = os.path.join(directory, f"{task}.json")
-            print("Readingf {task}.json...")
-        else:
-            raise FileNotFoundError(f"未找到{task}.h5/{task}.json文件！")
-        
-    # parse the real datafile
-    elif datafile.endswith(".h5"):
-        hfile = datafile
-        hdict = load_h5(hfile)
-        grid = hdict["/AtomInfo/Grid"]
-        # pot = np.asarray(potential["/Potential/TotalElectrostaticPotential"]).reshape(grid, order="F")
-        # DS-PAW 数据写入h5 列优先
-        # h5py 从h5读取数据 默认行优先
-        # np.array(data_list) 默认行优先
-        # 所以这里先以 行优先 把 “h5 行优先 读进来的数据” 转成一维， 再以 列优先 转成 grid 对应的维度
-        if task.lower() == "rho":
-            key = "/Rho/TotalCharge"
-        elif task.lower() == "potential":
-            key = "/Potential/TotalElectrostaticPotential"
-        elif task.lower() == "elf":
-            key = "/ELF/TotalELF"
-        elif task.lower() == "pcharge":
-            key = "/Pcharge/1/TotalCharge"
-        else:
-            key = "/Rho"
-        tmp_pot = np.asarray(hdict[key]).reshape([-1, 1], order="C")
-        ys = tmp_pot.reshape(grid, order="F")
-
-    elif datafile.endswith(".json"):
-        jfile = datafile
-        with open(jfile, "r") as f:
-            jdict = json.load(f)
-        grid = jdict["AtomInfo"]["Grid"]
-
-        if task.lower() == "rho":
-            ys = np.asarray(jdict["Rho"]["TotalCharge"]).reshape(grid, order="F")
-        elif task.lower() == "potential":
-            ys = np.asarray(jdict["Potential"]["TotalElectrostaticPotential"]).reshape(
-                grid, order="F"
-            )
-        elif task.lower() == "elf":
-            ys = np.asarray(jdict["ELF"]["TotalELF"]).reshape(grid, order="F")
-        elif task.lower() == "pcharge":
-            ys = np.asarray(jdict["Pcharge"][0]["TotalCharge"]).reshape(grid, order="F")
-        else:
-            ys = np.asarray(jdict["Rho"]).reshape(grid, order="F")
-
-    else:
-        raise TypeError("仅支持读取h5或json文件或直接传入数组！")
-
-    all_axis = [0, 1, 2]
-    all_axis.remove(axis)
-    y = np.mean(ys, tuple(all_axis))
-    x = np.arange(len(y))
-
-    if raw:
-        pd.DataFrame({"x": x, "y": y}).to_csv(f"raw{task}_axis{axis}.csv", index=False)
-    if smooth:
-        s = sm.nonparametric.lowess(y, x, frac=smooth_frac)
-        if raw:
-            pd.DataFrame({"x": s[:, 0], "y": s[:, 1]}).to_csv(
-                f"raw{task}_axis{axis}_smooth.csv", index=False
-            )
-
-        plt.plot(s[:, 0], s[:, 1], label="macroscopic average", **kwargs)
-
-    plt.plot(x, y, **kwargs)
-
-    return plt
-
-
-def plot_aimd(
-    datafile: str = "aimd.h5",
-    show: bool = True,
-    figName: str = "aimd.png",
-    flags_str="12345",
-    raw=False,
-):
-    """AIMD任务完成后，绘制关键物理量的收敛过程图
-
-    aimd.h5 -> aimd.png
-
-    Parameters
-    ----------
-    datafile : str
-        h5文件位置. 默认 'aimd.h5'
-    show : bool
-        是否展示交互界面. 默认 False
-    figName : str
-        保存的图片路径. 默认 'aimd.h5'
-    flags_str : str
-        子图编号.
-        1. 动能
-        2. 总能
-        3. 压力
-        4. 温度
-        5. 体积
-    raw : bool
-        是否输出原始数据到csv文件
-
-    Returns
-    ----------
-    figName : str
-        图片路径，默认 'aimd.png'
-
-    Examples
-    ----------
-    >>> from dspawpy.plot import plot_aimd
-    # 读取 aimd.h5 文件内容，画出动能、总能、温度、体积的收敛过程图，并保存相应数据到 rawaimd_*.csv 中
-    >>> plot_aimd(datafile='aimd.h5', flags_str='1245', raw=True)
-    """
-    # 处理用户读取，按顺序去重
-    temp = set()
-    flags = [x for x in flags_str if x not in temp and (temp.add(x) or True)]
-    if " " in flags:  # remove space
-        flags.remove(" ")
-
-    for flag in flags:
-        assert flag in ["1", "2", "3", "4", "5"], "读取错误！"
-
-    # 开始画组合图
-    N_figs = len(flags)
-    fig, axes = plt.subplots(N_figs, 1, sharex=True, figsize=(6, 2 * N_figs))
-    if N_figs == 1:  # 'AxesSubplot' object is not subscriptable
-        axes = [axes]  # 避免上述类型错误
-    fig.suptitle("DSPAW AIMD")
-    for i, flag in enumerate(flags):
-        print("正在处理子图" + flag)
-        # 读取数据
-        xs, ys = _read_aimd_converge_data(datafile, flag)
-        if raw:
-            pd.DataFrame({"x": xs, "y": ys}).to_csv(f"rawaimd_{flag}.csv", index=False)
-
-        axes[i].plot(xs, ys)  # 绘制坐标点
-        # 子图的y轴标签
-        if flag == "1":
-            axes[i].set_ylabel("Kinetic Energy (eV)")
-        elif flag == "2":
-            axes[i].set_ylabel("Energy (eV)")
-        elif flag == "3":
-            axes[i].set_ylabel("Pressure Kinetic (kbar)")
-        elif flag == "4":
-            axes[i].set_ylabel("Temperature (K)")
-        else:
-            axes[i].set_ylabel("Volume (Angstrom^3)")
-
-    fig.tight_layout()
-    plt.savefig(figName)
-    if show:
-        plt.show()
-
-    print(f"--> 图片已保存为 {os.path.abspath(figName)}")
-
-
-def plot_bandunfolding(
-    datafile: str = "band.h5", ef=0.0, de=0.05, dele=0.06, raw=False
-):
-    """能带反折叠任务完成后，读取 h5 或 json 文件数据绘图
-
-    band.h5/band.json -> bandunfolding.png
-
-    Parameters
-    ----------
-    datafile : str
-        h5或json文件路径或包含任意这些文件的文件夹，默认 'band.h5'
-    ef : float
-        费米能级，默认置于 y = 0 处
-    de : float
-        能带宽度，默认0.05
-    dele : float
-        能带间隔，默认0.06
-    raw : bool
-        是否输出原始数据到rawbandunfolding.csv
-
-    Returns
-    -------
-    axes: matplotlib.axes._subplots.AxesSubplot
-        可传递给其他函数进行进一步处理
-
-    Examples
-    --------
-    >>> from dspawpy.plot import plot_bandunfolding
-    # 直接绘图
-    >>> plot_bandunfolding("band.h5")
-    # 绘图并保存原始数据到rawbandunfolding.csv
-    >>> plot_bandunfolding("band.h5", raw=True)
-    """
-    # search datafile in the given directory
-    if os.path.isdir(datafile):
-        directory = datafile  # specified datafile is actually a directory
-        print("您指定了一个文件夹，正在查找相关h5或json文件...")
-        if os.path.exists(os.path.join(directory, "band.h5")):
-            datafile = os.path.join(directory, "band.h5")
-            print("Reading band.h5...")
-        elif os.path.exists(os.path.join(directory, "band.json")):
-            datafile = os.path.join(directory, "band.json")
-            print("Reading band.json...")
-        else:
-            raise FileNotFoundError("未找到band.h5/band.json文件！")
-
-    if datafile.endswith(".h5"):
-        band = load_h5(datafile)
-        number_of_band = band["/BandInfo/NumberOfBand"][0]
-        number_of_kpoints = band["/BandInfo/NumberOfKpoints"][0]
-        data = band["/UnfoldingBandInfo/Spin1/UnfoldingBand"]
-        weight = band["/UnfoldingBandInfo/Spin1/Weight"]
-    elif datafile.endswith(".json"):
-        with open(datafile, "r") as f:
-            band = json.load(f)
-        number_of_band = band["BandInfo"]["NumberOfBand"]
-        number_of_kpoints = band["BandInfo"]["NumberOfKpoints"]
-        data = band["UnfoldingBandInfo"]["Spin1"]["UnfoldingBand"]
-        weight = band["UnfoldingBandInfo"]["Spin1"]["Weight"]
-    else:
-        raise TypeError("仅支持读取h5或json文件！")
-
-    celtot = np.array(data).reshape((number_of_kpoints, number_of_band)).T
-    proj_wt = np.array(weight).reshape((number_of_kpoints, number_of_band)).T
-    X2, Y2, Z2, emin = getEwtData(
-        number_of_kpoints, number_of_band, celtot, proj_wt, ef, de, dele
-    )
-
-    if raw:
-        pd.DataFrame({"Y": Y2, "Z": Z2}, index=X2).to_csv(
-            "rawbandunfolding.csv", header=["Y", "color"], index=True, index_label="X"
-        )
-
-    plt.scatter(X2, Y2, c=Z2, cmap="hot")
-    plt.xlim(0, 200)
-    plt.ylim(emin - 0.5, 15)
-    ax = plt.gca()
-    plt.colorbar()
-    ax.set_facecolor("black")
-
-    return plt
-
-
-def plot_optical(
-    datafile: str = "optical.h5",
-    key: str = "AbsorptionCoefficient",
-    index: int = 0,
-    raw=False,
-):
-    """光学性质计算任务完成后，读取数据并绘制预览图
-
-    optical.h5/optical.json -> optical.png
-
-    Parameters
-    ----------
-    datafile : str
-        h5或json文件路径或包含任意这些文件的文件夹，默认 'optical.h5'
-    key: str
-        可选 "AbsorptionCoefficient", "ExtinctionCoefficient", "RefractiveIndex", "Reflectance" 中的任意一个，默认 "AbsorptionCoefficient"
-    index : int
-        序号，默认0
-    raw : bool
-        是否保存原始数据到csv
-
-    Returns
-    -------
-    axes: matplotlib.axes._subplots.AxesSubplot
-        可传递给其他函数进行进一步处理
-
-    Examples
-    --------
-    >>> from dspawpy.plot import plot_optical
-    # 直接绘图
-    >>> plot_optical("optical.h5", "AbsorptionCoefficient", 0)
-    # 绘图并保存原始数据到rawoptical.csv
-    >>> plot_optical("optical.h5", "AbsorptionCoefficient", 0, raw=True)
-    """
-    # search datafile in the given directory
-    if os.path.isdir(datafile):
-        directory = datafile  # specified datafile is actually a directory
-        print("您指定了一个文件夹，正在查找相关h5或json文件...")
-        if os.path.exists(os.path.join(directory, "optical.h5")):
-            datafile = os.path.join(directory, "optical.h5")
-            print("Reading optical.h5...")
-        elif os.path.exists(os.path.join(directory, "optical.json")):
-            datafile = os.path.join(directory, "optical.json")
-            print("Reading optical.json...")
-        else:
-            raise FileNotFoundError("未找到optical.h5/optical.json文件！")
-
-    # parse the real datafile
-    if datafile.endswith("h5"):
-        data_all = load_h5(datafile)
-        energy = data_all["/OpticalInfo/EnergyAxe"]
-        data = data_all["/OpticalInfo/" + key]
-    elif datafile.endswith("json"):
-        with open(datafile, "r") as fin:
-            data_all = json.load(fin)
-        energy = data_all["OpticalInfo"]["EnergyAxe"]
-        data = data_all["OpticalInfo"][key]
-    else:
-        raise TypeError("仅支持读取h5或json文件！")
-
-    data = np.asarray(data).reshape(len(energy), 6)[:, index]
-    inter_f = interp1d(energy, data, kind="cubic")
-    energy_spline = np.linspace(energy[0], energy[-1], 2001)
-    data_spline = inter_f(energy_spline)
-
-    if raw:
-        pd.DataFrame({"energy": energy, "data": data}).to_csv(
-            "rawoptical.csv", index=False
-        )
-        pd.DataFrame(
-            {"energy_spline": energy_spline, "data_spline": data_spline}
-        ).to_csv("rawoptical_spline.csv", index=False)
-
-    plt.plot(energy_spline, data_spline, c="b")
-    plt.xlabel("Photon energy (eV)")
-    plt.ylabel("%s %s" % (key, r"$\alpha (\omega )(cm^{-1})$"))
-
-
-def plot_phonon_thermal(
-    datafile: str = "phonon.h5",
-    figname: str = "phonon.png",
-    show: bool = True,
-    raw=False,
-):
-    """声子热力学计算任务完成后，绘制相关物理量随温度变化曲线
-
-    phonon.h5/phonon.json -> phonon.png
-
-    Parameters
-    ----------
-    datafile : str
-        h5或json文件路径或包含任意这些文件的文件夹，默认 'phonon.h5'
-    figname : str
-        保存图片的文件名
-    show : bool
-        是否弹出交互界面
-    raw : bool
-        是否保存原始数据到rawphonon.csv文件
-
-    Returns
-    ----------
-    figName : str
-        图片路径，默认 'phonon.png'
-
-    Examples
-    --------
-    >>> from dspawpy.plot import plot_phonon_thermal
-    # 保存图片为phonon.png并弹出交互界面
-    >>> plot_phonon_thermal('phonon.h5', figname='phonon.png', show=True)
-    # 也可以给定计算文件夹路径
-    >>> plot_phonon_thermal('my_phonon_task', figname='phonon.png', show=True)
-    """
-    # search datafile in the given directory
-    if os.path.isdir(datafile):
-        directory = datafile  # specified datafile is actually a directory
-        print("您指定了一个文件夹，正在查找相关h5或json文件...")
-        if os.path.exists(os.path.join(directory, "phonon.h5")):
-            datafile = os.path.join(directory, "phonon.h5")
-            print("Reading phonon.h5...")
-        elif os.path.exists(os.path.join(directory, "phonon.json")):
-            datafile = os.path.join(directory, "phonon.json")
-            print("Reading phonon.json...")
-        else:
-            raise FileNotFoundError("未找到phonon.h5/phonon.json文件！")
-
-    if datafile.endswith(".h5"):
-        hfile = datafile
-        ph = h5py.File(hfile, "r")
-        print(f"Reading {hfile}...")
-        temp = np.array(ph["/ThermalInfo/Temperatures"])
-        entropy = np.array(ph["/ThermalInfo/Entropy"])
-        heat_capacity = np.array(ph["/ThermalInfo/HeatCapacity"])
-        helmholts_free_energy = np.array(ph["/ThermalInfo/HelmholtzFreeEnergy"])
-    elif datafile.endswith(".json"):
-        jfile = datafile
-        with open(jfile, "r") as f:
-            data = json.load(f)
-        print(f"Reading {jfile}...")
-        temp = np.array(data["ThermalInfo"]["Temperatures"])
-        entropy = np.array(data["ThermalInfo"]["Entropy"])
-        heat_capacity = np.array(data["ThermalInfo"]["HeatCapacity"])
-        helmholts_free_energy = np.array(data["ThermalInfo"]["HelmholtzFreeEnergy"])
-    else:
-        raise TypeError("仅支持读取h5或json文件！")
-
-    if raw:
-        pd.DataFrame(
-            {
-                "temp": temp,
-                "entropy": entropy,
-                "heat_capacity": heat_capacity,
-                "helmholts_free_energy": helmholts_free_energy,
-            }
-        ).to_csv("rawphonon.csv", index=False)
-
-    plt.plot(temp, entropy, c="red", label="Entropy (J/K/mol)")
-    plt.plot(temp, heat_capacity, c="green", label="Heat Capacity (J/K/mol)")
-    plt.plot(
-        temp, helmholts_free_energy, c="blue", label="Helmholtz Free Energy (kJ/mol)"
-    )
-    plt.xlabel("Temperature(K)")
-    plt.ylabel("Thermal Properties")
-    plt.tick_params(direction="in")  # 刻度线朝内
-    plt.grid(alpha=0.2)
-    plt.legend()
-    plt.title("Thermal")
-    if figname:
-        plt.tight_layout()
-        plt.savefig(figname)
-        print("Figure saved as {}".format(figname))
-
-    if show:
-        plt.show()
-
-
-def plot_polarization_figure(
-    directory: str,
-    repetition: int = 2,
-    annotation: bool = False,
-    annotation_style: int = 1,
-    show: bool = True,
-    fig_name: str = "pol.png",
-    raw=False,
-):
-    """绘制铁电极化结果图
-
-    Parameters
-    ----------
-    directory : str
-        铁电极化计算任务主目录
-    repetition : int
-        沿上（或下）方向重复绘制的次数, 默认 2
-    annotation : bool
-        是否显示首尾构型的铁电极化数值, 默认显示
-    show : bool
-        是否交互显示图片, 默认 True
-    fig_name : str
-        图片保存路径, 默认 'pol.png'
-    raw : bool
-        是否将原始数据保存到csv文件
-
-    Returns
-    -------
-    axes: matplotlib.axes._subplots.AxesSubplot
-        可传递给其他函数进行进一步处理
-
-    Examples
-    --------
-    >>> from dspawpy.plot import plot_polarization_figure
-    >>> plot_polarization_figure(directory='.', fig_name='pol.png')
-    """
-    assert repetition >= 0, "重复次数必须是自然数"
-    subfolders, quantum, totals = _get_subfolders_quantum_totals(directory)
-    number_sfs = [int(sf) for sf in subfolders]
-    fig, axes = plt.subplots(1, 3, sharey=True)
-    xyz = ["x", "y", "z"]
-    for j in range(3):  # x, y, z
-        ys = np.empty(shape=(len(subfolders), repetition * 2 + 1))
-        for r in range(repetition + 1):
-            ys[:, repetition - r] = totals[:, j] - quantum[j] * r
-            ys[:, repetition + r] = totals[:, j] + quantum[j] * r
-
-        axes[j].plot(number_sfs, ys, ".")  # plot
-        axes[j].set_title("P%s" % xyz[j])
-        axes[j].xaxis.set_ticks(number_sfs)  # 设置x轴刻度
-        axes[j].set_xticklabels(labels=subfolders, rotation=90)
-        axes[j].grid(axis="x", color="gray", linestyle=":", linewidth=0.5)
-        axes[j].tick_params(direction="in")
-        # set y ticks using the first and last values
-        if annotation:
-            if annotation_style == 2:
-                style = "arc,angleA=-0,angleB=0,armA=-10,armB=0,rad=0"
-                for i in range(repetition * 2 + 1):
-                    axes[j].annotate(
-                        f"{ys[0,i]:.2f}",
-                        xy=(number_sfs[0], ys[0, i]),
-                        xycoords="data",
-                        xytext=(number_sfs[-1] + 2, ys[0, i] - 8),
-                        textcoords="data",
-                        arrowprops=dict(
-                            arrowstyle="->",
-                            color="black",
-                            linewidth=0.75,
-                            shrinkA=2,
-                            shrinkB=1,
-                            connectionstyle=style,
-                        ),
-                    )
-                    axes[j].annotate(
-                        f"{ys[-1,i]:.2f}",
-                        xy=(number_sfs[-1], ys[-1, i]),
-                        xycoords="data",
-                        xytext=(number_sfs[-1] + 2, ys[-1, i] + 8),
-                        textcoords="data",
-                        arrowprops=dict(
-                            arrowstyle="->",
-                            color="black",
-                            linewidth=0.75,
-                            shrinkA=2,
-                            shrinkB=1,
-                            connectionstyle=style,
-                        ),
-                    )
-            elif annotation_style == 1:
-                for i in range(repetition * 2 + 1):
-                    axes[j].annotate(
-                        text=f"{ys[0,i]:.2f}",
-                        xy=(0, ys[0, i]),
-                        xytext=(0, ys[0, i] - np.max(ys) / repetition / 5),
-                    )
-                    axes[j].annotate(
-                        text=f"{ys[-1,i]:.2f}",
-                        xy=(len(subfolders) - 1, ys[-1, i]),
-                        xytext=(
-                            len(subfolders) - 1,
-                            ys[-1, i] - np.max(ys) / repetition / 5,
-                        ),
-                    )
-            else:
-                raise ValueError("annotation_style must be 1 or 2")
-
-        if raw:
-            pd.DataFrame(ys, index=subfolders).to_csv(f"pol_{xyz[j]}.csv")
-
-    plt.tight_layout()
-    if fig_name:
-        plt.savefig(fig_name)
-        print(f"--> saved to {fig_name}")
-    if show:
-        plt.show()
-
-    return axes
-
-
-def _get_subfolders_quantum_totals(directory: str):
-    """返回铁电极化计算任务的子目录、量子数、极化总量；
-
-    请勿创建其他子目录，否则会被错误读取
-
-    Parameters
-    ----------
-    directory：str
-        铁电极化计算任务主目录
-
-    Returns
-    -------
-    subfolders : list
-        子目录列表
-    quantum : np.ndarray
-        量子数，xyz三个方向, shape=(1, 3)
-    totals : np.ndarray
-        极化总量，xyz三个方向, shape=(len(subfolders), 3)
-
-    Examples
-    --------
-    >>> from dspawpy.plot import get_subfolders_quantum_totals
-    >>> directory = '.' # current directory
-    >>> subfolders, quantum, totals = get_subfolders_quantum_totals(directory)
-    >>> subfolders
-    ['S00', 'S01', 'S06', 'S02', 'S07', 'S03', 'S08', 'S04', 'S10', 'S09', 'S05', 'S11']
-    >>> quantum
-    array([60.06722499, 60.38782135, 62.58443595])
-    >>> totals
-    array([[-4.28508315e-05, -8.71560447e+00, -1.95833103e-06],
-           [-2.67572295e-05,  6.03528774e+00, -3.07857463e-06],
-           [-2.14180047e-05,  5.74655747e+00,  1.54336369e-05],
-           [-6.11371699e-05,  1.92736107e+01, -2.87951455e-05],
-           [ 3.98336164e-05,  1.72479105e+01, -2.75634797e-06],
-           [ 1.20422859e-05, -2.89108722e+01, -1.72015009e-05],
-           [ 5.23329032e-05,  2.89106070e+01, -5.19480945e-06],
-           [-2.86362371e-05, -1.72479598e+01, -3.61790588e-05],
-           [-5.52844691e-05, -6.03482453e+00, -2.71118952e-05],
-           [-3.63338519e-05, -1.92737729e+01, -6.94462350e-06],
-           [ 9.30116520e-06, -5.74668477e+00,  2.91334059e-05],
-           [-4.53814037e-05,  8.71545041e+00,  1.18885825e-06]])
-    """
-
-    subfolders = next(os.walk(directory))[1]
-    if os.path.exists(f"{subfolders[0]}/polarization.json"):
-        # quantum number if constant across the whole calculation,
-        # so, read only once
-        with open(f"{subfolders[0]}/polarization.json", "r") as f:
-            quantum = json.load(f)["PolarizationInfo"]["Quantum"]
-        # the Total number is not constant
-        totals = np.empty(shape=(len(subfolders), 3))
-        for i, fd in enumerate(subfolders):
-            with open("%s/polarization.json" % fd, "r") as f:
-                data = json.load(f)
-            total = data["PolarizationInfo"]["Total"]
-            print("Total: ", total)
-            totals[i] = float(total)
-
-    elif os.path.exists(f"{subfolders[0]}/scf.h5"):
-        # quantum number if constant across the whole calculation,
-        # so, read only once
-        quantum = np.array(
-            h5py.File(f"{subfolders[0]}/scf.h5").get("/PolarizationInfo/Quantum")
-        )
-        # the Total number is not constant
-        totals = np.empty(shape=(len(subfolders), 3))
-        for i, fd in enumerate(subfolders):
-            data = h5py.File("./%s/scf.h5" % fd)
-            total = np.array(data.get("/PolarizationInfo/Total"))
-            totals[i] = total
-    else:
-        raise ValueError("no polarization.json or scf.h5 file found")
-
-    return subfolders, quantum, totals
-
-
-def getEwtData(nk, nb, celtot, proj_wt, ef, de, dele):
-    emin = np.min(celtot) - de
-    emax = np.max(celtot) - de
-
-    emin = np.floor(emin - 0.2)
-    emax = max(np.ceil(emax) * 1.0, 5.0)
-
-    nps = int((emax - emin) / de)
-
-    X = np.zeros((nps + 1, nk))
-    Y = np.zeros((nps + 1, nk))
-
-    X2 = []
-    Y2 = []
-    Z2 = []
-
-    for ik in range(nk):
-        for ip in range(nps + 1):
-            omega = ip * de + emin + ef
-            X[ip][ik] = ik
-            Y[ip][ik] = ip * de + emin
-            ewts_value = 0
-            for ib in range(nb):
-                smearing = dele / np.pi / ((omega - celtot[ib][ik]) ** 2 + dele**2)
-                ewts_value += smearing * proj_wt[ib][ik]
-            if ewts_value > 0.01:
-                X2.append(ik)
-                Y2.append(ip * de + emin)
-                Z2.append(ewts_value)
-
-    Z2_half = max(Z2) / 2
-
-    for i, x in enumerate(Z2):
-        if x > Z2_half:
-            Z2[i] = Z2_half
-
-    return X2, Y2, Z2, emin
-
-
-def _read_aimd_converge_data(datafile: str, index: str = None):
-    """从datafile指定的路径读取index指定的数据，返回绘图用的xs和ys两个数组
-
-    Parameters
-    ----------
-    datafile : str
-        hdf5文件路径
-    index : str
-        编号, 默认 None
-
-    Returns
-    -------
-    xs : np.ndarray
-        x轴数据
-    ys : np.ndarray
-        y轴数据
-    """
-    hf = h5py.File(datafile)  # 加载h5文件
-    Nstep = len(np.array(hf.get("/Structures"))) - 2  # 步数（可能存在未完成的）
-    ys = np.empty(Nstep)  # 准备一个空数组
-
-    # 开始读取
-    if index == "5":
-        for i in range(1, Nstep + 1):
-            ys[i - 1] = np.linalg.det(hf.get("/Structures/Step-%d/Lattice" % i))
-    else:
-        map = {
-            "1": "IonsKineticEnergy",
-            "2": "TotalEnergy0",
-            "3": "PressureKinetic",
-            "4": "Temperature",
-        }
-        for i in range(1, Nstep + 1):
-            # 如果计算中断，则没有PressureKinetic这个键
-            try:
-                ys[i - 1] = np.array(hf.get("/AimdInfo/Step-%d/%s" % (i, map[index])))
-            except:
-                ys[i - 1] = 0
-                ys = np.delete(ys, -1)
-                print(f"-> 计算中断于第{Nstep}步，未读取到该步的压力数据！")
-
-    Nstep = len(ys)  # 步数更新为实际完成的步数
-
-    # 返回xs，ys两个数组
-    return np.linspace(1, Nstep, Nstep), np.array(ys)
+import os
+import h5py
+import json
+import numpy as np
+import pandas as pd
+import statsmodels.api as sm
+import matplotlib.pyplot as plt
+from scipy.interpolate import interp1d
+from dspawpy.io.read import load_h5
+
+
+def average_along_axis(
+    datafile = "potential.h5",
+    task: str = "potential",
+    axis=2,
+    smooth=False,
+    smooth_frac=0.8,
+    raw=False,
+    **kwargs,
+):
+    """绘制沿着某个轴向的物理量平均值曲线
+
+    Parameters
+    ----------
+    datafile : str or list or np.ndarray
+        h5或json文件路径或包含任意这些文件的文件夹，默认 'potential.h5'
+    task: str
+        任务类型，可以是 'rho', 'potential', 'elf', 'pcharge', 'boundcharge'
+    axis : int
+        沿着哪个轴向绘制势能曲线, 默认2
+    smooth : bool
+        是否平滑, 默认False
+    smooth_frac : float
+        平滑系数, 默认0.8
+    raw : bool
+        是否返回原始数据到csv文件
+    **kwargs : dict
+        其他参数, 传递给 matplotlib.pyplot.plot
+
+    Returns
+    -------
+    axes: matplotlib.axes._subplots.AxesSubplot
+        可传递给其他函数进行进一步处理
+
+    Examples
+    --------
+    >>> from dspawpy.plot import average_along_axis
+    # 读取 potential.h5 文件中的数据，绘图并保存原始绘图数据到csv文件
+    >>> average_along_axis(datafile='potential.h5', task='potential', axis=2, smooth=True, smooth_frac=0.8, raw=True)
+    """
+    assert task.lower() in [
+        "rho",
+        "potential",
+        "elf",
+        "pcharge",
+        "boundcharge",
+    ], "仅支持 rho, potential, elf, pcharge, boundcharge 任务类型"
+    # only for compatibility
+    if isinstance(datafile, list) or isinstance(datafile, np.ndarray):
+        ys = datafile  # expect np.ndarray or list
+
+    # search datafile in the given directory
+    elif os.path.isdir(datafile):
+        directory = datafile  # specified datafile is actually a directory
+        print("您指定了一个文件夹，正在查找相关h5或json文件...")
+        if os.path.exists(os.path.join(directory, f"{task}.h5")):
+            datafile = os.path.join(directory, f"{task}.h5")
+            print("Readingf {task}.h5...")
+        elif os.path.exists(os.path.join(directory, f"{task}.json")):
+            datafile = os.path.join(directory, f"{task}.json")
+            print("Readingf {task}.json...")
+        else:
+            raise FileNotFoundError(f"未找到{task}.h5/{task}.json文件！")
+        
+    # parse the real datafile
+    elif datafile.endswith(".h5"):
+        hfile = datafile
+        hdict = load_h5(hfile)
+        grid = hdict["/AtomInfo/Grid"]
+        # pot = np.asarray(potential["/Potential/TotalElectrostaticPotential"]).reshape(grid, order="F")
+        # DS-PAW 数据写入h5 列优先
+        # h5py 从h5读取数据 默认行优先
+        # np.array(data_list) 默认行优先
+        # 所以这里先以 行优先 把 “h5 行优先 读进来的数据” 转成一维， 再以 列优先 转成 grid 对应的维度
+        if task.lower() == "rho":
+            key = "/Rho/TotalCharge"
+        elif task.lower() == "potential":
+            key = "/Potential/TotalElectrostaticPotential"
+        elif task.lower() == "elf":
+            key = "/ELF/TotalELF"
+        elif task.lower() == "pcharge":
+            key = "/Pcharge/1/TotalCharge"
+        else:
+            key = "/Rho"
+        tmp_pot = np.asarray(hdict[key]).reshape([-1, 1], order="C")
+        ys = tmp_pot.reshape(grid, order="F")
+
+    elif datafile.endswith(".json"):
+        jfile = datafile
+        with open(jfile, "r") as f:
+            jdict = json.load(f)
+        grid = jdict["AtomInfo"]["Grid"]
+
+        if task.lower() == "rho":
+            ys = np.asarray(jdict["Rho"]["TotalCharge"]).reshape(grid, order="F")
+        elif task.lower() == "potential":
+            ys = np.asarray(jdict["Potential"]["TotalElectrostaticPotential"]).reshape(
+                grid, order="F"
+            )
+        elif task.lower() == "elf":
+            ys = np.asarray(jdict["ELF"]["TotalELF"]).reshape(grid, order="F")
+        elif task.lower() == "pcharge":
+            ys = np.asarray(jdict["Pcharge"][0]["TotalCharge"]).reshape(grid, order="F")
+        else:
+            ys = np.asarray(jdict["Rho"]).reshape(grid, order="F")
+
+    else:
+        raise TypeError("仅支持读取h5或json文件或直接传入数组！")
+
+    all_axis = [0, 1, 2]
+    all_axis.remove(axis)
+    y = np.mean(ys, tuple(all_axis))
+    x = np.arange(len(y))
+
+    if raw:
+        pd.DataFrame({"x": x, "y": y}).to_csv(f"raw{task}_axis{axis}.csv", index=False)
+    if smooth:
+        s = sm.nonparametric.lowess(y, x, frac=smooth_frac)
+        if raw:
+            pd.DataFrame({"x": s[:, 0], "y": s[:, 1]}).to_csv(
+                f"raw{task}_axis{axis}_smooth.csv", index=False
+            )
+
+        plt.plot(s[:, 0], s[:, 1], label="macroscopic average", **kwargs)
+
+    plt.plot(x, y, **kwargs)
+
+    return plt
+
+
+def plot_aimd(
+    datafile: str = "aimd.h5",
+    show: bool = True,
+    figName: str = "aimd.png",
+    flags_str="12345",
+    raw=False,
+):
+    """AIMD任务完成后，绘制关键物理量的收敛过程图
+
+    aimd.h5 -> aimd.png
+
+    Parameters
+    ----------
+    datafile : str
+        h5文件位置. 默认 'aimd.h5'
+    show : bool
+        是否展示交互界面. 默认 False
+    figName : str
+        保存的图片路径. 默认 'aimd.h5'
+    flags_str : str
+        子图编号.
+        1. 动能
+        2. 总能
+        3. 压力
+        4. 温度
+        5. 体积
+    raw : bool
+        是否输出原始数据到csv文件
+
+    Returns
+    ----------
+    figName : str
+        图片路径，默认 'aimd.png'
+
+    Examples
+    ----------
+    >>> from dspawpy.plot import plot_aimd
+    # 读取 aimd.h5 文件内容，画出动能、总能、温度、体积的收敛过程图，并保存相应数据到 rawaimd_*.csv 中
+    >>> plot_aimd(datafile='aimd.h5', flags_str='1245', raw=True)
+    """
+    # 处理用户读取，按顺序去重
+    temp = set()
+    flags = [x for x in flags_str if x not in temp and (temp.add(x) or True)]
+    if " " in flags:  # remove space
+        flags.remove(" ")
+
+    for flag in flags:
+        assert flag in ["1", "2", "3", "4", "5"], "读取错误！"
+
+    # 开始画组合图
+    N_figs = len(flags)
+    fig, axes = plt.subplots(N_figs, 1, sharex=True, figsize=(6, 2 * N_figs))
+    if N_figs == 1:  # 'AxesSubplot' object is not subscriptable
+        axes = [axes]  # 避免上述类型错误
+    fig.suptitle("DSPAW AIMD")
+    for i, flag in enumerate(flags):
+        print("正在处理子图" + flag)
+        # 读取数据
+        xs, ys = _read_aimd_converge_data(datafile, flag)
+        if raw:
+            pd.DataFrame({"x": xs, "y": ys}).to_csv(f"rawaimd_{flag}.csv", index=False)
+
+        axes[i].plot(xs, ys)  # 绘制坐标点
+        # 子图的y轴标签
+        if flag == "1":
+            axes[i].set_ylabel("Kinetic Energy (eV)")
+        elif flag == "2":
+            axes[i].set_ylabel("Energy (eV)")
+        elif flag == "3":
+            axes[i].set_ylabel("Pressure Kinetic (kbar)")
+        elif flag == "4":
+            axes[i].set_ylabel("Temperature (K)")
+        else:
+            axes[i].set_ylabel("Volume (Angstrom^3)")
+
+    fig.tight_layout()
+    plt.savefig(figName)
+    if show:
+        plt.show()
+
+    print(f"--> 图片已保存为 {os.path.abspath(figName)}")
+
+
+def plot_bandunfolding(
+    datafile: str = "band.h5", ef=0.0, de=0.05, dele=0.06, raw=False
+):
+    """能带反折叠任务完成后，读取 h5 或 json 文件数据绘图
+
+    band.h5/band.json -> bandunfolding.png
+
+    Parameters
+    ----------
+    datafile : str
+        h5或json文件路径或包含任意这些文件的文件夹，默认 'band.h5'
+    ef : float
+        费米能级，默认置于 y = 0 处
+    de : float
+        能带宽度，默认0.05
+    dele : float
+        能带间隔，默认0.06
+    raw : bool
+        是否输出原始数据到rawbandunfolding.csv
+
+    Returns
+    -------
+    axes: matplotlib.axes._subplots.AxesSubplot
+        可传递给其他函数进行进一步处理
+
+    Examples
+    --------
+    >>> from dspawpy.plot import plot_bandunfolding
+    # 直接绘图
+    >>> plot_bandunfolding("band.h5")
+    # 绘图并保存原始数据到rawbandunfolding.csv
+    >>> plot_bandunfolding("band.h5", raw=True)
+    """
+    # search datafile in the given directory
+    if os.path.isdir(datafile):
+        directory = datafile  # specified datafile is actually a directory
+        print("您指定了一个文件夹，正在查找相关h5或json文件...")
+        if os.path.exists(os.path.join(directory, "band.h5")):
+            datafile = os.path.join(directory, "band.h5")
+            print("Reading band.h5...")
+        elif os.path.exists(os.path.join(directory, "band.json")):
+            datafile = os.path.join(directory, "band.json")
+            print("Reading band.json...")
+        else:
+            raise FileNotFoundError("未找到band.h5/band.json文件！")
+
+    if datafile.endswith(".h5"):
+        band = load_h5(datafile)
+        number_of_band = band["/BandInfo/NumberOfBand"][0]
+        number_of_kpoints = band["/BandInfo/NumberOfKpoints"][0]
+        data = band["/UnfoldingBandInfo/Spin1/UnfoldingBand"]
+        weight = band["/UnfoldingBandInfo/Spin1/Weight"]
+    elif datafile.endswith(".json"):
+        with open(datafile, "r") as f:
+            band = json.load(f)
+        number_of_band = band["BandInfo"]["NumberOfBand"]
+        number_of_kpoints = band["BandInfo"]["NumberOfKpoints"]
+        data = band["UnfoldingBandInfo"]["Spin1"]["UnfoldingBand"]
+        weight = band["UnfoldingBandInfo"]["Spin1"]["Weight"]
+    else:
+        raise TypeError("仅支持读取h5或json文件！")
+
+    celtot = np.array(data).reshape((number_of_kpoints, number_of_band)).T
+    proj_wt = np.array(weight).reshape((number_of_kpoints, number_of_band)).T
+    X2, Y2, Z2, emin = getEwtData(
+        number_of_kpoints, number_of_band, celtot, proj_wt, ef, de, dele
+    )
+
+    if raw:
+        pd.DataFrame({"Y": Y2, "Z": Z2}, index=X2).to_csv(
+            "rawbandunfolding.csv", header=["Y", "color"], index=True, index_label="X"
+        )
+
+    plt.scatter(X2, Y2, c=Z2, cmap="hot")
+    plt.xlim(0, 200)
+    plt.ylim(emin - 0.5, 15)
+    ax = plt.gca()
+    plt.colorbar()
+    ax.set_facecolor("black")
+
+    return plt
+
+
+def plot_optical(
+    datafile: str = "optical.h5",
+    key: str = "AbsorptionCoefficient",
+    index: int = 0,
+    raw=False,
+):
+    """光学性质计算任务完成后，读取数据并绘制预览图
+
+    optical.h5/optical.json -> optical.png
+
+    Parameters
+    ----------
+    datafile : str
+        h5或json文件路径或包含任意这些文件的文件夹，默认 'optical.h5'
+    key: str
+        可选 "AbsorptionCoefficient", "ExtinctionCoefficient", "RefractiveIndex", "Reflectance" 中的任意一个，默认 "AbsorptionCoefficient"
+    index : int
+        序号，默认0
+    raw : bool
+        是否保存原始数据到csv
+
+    Returns
+    -------
+    axes: matplotlib.axes._subplots.AxesSubplot
+        可传递给其他函数进行进一步处理
+
+    Examples
+    --------
+    >>> from dspawpy.plot import plot_optical
+    # 直接绘图
+    >>> plot_optical("optical.h5", "AbsorptionCoefficient", 0)
+    # 绘图并保存原始数据到rawoptical.csv
+    >>> plot_optical("optical.h5", "AbsorptionCoefficient", 0, raw=True)
+    """
+    # search datafile in the given directory
+    if os.path.isdir(datafile):
+        directory = datafile  # specified datafile is actually a directory
+        print("您指定了一个文件夹，正在查找相关h5或json文件...")
+        if os.path.exists(os.path.join(directory, "optical.h5")):
+            datafile = os.path.join(directory, "optical.h5")
+            print("Reading optical.h5...")
+        elif os.path.exists(os.path.join(directory, "optical.json")):
+            datafile = os.path.join(directory, "optical.json")
+            print("Reading optical.json...")
+        else:
+            raise FileNotFoundError("未找到optical.h5/optical.json文件！")
+
+    # parse the real datafile
+    if datafile.endswith("h5"):
+        data_all = load_h5(datafile)
+        energy = data_all["/OpticalInfo/EnergyAxe"]
+        data = data_all["/OpticalInfo/" + key]
+    elif datafile.endswith("json"):
+        with open(datafile, "r") as fin:
+            data_all = json.load(fin)
+        energy = data_all["OpticalInfo"]["EnergyAxe"]
+        data = data_all["OpticalInfo"][key]
+    else:
+        raise TypeError("仅支持读取h5或json文件！")
+
+    data = np.asarray(data).reshape(len(energy), 6)[:, index]
+    inter_f = interp1d(energy, data, kind="cubic")
+    energy_spline = np.linspace(energy[0], energy[-1], 2001)
+    data_spline = inter_f(energy_spline)
+
+    if raw:
+        pd.DataFrame({"energy": energy, "data": data}).to_csv(
+            "rawoptical.csv", index=False
+        )
+        pd.DataFrame(
+            {"energy_spline": energy_spline, "data_spline": data_spline}
+        ).to_csv("rawoptical_spline.csv", index=False)
+
+    plt.plot(energy_spline, data_spline, c="b")
+    plt.xlabel("Photon energy (eV)")
+    plt.ylabel("%s %s" % (key, r"$\alpha (\omega )(cm^{-1})$"))
+
+
+def plot_phonon_thermal(
+    datafile: str = "phonon.h5",
+    figname: str = "phonon.png",
+    show: bool = True,
+    raw=False,
+):
+    """声子热力学计算任务完成后，绘制相关物理量随温度变化曲线
+
+    phonon.h5/phonon.json -> phonon.png
+
+    Parameters
+    ----------
+    datafile : str
+        h5或json文件路径或包含任意这些文件的文件夹，默认 'phonon.h5'
+    figname : str
+        保存图片的文件名
+    show : bool
+        是否弹出交互界面
+    raw : bool
+        是否保存原始数据到rawphonon.csv文件
+
+    Returns
+    ----------
+    figName : str
+        图片路径，默认 'phonon.png'
+
+    Examples
+    --------
+    >>> from dspawpy.plot import plot_phonon_thermal
+    # 保存图片为phonon.png并弹出交互界面
+    >>> plot_phonon_thermal('phonon.h5', figname='phonon.png', show=True)
+    # 也可以给定计算文件夹路径
+    >>> plot_phonon_thermal('my_phonon_task', figname='phonon.png', show=True)
+    """
+    # search datafile in the given directory
+    if os.path.isdir(datafile):
+        directory = datafile  # specified datafile is actually a directory
+        print("您指定了一个文件夹，正在查找相关h5或json文件...")
+        if os.path.exists(os.path.join(directory, "phonon.h5")):
+            datafile = os.path.join(directory, "phonon.h5")
+            print("Reading phonon.h5...")
+        elif os.path.exists(os.path.join(directory, "phonon.json")):
+            datafile = os.path.join(directory, "phonon.json")
+            print("Reading phonon.json...")
+        else:
+            raise FileNotFoundError("未找到phonon.h5/phonon.json文件！")
+
+    if datafile.endswith(".h5"):
+        hfile = datafile
+        ph = h5py.File(hfile, "r")
+        print(f"Reading {hfile}...")
+        temp = np.array(ph["/ThermalInfo/Temperatures"])
+        entropy = np.array(ph["/ThermalInfo/Entropy"])
+        heat_capacity = np.array(ph["/ThermalInfo/HeatCapacity"])
+        helmholts_free_energy = np.array(ph["/ThermalInfo/HelmholtzFreeEnergy"])
+    elif datafile.endswith(".json"):
+        jfile = datafile
+        with open(jfile, "r") as f:
+            data = json.load(f)
+        print(f"Reading {jfile}...")
+        temp = np.array(data["ThermalInfo"]["Temperatures"])
+        entropy = np.array(data["ThermalInfo"]["Entropy"])
+        heat_capacity = np.array(data["ThermalInfo"]["HeatCapacity"])
+        helmholts_free_energy = np.array(data["ThermalInfo"]["HelmholtzFreeEnergy"])
+    else:
+        raise TypeError("仅支持读取h5或json文件！")
+
+    if raw:
+        pd.DataFrame(
+            {
+                "temp": temp,
+                "entropy": entropy,
+                "heat_capacity": heat_capacity,
+                "helmholts_free_energy": helmholts_free_energy,
+            }
+        ).to_csv("rawphonon.csv", index=False)
+
+    plt.plot(temp, entropy, c="red", label="Entropy (J/K/mol)")
+    plt.plot(temp, heat_capacity, c="green", label="Heat Capacity (J/K/mol)")
+    plt.plot(
+        temp, helmholts_free_energy, c="blue", label="Helmholtz Free Energy (kJ/mol)"
+    )
+    plt.xlabel("Temperature(K)")
+    plt.ylabel("Thermal Properties")
+    plt.tick_params(direction="in")  # 刻度线朝内
+    plt.grid(alpha=0.2)
+    plt.legend()
+    plt.title("Thermal")
+    if figname:
+        plt.tight_layout()
+        plt.savefig(figname)
+        print("Figure saved as {}".format(figname))
+
+    if show:
+        plt.show()
+
+
+def plot_polarization_figure(
+    directory: str,
+    repetition: int = 2,
+    annotation: bool = False,
+    annotation_style: int = 1,
+    show: bool = True,
+    fig_name: str = "pol.png",
+    raw=False,
+):
+    """绘制铁电极化结果图
+
+    Parameters
+    ----------
+    directory : str
+        铁电极化计算任务主目录
+    repetition : int
+        沿上（或下）方向重复绘制的次数, 默认 2
+    annotation : bool
+        是否显示首尾构型的铁电极化数值, 默认显示
+    show : bool
+        是否交互显示图片, 默认 True
+    fig_name : str
+        图片保存路径, 默认 'pol.png'
+    raw : bool
+        是否将原始数据保存到csv文件
+
+    Returns
+    -------
+    axes: matplotlib.axes._subplots.AxesSubplot
+        可传递给其他函数进行进一步处理
+
+    Examples
+    --------
+    >>> from dspawpy.plot import plot_polarization_figure
+    >>> plot_polarization_figure(directory='.', fig_name='pol.png')
+    """
+    assert repetition >= 0, "重复次数必须是自然数"
+    subfolders, quantum, totals = _get_subfolders_quantum_totals(directory)
+    number_sfs = [int(sf) for sf in subfolders]
+    fig, axes = plt.subplots(1, 3, sharey=True)
+    xyz = ["x", "y", "z"]
+    for j in range(3):  # x, y, z
+        ys = np.empty(shape=(len(subfolders), repetition * 2 + 1))
+        for r in range(repetition + 1):
+            ys[:, repetition - r] = totals[:, j] - quantum[j] * r
+            ys[:, repetition + r] = totals[:, j] + quantum[j] * r
+
+        axes[j].plot(number_sfs, ys, ".")  # plot
+        axes[j].set_title("P%s" % xyz[j])
+        axes[j].xaxis.set_ticks(number_sfs)  # 设置x轴刻度
+        axes[j].set_xticklabels(labels=subfolders, rotation=90)
+        axes[j].grid(axis="x", color="gray", linestyle=":", linewidth=0.5)
+        axes[j].tick_params(direction="in")
+        # set y ticks using the first and last values
+        if annotation:
+            if annotation_style == 2:
+                style = "arc,angleA=-0,angleB=0,armA=-10,armB=0,rad=0"
+                for i in range(repetition * 2 + 1):
+                    axes[j].annotate(
+                        f"{ys[0,i]:.2f}",
+                        xy=(number_sfs[0], ys[0, i]),
+                        xycoords="data",
+                        xytext=(number_sfs[-1] + 2, ys[0, i] - 8),
+                        textcoords="data",
+                        arrowprops=dict(
+                            arrowstyle="->",
+                            color="black",
+                            linewidth=0.75,
+                            shrinkA=2,
+                            shrinkB=1,
+                            connectionstyle=style,
+                        ),
+                    )
+                    axes[j].annotate(
+                        f"{ys[-1,i]:.2f}",
+                        xy=(number_sfs[-1], ys[-1, i]),
+                        xycoords="data",
+                        xytext=(number_sfs[-1] + 2, ys[-1, i] + 8),
+                        textcoords="data",
+                        arrowprops=dict(
+                            arrowstyle="->",
+                            color="black",
+                            linewidth=0.75,
+                            shrinkA=2,
+                            shrinkB=1,
+                            connectionstyle=style,
+                        ),
+                    )
+            elif annotation_style == 1:
+                for i in range(repetition * 2 + 1):
+                    axes[j].annotate(
+                        text=f"{ys[0,i]:.2f}",
+                        xy=(0, ys[0, i]),
+                        xytext=(0, ys[0, i] - np.max(ys) / repetition / 5),
+                    )
+                    axes[j].annotate(
+                        text=f"{ys[-1,i]:.2f}",
+                        xy=(len(subfolders) - 1, ys[-1, i]),
+                        xytext=(
+                            len(subfolders) - 1,
+                            ys[-1, i] - np.max(ys) / repetition / 5,
+                        ),
+                    )
+            else:
+                raise ValueError("annotation_style must be 1 or 2")
+
+        if raw:
+            pd.DataFrame(ys, index=subfolders).to_csv(f"pol_{xyz[j]}.csv")
+
+    plt.tight_layout()
+    if fig_name:
+        plt.savefig(fig_name)
+        print(f"--> saved to {fig_name}")
+    if show:
+        plt.show()
+
+    return axes
+
+
+def _get_subfolders_quantum_totals(directory: str):
+    """返回铁电极化计算任务的子目录、量子数、极化总量；
+
+    请勿创建其他子目录，否则会被错误读取
+
+    Parameters
+    ----------
+    directory：str
+        铁电极化计算任务主目录
+
+    Returns
+    -------
+    subfolders : list
+        子目录列表
+    quantum : np.ndarray
+        量子数，xyz三个方向, shape=(1, 3)
+    totals : np.ndarray
+        极化总量，xyz三个方向, shape=(len(subfolders), 3)
+
+    Examples
+    --------
+    >>> from dspawpy.plot import get_subfolders_quantum_totals
+    >>> directory = '.' # current directory
+    >>> subfolders, quantum, totals = get_subfolders_quantum_totals(directory)
+    >>> subfolders
+    ['S00', 'S01', 'S06', 'S02', 'S07', 'S03', 'S08', 'S04', 'S10', 'S09', 'S05', 'S11']
+    >>> quantum
+    array([60.06722499, 60.38782135, 62.58443595])
+    >>> totals
+    array([[-4.28508315e-05, -8.71560447e+00, -1.95833103e-06],
+           [-2.67572295e-05,  6.03528774e+00, -3.07857463e-06],
+           [-2.14180047e-05,  5.74655747e+00,  1.54336369e-05],
+           [-6.11371699e-05,  1.92736107e+01, -2.87951455e-05],
+           [ 3.98336164e-05,  1.72479105e+01, -2.75634797e-06],
+           [ 1.20422859e-05, -2.89108722e+01, -1.72015009e-05],
+           [ 5.23329032e-05,  2.89106070e+01, -5.19480945e-06],
+           [-2.86362371e-05, -1.72479598e+01, -3.61790588e-05],
+           [-5.52844691e-05, -6.03482453e+00, -2.71118952e-05],
+           [-3.63338519e-05, -1.92737729e+01, -6.94462350e-06],
+           [ 9.30116520e-06, -5.74668477e+00,  2.91334059e-05],
+           [-4.53814037e-05,  8.71545041e+00,  1.18885825e-06]])
+    """
+
+    subfolders = next(os.walk(directory))[1]
+    if os.path.exists(f"{subfolders[0]}/polarization.json"):
+        # quantum number if constant across the whole calculation,
+        # so, read only once
+        with open(f"{subfolders[0]}/polarization.json", "r") as f:
+            quantum = json.load(f)["PolarizationInfo"]["Quantum"]
+        # the Total number is not constant
+        totals = np.empty(shape=(len(subfolders), 3))
+        for i, fd in enumerate(subfolders):
+            with open("%s/polarization.json" % fd, "r") as f:
+                data = json.load(f)
+            total = data["PolarizationInfo"]["Total"]
+            print("Total: ", total)
+            totals[i] = float(total)
+
+    elif os.path.exists(f"{subfolders[0]}/scf.h5"):
+        # quantum number if constant across the whole calculation,
+        # so, read only once
+        quantum = np.array(
+            h5py.File(f"{subfolders[0]}/scf.h5").get("/PolarizationInfo/Quantum")
+        )
+        # the Total number is not constant
+        totals = np.empty(shape=(len(subfolders), 3))
+        for i, fd in enumerate(subfolders):
+            data = h5py.File("./%s/scf.h5" % fd)
+            total = np.array(data.get("/PolarizationInfo/Total"))
+            totals[i] = total
+    else:
+        raise ValueError("no polarization.json or scf.h5 file found")
+
+    return subfolders, quantum, totals
+
+
+def getEwtData(nk, nb, celtot, proj_wt, ef, de, dele):
+    emin = np.min(celtot) - de
+    emax = np.max(celtot) - de
+
+    emin = np.floor(emin - 0.2)
+    emax = max(np.ceil(emax) * 1.0, 5.0)
+
+    nps = int((emax - emin) / de)
+
+    X = np.zeros((nps + 1, nk))
+    Y = np.zeros((nps + 1, nk))
+
+    X2 = []
+    Y2 = []
+    Z2 = []
+
+    for ik in range(nk):
+        for ip in range(nps + 1):
+            omega = ip * de + emin + ef
+            X[ip][ik] = ik
+            Y[ip][ik] = ip * de + emin
+            ewts_value = 0
+            for ib in range(nb):
+                smearing = dele / np.pi / ((omega - celtot[ib][ik]) ** 2 + dele**2)
+                ewts_value += smearing * proj_wt[ib][ik]
+            if ewts_value > 0.01:
+                X2.append(ik)
+                Y2.append(ip * de + emin)
+                Z2.append(ewts_value)
+
+    Z2_half = max(Z2) / 2
+
+    for i, x in enumerate(Z2):
+        if x > Z2_half:
+            Z2[i] = Z2_half
+
+    return X2, Y2, Z2, emin
+
+
+def _read_aimd_converge_data(datafile: str, index: str = None):
+    """从datafile指定的路径读取index指定的数据，返回绘图用的xs和ys两个数组
+
+    Parameters
+    ----------
+    datafile : str
+        hdf5文件路径
+    index : str
+        编号, 默认 None
+
+    Returns
+    -------
+    xs : np.ndarray
+        x轴数据
+    ys : np.ndarray
+        y轴数据
+    """
+    hf = h5py.File(datafile)  # 加载h5文件
+    Nstep = len(np.array(hf.get("/Structures"))) - 2  # 步数（可能存在未完成的）
+    ys = np.empty(Nstep)  # 准备一个空数组
+
+    # 开始读取
+    if index == "5":
+        for i in range(1, Nstep + 1):
+            ys[i - 1] = np.linalg.det(hf.get("/Structures/Step-%d/Lattice" % i))
+    else:
+        map = {
+            "1": "IonsKineticEnergy",
+            "2": "TotalEnergy0",
+            "3": "PressureKinetic",
+            "4": "Temperature",
+        }
+        for i in range(1, Nstep + 1):
+            # 如果计算中断，则没有PressureKinetic这个键
+            try:
+                ys[i - 1] = np.array(hf.get("/AimdInfo/Step-%d/%s" % (i, map[index])))
+            except:
+                ys[i - 1] = 0
+                ys = np.delete(ys, -1)
+                print(f"-> 计算中断于第{Nstep}步，未读取到该步的压力数据！")
+
+    Nstep = len(ys)  # 步数更新为实际完成的步数
+
+    # 返回xs，ys两个数组
+    return np.linspace(1, Nstep, Nstep), np.array(ys)
```

## dspawpy/analysis/aimdtools.py

```diff
@@ -1,1321 +1,1344 @@
-# -*- coding: utf-8 -*-
-import os
-import numpy as np
-from typing import List, Tuple, Union
-from scipy.ndimage import gaussian_filter1d
-from pymatgen.core import Structure
-import matplotlib.pyplot as plt
-
-
-class MSD:
-    # 用于实际计算均方差的类，摘自pymatgen开源项目
-    def __init__(
-        self,
-        structures: List[Structure],
-        select: Union[str, List[int]] = "all",
-        msd_type="xyz",
-    ):
-        self.structures = structures
-        self.msd_type = msd_type
-
-        self.n_frames = len(structures)
-        if select == "all":
-            self.n_particles = len(structures[0])
-        else:
-            self.n_particles = len(select)
-        self.lattice = structures[0].lattice
-
-        self._parse_msd_type()
-
-        self._position_array = np.zeros((self.n_frames, self.n_particles, self.dim_fac))
-
-        if select == "all":
-            for i, s in enumerate(self.structures):
-                self._position_array[i, :, :] = s.frac_coords[:, self._dim]
-        else:
-            for i, s in enumerate(self.structures):
-                self._position_array[i, :, :] = s.frac_coords[select, :][:, self._dim]
-
-    def _parse_msd_type(self):
-        r"""Sets up the desired dimensionality of the MSD."""
-        keys = {
-            "x": [0],
-            "y": [1],
-            "z": [2],
-            "xy": [0, 1],
-            "xz": [0, 2],
-            "yz": [1, 2],
-            "xyz": [0, 1, 2],
-        }
-
-        self.msd_type = self.msd_type.lower()
-
-        try:
-            self._dim = keys[self.msd_type]
-        except KeyError:
-            raise ValueError(
-                "invalid msd_type: {} specified, please specify one of xyz, "
-                "xy, xz, yz, x, y, z".format(self.msd_type)
-            )
-
-        self.dim_fac = len(self._dim)
-
-    def run(self):
-        print('Calculating MSD...')
-        result = np.zeros((self.n_frames, self.n_particles))
-
-        rd = np.zeros((self.n_frames, self.n_particles, self.dim_fac))
-        for i in range(1, self.n_frames):
-            disp = self._position_array[i, :, :] - self._position_array[i - 1, :, :]
-            # mic by periodic boundary condition
-            disp[np.abs(disp) > 0.5] = disp[np.abs(disp) > 0.5] - np.sign(
-                disp[np.abs(disp) > 0.5]
-            )
-            disp = np.dot(disp, self.lattice.matrix)
-            rd[i, :, :] = disp
-        rd = np.cumsum(rd, axis=0)
-        for n in range(1, self.n_frames):
-            disp = rd[n:, :, :] - rd[:-n, :, :]  # [n:-n] window
-            sqdist = np.square(disp).sum(axis=-1)
-            result[n, :] = sqdist.mean(axis=0)
-
-        return result.mean(axis=1)
-
-
-class RDF:
-    # 用于快速计算径向分布函数的类
-    # Copyright (c) Materials Virtual Lab.
-    # Distributed under the terms of the BSD License.
-    def __init__(
-        self,
-        structures: Union[Structure, List[Structure]],
-        rmin: float = 0.0,
-        rmax: float = 10.0,
-        ngrid: float = 101,
-        sigma: float = 0.0,
-    ):
-        """This method calculates rdf on `np.linspace(rmin, rmax, ngrid)` points
-
-        Parameter
-        ---------
-        structures (list of pymatgen Structures): structures to compute RDF
-        rmin (float): minimal radius
-        rmax (float): maximal radius
-        ngrid (int): number of grid points, defaults to 101
-        sigma (float): smooth parameter
-        """
-        if isinstance(structures, Structure):
-            structures = [structures]
-        self.structures = structures
-        # Number of atoms in all structures should be the same
-        assert len({len(i) for i in self.structures}) == 1, "不同构型的原子数不等！"
-        elements = [[i.specie for i in j.sites] for j in self.structures]
-        unique_elements_on_sites = [len(set(i)) == 1 for i in list(zip(*elements))]
-
-        # For the same site index, all structures should have the same element there
-        if not all(unique_elements_on_sites):
-            raise RuntimeError("Elements are not the same at least for one site")
-
-        self.rmin = rmin
-        self.rmax = rmax
-        self.ngrid = ngrid
-
-        self.dr = (self.rmax - self.rmin) / (self.ngrid - 1)  # end points are on grid
-        self.r = np.linspace(self.rmin, self.rmax, self.ngrid)  # type: ignore
-
-        max_r = self.rmax + self.dr / 2.0  # add a small shell to improve robustness
-
-        self.neighbor_lists = [i.get_neighbor_list(max_r) for i in self.structures]
-        # each neighbor list is a tuple of
-        # center_indices, neighbor_indices, image_vectors, distances
-        (
-            self.center_indices,
-            self.neighbor_indices,
-            self.image_vectors,
-            self.distances,  # 完整的距离列表（遍历体系所有原子）
-        ) = list(
-            zip(*self.neighbor_lists)
-        )
-
-        elements = np.array([str(i.specie) for i in structures[0]])  # type: ignore
-        self.center_elements = [elements[i] for i in self.center_indices]
-        self.neighbor_elements = [elements[i] for i in self.neighbor_indices]
-        self.density = [{}] * len(self.structures)
-
-        self.natoms = [
-            i.composition.to_data_dict["unit_cell_composition"]
-            for i in self.structures  # 抽成单胞化学式
-        ]  # {'H': 2, 'O': 1} 字典构成的列表
-
-        for s_index, natoms in enumerate(self.natoms):  # s_index是结构序号，natoms是单胞化学式字典
-            for i, j in natoms.items():  # i是元素符号，j是原子个数
-                self.density[s_index][i] = (
-                    j / self.structures[s_index].volume
-                )  # 原子数除以体积
-
-        self.volumes = 4.0 * np.pi * self.r**2 * self.dr  # 分母的一部分
-        self.volumes[self.volumes < 1e-8] = 1e8  # avoid divide by zero
-        self.n_structures = len(self.structures)
-        self.sigma = np.ceil(sigma / self.dr)
-        # print(elements)
-
-    def _dist_to_counts(self, d):
-        """Convert a distance array for counts in the bin
-
-        Parameter
-        ---------
-            d: (1D np.array)
-
-        Returns:
-            1D array of counts in the bins centered on self.r
-        """
-        # print(len(d))
-        # print(f'{d=}\n')
-        counts = np.zeros((self.ngrid,))
-        indices = np.array(
-            np.floor((d - self.rmin + 0.5 * self.dr) / self.dr), dtype=int
-        )  # 将找到配对的距离转换为格点序号 (向下取整)
-        # print(len(indices))
-        # print(f'{indices=}\n')
-        # 取整操作，导致格点序号很可能重复，因此需要统计每个格点序号出现的次数并去重
-        unique, val_counts = np.unique(indices, return_counts=True)
-        # print(len(unique))
-        # print(f'{unique=}\n')
-        counts[unique] = val_counts
-        # print(f'{counts=}\n')
-        # raise IndexError
-        return counts
-
-    def get_rdf(
-        self,
-        ref_species: Union[str, List[str]],
-        species: Union[str, List[str]],
-        is_average=True,
-    ):
-        """Wrapper to get the rdf for a given species pair
-
-        Parameter
-        ---------
-        ref_species (list of species or just single specie str):
-            The reference species. The rdfs are calculated with these species at the center
-        species (list of species or just single specie str):
-            the species that we are interested in. The rdfs are calculated on these species.
-        is_average (bool):
-            whether to take the average over all structures
-
-        Returns
-        -------
-        (x, rdf)
-            x is the radial points, and rdf is the rdf value.
-        """
-        print('Calculating RDF...')
-        all_rdfs = [
-                self.get_one_rdf(ref_species, species, i)[1]
-                for i in range(self.n_structures)
-            ]
-        if is_average:
-            all_rdfs = np.mean(all_rdfs, axis=0)
-        return self.r, all_rdfs
-
-    def get_one_rdf(
-        self,
-        ref_species: Union[str, List[str]],
-        species: Union[str, List[str]],
-        index=0,
-    ):
-        """Get the RDF for one structure, indicated by the index of the structure
-        in all structures
-
-        Parameter
-        ---------
-        ref_species (list of species or just single specie str):
-            the reference species. The rdfs are calculated with these species at the center
-        species (list of species or just single specie str):
-            the species that we are interested in. The rdfs are calculated on these species.
-        index (int):
-            structure index in the list
-
-        Returns
-        -------
-            (x, rdf) x is the radial points, and rdf is the rdf value.
-        """
-        if isinstance(ref_species, str):
-            ref_species = [ref_species]
-
-        if isinstance(species, str):
-            species = [species]
-        # print(f'{len(self.center_elements[index])=}')
-        indices = (  # 须同时满足下列条件
-            (np.isin(self.center_elements[index], ref_species))
-            & (np.isin(self.neighbor_elements[index], species))
-            & (self.distances[index] >= self.rmin - self.dr / 2.0)
-            & (self.distances[index] <= self.rmax + self.dr / 2.0)
-            & (self.distances[index] > 1e-8)
-        )
-        # print(f'{len(indices)=}')
-        # raise ValueError
-        # print(f'{indices=}\n')
-        density = sum(self.density[index][i] for i in species)  # 目标元素的原子数密度，单浮点数
-        natoms = sum(self.natoms[index][i] for i in ref_species)  # 中心元素的原子总数，单整数
-        distances = self.distances[index][indices]  # 针对每个中心原子，目标元素的距离列表
-        counts = self._dist_to_counts(distances)  # 统计该距离内目标元素的原子数，列表
-        rdf_temp = (
-            counts / density / self.volumes / natoms
-        )  # counts包含了所有中心元素(ref_species)对应的原子的信息，因此需要除以中心原子总数
-        if self.sigma > 1e-8:
-            rdf_temp = gaussian_filter1d(rdf_temp, self.sigma)
-        return self.r, rdf_temp
-
-    def get_coordination_number(self, ref_species, species, is_average=True):
-        """returns running coordination number
-
-        Parameter
-        ---------
-        ref_species (list of species or just single specie str):
-            the reference species. The rdfs are calculated with these species at the center
-        species (list of species or just single specie str):
-            the species that we are interested in. The rdfs are calculated on these species.
-        is_average (bool): whether to take structural average
-
-        Returns
-        --------
-        numpy array
-        """
-        print('Calculating coordination number...')
-        # Note: The average density from all input structures is used here.
-        all_rdf = self.get_rdf(ref_species, species, is_average=False)[1]
-        if isinstance(species, str):
-            species = [species]
-        density = [sum(i[j] for j in species) for i in self.density]
-        cn = [
-            np.cumsum(rdf * density[i] * 4.0 * np.pi * self.r**2 * self.dr)
-            for i, rdf in enumerate(all_rdf)
-        ]
-        if is_average:
-            cn = np.mean(cn, axis=0)
-        return self.r, cn
-
-
-class RMSD:
-    # 用于计算均方差根（Root Mean Square Deviation）的类，摘自pymatgen开源项目
-    def __init__(self, structures: List[Structure]):
-        self.structures = structures
-
-        self.n_frames = len(self.structures)
-        self.n_particles = len(self.structures[0])
-        self.lattice = self.structures[0].lattice
-
-        self._position_array = np.zeros((self.n_frames, self.n_particles, 3))
-
-        for i, s in enumerate(self.structures):
-            self._position_array[i, :, :] = s.frac_coords
-
-    def run(self, base_index=0):
-        print('Calculating RMSD...')
-        result = np.zeros(self.n_frames)
-        rd = np.zeros((self.n_frames, self.n_particles, 3))
-        for i in range(1, self.n_frames):
-            disp = self._position_array[i, :, :] - self._position_array[i - 1, :, :]
-            # mic by periodic boundary condition
-            disp[np.abs(disp) > 0.5] = disp[np.abs(disp) > 0.5] - np.sign(
-                disp[np.abs(disp) > 0.5]
-            )
-            disp = np.dot(disp, self.lattice.matrix)
-            rd[i, :, :] = disp
-        rd = np.cumsum(rd, axis=0)
-
-        for i in range(self.n_frames):
-            sqdist = np.square(rd[i] - rd[base_index]).sum(axis=-1)
-            result[i] = sqdist.mean()
-
-        return np.sqrt(result)
-
-
-def build_Structures_from_datafile(datafile: Union[str, List[str]]) -> List[Structure]:
-    """读取一/多个h5/json文件，返回pymatgen的Structures列表
-
-    Parameters
-    ----------
-    datafile : 字符串或字符串列表
-        aimd.h5/aimd.json文件或包含任意这些文件文件夹；若给定字符串列表，将依次读取数据并合并成一个Structures列表
-
-    Returns
-    -------
-    List[Structure] : pymatgen structures 列表
-
-    Examples
-    --------
-    >>> from dspawpy.analysis.aimdtools import build_Structures_from_datafile
-    # 读取单个文件
-    >>> pymatgen_Structures = build_Structures_from_datafile(datafile='aimd1.h5')
-    # 给定包含aimd.h5或aimd.json文件的文件夹位置
-    >>> pymatgen_Structures = build_Structures_from_datafile(datafile='my_aimd_task')
-    # 当datafile为列表时，将依次读取多个文件，合并成一个Structures列表
-    >>> pymatgen_Structures = build_Structures_from_datafile(datafile=['aimd1.h5','aimd2.h5'])
-    """
-    dfs = []
-    if isinstance(datafile, list):  # 续算模式，给的是多个文件
-        dfs = datafile
-    else:  # 单次计算模式，处理单个文件
-        if os.path.isdir(datafile):
-            print(f"正在查找读取文件夹 {datafile} 中的aimd.h5或aimd.json文件...")
-            if os.path.exists(os.path.join(datafile, "aimd.h5")):
-                df = os.path.join(datafile, "aimd.h5")
-            elif os.path.exists(os.path.join(datafile, "aimd.json")):
-                df = os.path.join(datafile, "aimd.json")
-            else:
-                raise FileNotFoundError("不存在相应的aimd.h5或aimd.json文件！")
-
-        if datafile.endswith(".h5") or datafile.endswith(".json"):
-            df = datafile
-        else:
-            raise FileNotFoundError("未找到aimd.h5或aimd.json文件！")
-        dfs.append(df)
-
-    # 读取结构数据
-    pymatgen_Structures = []
-    for df in dfs:
-        # TODO 支持选取特定帧
-        structure_list = _get_structure_list(df)
-        pymatgen_Structures.extend(structure_list)
-
-    return pymatgen_Structures
-
-
-def get_lagtime_msd(
-    datafile: Union[str, List[str]],
-    select: Union[str, List[int]] = "all",
-    msd_type: str = "xyz",
-    timestep: float = 1.0,
-):
-    """计算不同时间步长下的均方差
-
-    Parameters
-    ----------
-    datafile : str or list of str
-        aimd.h5或aimd.json文件或包含这两个文件之一的文件夹；
-        写成列表的话将依次读取数据并合并到一起
-    select : str or list of int
-        原子序号列表，原子序号从0开始编号；默认为'all'，计算所有原子
-        暂不支持计算多个元素的MSD
-    msd_type : str
-        计算MSD的类型，可选xyz,xy,xz,yz,x,y,z，默认为'xyz'，即计算所有分量
-    timestep : float
-        时间间隔，单位为fs，默认1.0fs
-
-    Returns
-    -------
-    lagtime : np.ndarray
-        时间序列
-    result : np.ndarray
-        均方差序列
-
-    Examples
-    --------
-    >>> from dspawpy.analysis.aimdtools import get_lagtime_msd
-    >>> lagtime, msd = get_lagtime_msd(datafile='aimd.h5', select='all', msd_type='xyz', timestep=1.0)
-    >>> lagtime
-    array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
-            11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
-            ...,
-            990., 991., 992., 993., 994., 995., 996., 997., 998., 999.])
-    >>> msd
-    array([   0.        ,   67.07025573,  132.46384987,  193.1025821 ,
-            250.1513171 ,  301.71988034,  349.76713326,  397.42586668,
-            ...,
-            1092.833737  , 1067.50385434, 1009.90265319, 1206.1645769 ])
-    """
-    strs = build_Structures_from_datafile(datafile)
-
-    msd = MSD(strs, select, msd_type)
-    result = msd.run()
-
-    nframes = msd.n_frames
-    lagtime = np.arange(nframes) * timestep  # make the lag-time axis
-
-    return lagtime, result
-
-
-def get_lagtime_rmsd(datafile: Union[str, List[str]], timestep: float = 1.0):
-    """
-
-    Parameters
-    ----------
-    datafile : str or list of str
-        aimd.h5或aimd.json文件或包含这两个文件之一的文件夹；
-        写成列表的话将依次读取数据并合并到一起
-    timestep : float
-        时间步长，单位fs，默认1fs
-
-    Returns
-    -------
-    lagtime : numpy.ndarray
-        时间序列
-    rmsd : numpy.ndarray
-        均方根序列
-
-    Examples
-    --------
-    >>> from dspawpy.analysis.aimdtools import get_lagtime_rmsd
-    >>> lagtime, rmsd = get_lagtime_rmsd(datafile='aimd.h5', timestep=1.0)
-    >>> lagtime
-    array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
-            11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
-            ...,
-            990., 991., 992., 993., 994., 995., 996., 997., 998., 999.])
-    >>> rmsd
-    array([ 0.        , 19.61783543, 19.62557403, 19.63797614, 19.65407193,
-            27.77329091, 27.7898651 , 19.72260788,  2.34196454,  2.62175006,
-            ...,
-            43.97237636, 39.57388473, 39.67579857, 34.61880282, 34.72988017])
-    """
-    strs = build_Structures_from_datafile(datafile)
-
-    rmsd = RMSD(structures=strs)
-    result = rmsd.run()
-
-    # Plot
-    nframes = rmsd.n_frames
-    lagtime = np.arange(nframes) * timestep  # make the lag-time axis
-
-    return lagtime, result
-
-
-def get_rs_rdfs(
-    datafile: Union[str, List[str]],
-    ele1: str,
-    ele2: str,
-    rmin: float = 0,
-    rmax: float = 10,
-    ngrid: float = 101,
-    sigma: float = 0,
-):
-    """计算rdf分布函数
-
-    Parameters
-    ----------
-    datafile : str or list of str
-        aimd.h5或aimd.json文件路径或包含这两个文件之一的文件夹；
-        写成列表的话将依次读取数据并合并到一起
-    ele1 : list
-        中心元素
-    ele2 : list
-        相邻元素
-    rmin : float
-        径向分布最小值，默认为0
-    rmax : float
-        径向分布最大值，默认为10
-    ngrid : int
-        径向分布网格数，默认为101
-    sigma : float
-        平滑参数
-
-    Returns
-    -------
-    r : numpy.ndarray
-        径向分布网格点
-    rdf : numpy.ndarray
-        径向分布函数
-
-    Examples
-    --------
-    >>> from dspawpy.analysis.aimdtools import get_rs_rdfs
-    >>> rs, rdfs = get_rs_rdfs(datafile='aimd.h5', ele1='H', ele2='O', rmin=0, rmax=10, ngrid=101, sigma=0)
-    >>> rs
-    array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,
-            1.1,  1.2,  1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ,  2.1,
-            ...,
-            9.9, 10. ])
-    >>> rdfs
-    array([0.        , 0.        , 0.        , 0.        , 0.        ,
-           0.        , 0.        , 0.        , 0.        , 0.        ,
-           ...,
-           0.97097276])]
-    """
-    strs = build_Structures_from_datafile(datafile)
-    # print(strs[0]) # check pbc
-    # raise ValueError
-
-    # 计算rdf并绘制主要曲线
-    obj = RDF(
-        structures=strs, rmin=rmin, rmax=rmax, ngrid=ngrid, sigma=sigma
-    )
-
-    rs, rdfs = obj.get_rdf(ele1, ele2)
-    return rs, rdfs
-
-
-def plot_msd(
-    lagtime: np.ndarray,
-    result: np.ndarray,
-    xlim: List[float] = None,
-    ylim: List[float] = None,
-    figname: str = None,
-    show: bool = True,
-    ax=None,
-    **kwargs,
-):
-    """AIMD任务完成后，计算均方差（MSD）
-
-    Parameters
-    ----------
-    lagtime : np.ndarray
-        时间序列
-    result : np.ndarray
-        均方差序列
-    xlim : list of float
-        x轴的范围，默认为None，自动设置
-    ylim : list of float
-        y轴的范围，默认为None，自动设置
-    figname : str
-        图片名称，默认为None，不保存图片
-    show : bool
-        是否显示图片，默认为True
-    ax: matplotlib axes object
-        用于将图片绘制到matplotlib的子图上
-    **kwargs : dict
-        其他参数，如线条宽度、颜色等，传递给plt.plot函数
-
-    Returns
-    -------
-    MSD分析后的图片
-
-    Examples
-    --------
-    >>> from dspawpy.analysis.aimdtools import get_lagtime_msd, plot_msd
-    # 指定h5文件位置，用 get_lagtime_msd 函数获取数据，select 参数选择第n个原子（不是元素）
-    >>> lagtime, msd = get_lagtime_msd('H2O-aimd1.h5', select=[0])
-    # 用获取的数据画图并保存
-    >>> plot_msd(lagtime, msd, figname='MSD.png')
-    """
-    if ax:
-        ishow = False
-        ax.plot(lagtime, result, c="black", ls="-", **kwargs)
-    else:
-        ishow = True
-        fig, ax = plt.subplots()
-        ax.plot(lagtime, result, c="black", ls="-", **kwargs)
-        ax.set_xlabel("Time (fs)")
-        ax.set_ylabel("MSD (Å)")
-
-    if xlim:
-        ax.set_xlim(xlim)
-    if ylim:
-        ax.set_ylim(ylim)
-
-    if figname:
-        plt.savefig(figname)
-        print("MSD图片保存在", os.path.abspath(figname))
-    if ishow and show:  # 画子图的话，不应每个子图都show
-        plt.show()  # show会自动清空图片
-
-    return ax
-
-
-def plot_rdf(
-    rs: np.ndarray,
-    rdfs: np.ndarray,
-    ele1: str,
-    ele2: str,
-    xlim: list = None,
-    ylim: list = None,
-    figname: str = None,
-    show: bool = True,
-    ax: plt.Axes = None,
-    **kwargs,
-):
-    """AIMD计算后分析rdf并画图
-
-    Parameters
-    ----------
-    rs : numpy.ndarray
-        径向分布网格点
-    rdfs : numpy.ndarray
-        径向分布函数
-    ele1 : list
-        中心元素
-    ele2 : list
-        相邻元素
-    xlim : list
-        x轴范围，默认为None，即自动设置
-    ylim : list
-        y轴范围，默认为None，即自动设置
-    figname : str
-        图片名称，默认为None，即不保存图片
-    show : bool
-        是否显示图片，默认为True
-    ax: matplotlib.axes.Axes
-        画图的坐标轴，默认为None，即新建坐标轴
-    **kwargs : dict
-        其他参数，如线条宽度、颜色等，传递给plt.plot函数
-
-    Returns
-    -------
-    rdf分析后的图片
-
-    Examples
-    --------
-    >>> from dspawpy.analysis.aimdtools import get_rs_rdfs, plot_rdf
-    # 先获取rs和rdfs数据作为xy轴数据
-    >>> rs, rdfs = get_rs_rdfs(['LiO-aimd1.h5','LiO-aimd2.h5','LiO-aimd3.h5'], 'Li', 'O', rmax=6)
-    # 将xy轴数据传入plot_rdf函数绘图
-    >>> plot_rdf(rs, rdfs, 'Li','O', xlim=[0,6], ylim=[0,35], color='red')
-    """
-    if ax:
-        ax.plot(
-            rs,
-            rdfs,
-            label=r"$g_{\alpha\beta}(r)$" + f"[{ele1},{ele2}]",
-            **kwargs,
-        )
-
-    else:
-        fig, ax = plt.subplots()
-        ax.plot(
-            rs,
-            rdfs,
-            label=r"$g_{\alpha\beta}(r)$" + f"[{ele1},{ele2}]",
-            **kwargs,
-        )
-
-        ax.set_xlabel(r"$r$" + "(Å)")
-        ax.set_ylabel(r"$g(r)$")
-
-    ax.legend()
-
-    # 绘图细节
-    if xlim:
-        ax.set_xlim(xlim)
-    if ylim:
-        ax.set_ylim(ylim)
-
-    if figname:
-        plt.savefig(figname)
-        print(f"图片已保存到 {os.path.abspath(figname)}")
-    if show:  # 画子图的话，不应每个子图都show
-        plt.show()  # show会自动清空图片
-
-
-def plot_rmsd(
-    lagtime: np.ndarray,
-    result: np.ndarray,
-    xlim: list = None,
-    ylim: list = None,
-    figname: str = None,
-    show: bool = True,
-    ax=None,
-    **kwargs,
-):
-    """AIMD计算后分析rmsd并画图
-
-    Parameters
-    ----------
-    lagtime:
-        时间序列
-    result:
-        均方根序列
-    xlim : list
-        x轴范围
-    ylim : list
-        y轴范围
-    figname : str
-        图片保存路径
-    show : bool
-        是否显示图片
-    ax : matplotlib.axes._subplots.AxesSubplot
-        画子图的话，传入子图对象
-    **kwargs : dict
-        传入plt.plot的参数
-
-    Returns
-    -------
-    rmsd分析结构的图片
-
-    Examples
-    --------
-    >>> from dspawpy.analysis.aimdtools import get_lagtime_rmsd, plot_rmsd
-    # timestep 表示时间步长
-    >>> lagtime, rmsd = get_lagtime_rmsd(datafile='H2O-aimd1.h5', timestep=0.1)
-    # 直接保存为RMSD.png图片
-    >>> plot_rmsd(lagtime, rmsd, figname='RMSD.png', show=True)
-    """
-    # 参数初始化
-    if not ax:
-        ishow = True
-    else:
-        ishow = False
-
-    if ax:
-        ax.plot(lagtime, result, **kwargs)
-    else:
-        fig, ax = plt.subplots()
-        ax.plot(lagtime, result, **kwargs)
-        ax.set_xlabel("Time (fs)")
-        ax.set_ylabel("RMSD (Å)")
-
-    if xlim:
-        ax.set_xlim(xlim)
-    if ylim:
-        ax.set_ylim(ylim)
-
-    if figname:
-        plt.savefig(figname)
-    if show and ishow:  # 画子图的话，不应每个子图都show
-        plt.show()  # show会自动清空图片
-
-    return ax
-
-
-def read_h5(
-    hpath: str,
-    index = None,
-    ele = None,
-    ai = None,
-    return_scaled: bool = False,
-):
-    """从hpath指定的路径读取h5文件中的数据
-
-    Parameters
-    ----------
-    hpath: str
-        h5文件路径
-    index: int or list or str
-        运动轨迹中的第几步，从1开始计数
-        如果要切片，用字符串写法： '1, 10:20, 23'
-    ele: str or list or np.array
-        元素，例如 'C'，'H'，'O'，'N'
-    ai: int or list or np.array
-        原子序号（体系中的第几个原子，不是质子数）
-        如果要切片，用字符串写法： '1, 10:20, 23'
-    return_scaled: bool
-        是否返回原子分数坐标，默认为False
-
-    Return
-    -------
-    Nstep: int
-        离子步总数
-    elements: list
-        元素列表, Natom x 1
-    positions: np.ndarray
-        原子位置,  Nstep x Natom x 3
-    lattices: np.ndarray
-        晶胞, Nstep x 3 x 3
-
-    Examples
-    --------
-    >>> from dspawpy.analysis.aimdtools import read_h5
-    >>> Nstep, elements, positions, lattices = read_h5(hpath='aimd.h5', ele='H', index='1:2')
-    >>> Nstep
-    2
-    >>> elements
-    ['H', 'H']
-    >>> positions
-    array([[[0.56037855, 5.60910012, 0.06341764],
-            [0.57622933, 0.78033174, 6.28639689]],
-           [[0.55354018, 5.60627362, 0.12845834],
-            [0.57012995, 0.80246543, 6.22272366]]])
-    >>> lattices
-    array([[[6.35016, 0.     , 0.     ],
-            [0.     , 6.35016, 0.     ],
-            [0.     , 0.     , 6.35016]],
-           [[6.35016, 0.     , 0.     ],
-            [0.     , 6.35016, 0.     ],
-            [0.     , 0.     , 6.35016]]])
-    """
-    import h5py
-
-    print(f"Reading {os.path.abspath(hpath)} ...")
-    hf = h5py.File(hpath)  # 加载h5文件
-    Total_step = len(np.array(hf.get("/Structures"))) - 2  # 总步数
-
-    if ele and ai:
-        raise ValueError("暂不支持同时指定元素和原子序号")
-    # 步数
-    if index:
-        if isinstance(index, int):  # 1
-            indices = [index]
-
-        elif isinstance(index, list) or isinstance(ai, np.ndarray):  # [1,2,3]
-            indices = index
-
-        elif isinstance(index, str):  # ':', '-3:'
-            indices = _parse_indices(index, Total_step)
-
-        else:
-            raise ValueError("请输入正确格式的index")
-
-        Nstep = len(indices)
-    else:
-        Nstep = Total_step
-        indices = list(range(1, Nstep + 1))
-
-    # 读取元素列表，这个列表不会随步数改变，也不会“合并同类项”
-    from dspawpy.io.utils import get_ele_from_h5
-
-    Elements = np.array(get_ele_from_h5(hpath), dtype="str")
-
-    # 开始读取晶胞和原子位置
-    lattices = np.empty((Nstep, 3, 3))  # Nstep x 3 x 3
-    location = []
-    if ele:  # 如果用户指定元素
-        if isinstance(ele, str):  # 单个元素符号，例如 'Fe'
-            ele_list = np.array(ele, dtype="str")
-            location = np.where(Elements == ele_list)[0]
-        # 多个元素符号组成的列表，例如 ['Fe', 'O']
-        elif isinstance(ele, list) or isinstance(ele, np.ndarray):
-            for e in ele:
-                loc = np.where(Elements == e)[0]
-                location.append(loc)
-            location = np.concatenate(location)
-        else:
-            raise TypeError("请输入正确的元素或元素列表")
-        elements = Elements[location]
-
-    elif ai:  # 如果用户指定原子序号
-        if isinstance(ai, int):  # 1
-            ais = [ai]
-        elif isinstance(ai, list) or isinstance(ai, np.ndarray):  # [1,2,3]
-            ais = ai
-        elif isinstance(ai, str):  # ':', '-3:'
-            ais = _parse_indices(ai, Total_step)
-        else:
-            raise ValueError("请输入正确格式的ai")
-        ais = [i - 1 for i in ais]  # python从0开始计数，但是用户从1开始计数
-        elements = Elements[ais]
-        location = ais
-
-    else:  # 如果都没指定
-        elements = Elements
-        location = list(range(len(Elements)))
-
-    elements = elements.tolist()  # for pretty output
-
-    if return_scaled:
-        scaled_positions = np.empty(shape=(len(indices), len(elements), 3))
-        for i, index in enumerate(indices):  # 步数
-            lats = np.array(hf.get("/Structures/Step-" + str(index) + "/Lattice"))
-            lattices[i] = lats
-            # [x1,y1,z1,x2,y2,z2,x3,y3,z3], ...
-            # 结构优化时输出的都是分数坐标，不管CoordinateType写的是啥！
-            spos = np.array(hf.get("/Structures/Step-" + str(index) + "/Position"))
-            wrapped_spos = spos - np.floor(spos)  # wrap into [0,1)
-            wrapped_spos = wrapped_spos.flatten().reshape(-1, 3).T  # reshape
-            for j, sli in enumerate(location):
-                scaled_positions[i, j, :] = np.dot(wrapped_spos[:, sli], np.eye(3, 3))
-        return Nstep, elements, scaled_positions, lattices
-
-    else:
-        # Nstep x Natom x 3
-        positions = np.empty(shape=(len(indices), len(elements), 3))
-        for i, index in enumerate(indices):  # 步数
-            lats = np.array(hf.get("/Structures/Step-" + str(index) + "/Lattice"))
-            lattices[i] = lats
-            # [x1,y1,z1,x2,y2,z2,x3,y3,z3], ...
-            # 结构优化时输出的都是分数坐标，不管CoordinateType写的是啥！
-            spos = np.array(hf.get("/Structures/Step-" + str(index) + "/Position"))
-            wrapped_spos = spos - np.floor(spos)  # wrap into [0,1)
-            wrapped_spos = wrapped_spos.flatten().reshape(-1, 3).T  # reshape
-            for j, sli in enumerate(location):
-                positions[i, j, :] = np.dot(wrapped_spos[:, sli], lats)
-
-        return Nstep, elements, positions, lattices
-
-
-# 2. 写轨迹文件
-def write_xyz_traj(
-    h5_path: str = "aimd.h5",
-    jpath: str = None,
-    ai=None,
-    ele=None,
-    index=None,
-    xyzfile="aimdTraj.xyz",
-):
-    """保存xyz格式的轨迹文件
-
-    Parameters
-    ----------
-    h5_path : str
-        DSPAW计算完成后保存的h5文件路径
-    jpath : str
-        DSPAW计算完成后保存的json文件路径
-    ai : int
-        原子编号列表（体系中的第几号原子，不是质子数）
-    ele : str
-        元素，例如 'C'，'H'，'O'，'N'
-    index : int
-        优化过程中的第几步
-
-    Returns
-    -------
-    xyzfile: str
-        写入xyz格式的轨迹文件，默认为aimdTraj.xyz
-
-    Example
-    -------
-    >>> from dspawpy.analysis.aimdtools import write_xyz_traj
-    >>> write_xyz_traj(h5_path='aimd.h5', ai=[1,2,3], index=1, xyzfile='aimdTraj.xyz')
-    """
-    if jpath:
-        Nstep, eles, poses, lats = _read_json(jpath, index, ele, ai)
-    else:
-        Nstep, eles, poses, lats = read_h5(h5_path, index, ele, ai)
-    # 写入文件
-    with open(xyzfile, "w") as f:
-        # Nstep
-        for n in range(Nstep):
-            # 原子数不会变，就是不合并的元素总数
-            f.write("%d\n" % len(eles))
-            # lattice
-            f.write(
-                'Lattice="%f %f %f %f %f %f %f %f %f" Properties=species:S:1:pos:R:3 pbc="T T T"\n'
-                % (
-                    lats[n, 0, 0],
-                    lats[n, 0, 1],
-                    lats[n, 0, 2],
-                    lats[n, 1, 0],
-                    lats[n, 1, 1],
-                    lats[n, 1, 2],
-                    lats[n, 2, 0],
-                    lats[n, 2, 1],
-                    lats[n, 2, 2],
-                )
-            )
-            # position and element
-            for i in range(len(eles)):
-                f.write(
-                    "%s %f %f %f\n"
-                    % (eles[i], poses[n, i, 0], poses[n, i, 1], poses[n, i, 2])
-                )
-
-
-def write_dump_traj(
-    h5_path="aimd.h5",
-    jpath=None,
-    ai=None,
-    ele=None,
-    index=None,
-    dumpfile="aimdTraj.dump",
-):
-    """保存为lammps的dump格式的轨迹文件，暂时只支持正交晶胞
-
-    Parameters
-    ----------
-    h5_path : str
-        DSPAW计算完成后保存的h5文件路径
-    jpath : str
-        DSPAW计算完成后保存的json文件路径
-    ai : int
-        原子编号列表（体系中的第几号原子，不是质子数）
-    ele : str
-        元素，例如 'C'，'H'，'O'，'N'
-    index : int
-        优化过程中的第几步
-
-    Returns
-    -------
-    dumpfile: str
-        写入xyz格式的轨迹文件，默认为aimdTraj.xyz
-
-    Example
-    -------
-    >>> from dspawpy.analysis.aimdtools import write_dump_traj
-    >>> write_dump_traj(h5_path='aimd.h5', ai=[1,2,3], index=1, dumpfile='aimdTraj.dump')
-    """
-    if jpath:
-        Nstep, eles, poses, lats = _read_json(jpath, index, ele, ai)
-    else:
-        Nstep, eles, poses, lats = read_h5(h5_path, index, ele, ai)
-
-    # 写入文件
-    with open(dumpfile, "w") as f:
-        for n in range(Nstep):
-            box_bounds = _get_lammps_non_orthogonal_box(lats[n])
-            f.write("ITEM: TIMESTEP\n%d\n" % n)
-            f.write("ITEM: NUMBER OF ATOMS\n%d\n" % (len(eles)))
-            f.write("ITEM: BOX BOUNDS xy xz yz xx yy zz\n")
-            f.write(
-                "%f %f %f\n%f %f %f\n %f %f %f\n"
-                % (
-                    box_bounds[0][0],
-                    box_bounds[0][1],
-                    box_bounds[0][2],
-                    box_bounds[1][0],
-                    box_bounds[1][1],
-                    box_bounds[1][2],
-                    box_bounds[2][0],
-                    box_bounds[2][1],
-                    box_bounds[2][2],
-                )
-            )
-            f.write("ITEM: ATOMS type x y z id\n")
-            for i in range(len(eles)):
-                f.write(
-                    "%s %f %f %f %d\n"
-                    % (eles[i], poses[n, i, 0], poses[n, i, 1], poses[n, i, 2], i + 1)
-                )
-
-
-def _get_structure_list(df: str = "aimd.h5") -> List[Structure]:
-    """get pymatgen structures from single datafile
-
-    Parameters
-    ----------
-    df : str, optional
-        datafile, by default "aimd.h5"
-
-    Returns
-    -------
-    List[Structure] : list of pymatgen structures
-
-    Examples
-    --------
-    >>> from dspawpy.analysis.aimdtools import get_structure_list
-    >>> structure_list = get_structure_list(df='aimd.h5')
-    """
-    if df.endswith(".h5"):
-        # create Structure structure_list from aimd.h5
-        Nstep, elements, positions, lattices = read_h5(df)
-        strs = []
-        for i in range(Nstep):
-            strs.append(
-                Structure(
-                    lattices[i], elements, positions[i], coords_are_cartesian=False
-                )
-            )
-    elif df.endswith(".json"):
-        from dspawpy.io.read import json2structures
-
-        strs = json2structures(df)
-    else:
-        raise ValueError(f"{df} file format not supported")
-
-    return strs
-
-
-def _get_neighbor_list(structure, r) -> Tuple:
-    """Thin wrapper to enable parallel calculations
-
-    Parameter
-    ---------
-    structure (pymatgen Structure): pymatgen structure
-    r (float): cutoff radius
-
-    Returns
-    --------
-    tuple of neighbor list
-    """
-    return structure.get_neighbor_list(r)
-
-
-def _parse_indices(index: str, total_step) -> list:
-    """解析用户输入的原子序号字符串
-
-    输入：
-        - index: 用户输入的原子序号/元素字符串，例如 '1:3,5,7:10'
-    输出：
-        - indices: 解析后的原子序号列表，例如 [1,2,3,4,5,6,7,8,9,10]
-    """
-    assert ":" in index, "如果不想切片索引，请输入整数或者列表"
-    blcs = index.split(",")
-    indices = []
-    for blc in blcs:
-        if ":" in blc:  # 切片
-            low = blc.split(":")[0]
-            if not low:
-                low = 1  # 从1开始
-            else:
-                low = int(low)
-                assert low > 0, "索引从1开始！"
-            high = blc.split(":")[1]
-            if not high:
-                high = total_step
-            else:
-                high = int(high)
-                assert high <= total_step, "索引超出范围！"
-
-            for i in range(low, high + 1):
-                indices.append(i)
-        else:  # 单个数字
-            indices.append(int(blc))
-    return indices
-
-
-def _read_json(
-    jpath: str,
-    index= None,
-    ele = None,
-    ai= None,
-):
-    """从json指定的路径读取数据
-
-    输入:
-    - jpath: json文件路径
-    - ai: 原子序号（体系中的第几个原子，不是质子数）
-    - ele: 元素，例如 'C'，'H'，'O'，'N'
-    - index: 运动轨迹中的第几步，从1开始
-
-    输出：
-    - Nstep: 总共要保存多少步的信息, int
-    - elements: 元素列表, list, Natom x 1
-    - positions: 原子位置, list, Nstep x Natom x 3
-    - lattices: 晶胞, list, Nstep x 3 x 3
-    """
-    import json
-
-    with open(jpath, "r") as f:
-        data = json.load(f)  # 加载json文件
-
-    Total_step = len(data["Structures"])  # 总步数
-
-    if ele and ai:
-        raise ValueError("暂不支持同时指定元素和原子序号")
-    # 步数
-    if index:
-        if isinstance(index, int):  # 1
-            indices = [index]
-
-        elif isinstance(index, list) or isinstance(ai, np.ndarray):  # [1,2,3]
-            indices = index
-
-        elif isinstance(index, str):  # ':', '-3:'
-            indices = _parse_indices(index, Total_step)
-
-        else:
-            raise ValueError("请输入正确格式的index")
-
-        Nstep = len(indices)
-    else:
-        Nstep = Total_step
-        indices = list(range(1, Nstep + 1))  # [1,Nstep+1)
-
-    # 预先读取全部元素的总列表，这个列表不会随步数改变，也不会“合并同类项”
-    # 这样可以避免在循环内部频繁判断元素是否符合用户需要
-
-    Nele = len(data["Structures"][0]["Atoms"])  # 总元素数量
-    total_elements = np.empty(shape=(Nele), dtype="str")  # 未合并的元素列表
-    for i in range(Nele):
-        element = data["Structures"][0]["Atoms"][i]["Element"]
-        total_elements[i] = element
-
-    # 开始读取晶胞和原子位置
-    # 在data['Structures']['%d' % index]['Atoms']中根据元素所在序号选择结构
-    if ele:  # 用户指定要某些元素
-        location = []
-        if isinstance(ele, str):  # 单个元素符号，例如 'Fe'
-            ele_list = list(ele)
-        # 多个元素符号组成的列表，例如 ['Fe', 'O']
-        elif isinstance(ele, list) or isinstance(ele, np.ndarray):
-            ele_list = ele
-        else:
-            raise TypeError("请输入正确的元素或元素列表")
-        for e in ele_list:
-            location.append(np.where(total_elements == e)[0])
-        location = np.concatenate(location)
-
-    elif ai:  # 如果用户指定原子序号，也要据此筛选元素列表
-        if isinstance(ai, int):  # 1
-            ais = [ai]
-        elif isinstance(ai, list) or isinstance(ai, np.ndarray):  # [1,2,3]
-            ais = ai
-        elif isinstance(ai, str):  # ':', '-3:'
-            ais = _parse_indices(ai, Total_step)
-        else:
-            raise ValueError("请输入正确格式的ai")
-        ais = [i - 1 for i in ais]  # python从0开始计数，但是用户从1开始计数
-        location = ais
-        # read lattices and poses
-
-    else:  # 如果都没指定
-        location = list(range(Total_step))
-
-    # 满足用户需要的elements列表
-    elements = np.empty(shape=len(location), dtype="str")
-    for i in range(len(location)):
-        elements[i] = total_elements[location[i]]
-
-    # Nstep x Natom x 3
-    positions = np.empty(shape=(len(indices), len(elements), 3))
-    lattices = np.empty(shape=(Nstep, 3, 3))  # Nstep x 3 x 3
-    for i, index in enumerate(indices):  # 步数
-        lat = data["Structures"][index - 1]["Lattice"]
-        lattices[i] = np.array(lat).reshape(3, 3)
-        for j, sli in enumerate(location):
-            positions[i, j, :] = data["Structures"][index - 1]["Atoms"][sli][
-                "Position"
-            ][:]
-
-    return Nstep, elements, positions, lattices
-
-
-def _get_lammps_non_orthogonal_box(lat: np.ndarray):
-    """计算用于输入lammps的盒子边界参数
-
-    Parameters
-    ----------
-    lat : np.ndarray
-        常见的非三角3x3矩阵
-
-    Returns
-    -------
-    box_bounds:
-        用于输入lammps的盒子边界
-    """
-    # https://docs.lammps.org/Howto_triclinic.html
-    A = lat[0]
-    B = lat[1]
-    C = lat[2]
-    assert np.cross(A, B).dot(C) > 0, "Lat is not right handed"
-
-    # 将常规3x3矩阵转成标准的上三角矩阵
-    alpha = np.arccos(np.dot(B, C) / (np.linalg.norm(B) * np.linalg.norm(C)))
-    beta = np.arccos(np.dot(A, C) / (np.linalg.norm(A) * np.linalg.norm(C)))
-    gamma = np.arccos(np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B)))
-
-    ax = np.linalg.norm(A)
-    a = np.array([ax, 0, 0])
-
-    bx = np.linalg.norm(B) * np.cos(gamma)
-    by = np.linalg.norm(B) * np.sin(gamma)
-    b = np.array([bx, by, 0])
-
-    cx = np.linalg.norm(C) * np.cos(beta)
-    cy = (np.linalg.norm(B) * np.linalg.norm(C) - bx * cx) / by
-    cz = np.sqrt(abs(np.linalg.norm(C) ** 2 - cx**2 - cy**2))
-    c = np.array([cx, cy, cz])
-
-    # triangluar matrix in lammmps cell format
-    # note that in OVITO, it will be down-triangular one
-    # lammps_lattice = np.array([a,b,c]).T
-
-    # write lammps box parameters
-    # https://docs.lammps.org/Howto_triclinic.html#:~:text=The%20inverse%20relationship%20can%20be%20written%20as%20follows
-    lx = np.linalg.norm(a)
-    xy = np.linalg.norm(b) * np.cos(gamma)
-    xz = np.linalg.norm(c) * np.cos(beta)
-    ly = np.sqrt(np.linalg.norm(b) ** 2 - xy**2)
-    yz = (np.linalg.norm(b) * np.linalg.norm(c) * np.cos(alpha) - xy * xz) / ly
-    lz = np.sqrt(np.linalg.norm(c) ** 2 - xz**2 - yz**2)
-
-    # "The parallelepiped has its “origin” at (xlo,ylo,zlo) and is defined by 3 edge vectors starting from the origin given by a = (xhi-xlo,0,0); b = (xy,yhi-ylo,0); c = (xz,yz,zhi-zlo)."
-    # 令原点在(0,0,0)，则 xlo = ylo = zlo = 0
-    xlo = ylo = zlo = 0
-    # https://docs.lammps.org/Howto_triclinic.html#:~:text=the%20LAMMPS%20box%20sizes%20(lx%2Cly%2Clz)%20%3D%20(xhi%2Dxlo%2Cyhi%2Dylo%2Czhi%2Dzlo)
-    xhi = lx + xlo
-    yhi = ly + ylo
-    zhi = lz + zlo
-    # https://docs.lammps.org/Howto_triclinic.html#:~:text=This%20bounding%20box%20is%20convenient%20for%20many%20visualization%20programs%20and%20is%20calculated%20from%20the%209%20triclinic%20box%20parameters%20(xlo%2Cxhi%2Cylo%2Cyhi%2Czlo%2Czhi%2Cxy%2Cxz%2Cyz)%20as%20follows%3A
-    xlo_bound = xlo + np.min([0, xy, xz, xy + xz])
-    xhi_bound = xhi + np.max([0, xy, xz, xy + xz])
-    ylo_bound = ylo + np.min([0, yz])
-    yhi_bound = yhi + np.max([0, yz])
-    zlo_bound = zlo
-    zhi_bound = zhi
-    box_bounds = np.array(
-        [
-            [xlo_bound, xhi_bound, xy],
-            [ylo_bound, yhi_bound, xz],
-            [zlo_bound, zhi_bound, yz],
-        ]
-    )
-
-    return box_bounds
+# -*- coding: utf-8 -*-
+import os
+import numpy as np
+from typing import List, Tuple, Union
+from scipy.ndimage import gaussian_filter1d
+from pymatgen.core import Structure
+import matplotlib.pyplot as plt
+
+
+class MSD:
+    # 用于实际计算均方差的类，摘自pymatgen开源项目
+    def __init__(
+        self,
+        structures: List[Structure],
+        select: Union[str, List[int]] = "all",
+        msd_type="xyz",
+    ):
+        self.structures = structures
+        self.msd_type = msd_type
+
+        self.n_frames = len(structures)
+        if select == "all":
+            self.n_particles = len(structures[0])
+        else:
+            self.n_particles = len(select)
+        self.lattice = structures[0].lattice
+
+        self._parse_msd_type()
+
+        self._position_array = np.zeros((self.n_frames, self.n_particles, self.dim_fac))
+
+        if select == "all":
+            for i, s in enumerate(self.structures):
+                self._position_array[i, :, :] = s.frac_coords[:, self._dim]
+        else:
+            for i, s in enumerate(self.structures):
+                self._position_array[i, :, :] = s.frac_coords[select, :][:, self._dim]
+
+    def _parse_msd_type(self):
+        r"""Sets up the desired dimensionality of the MSD."""
+        keys = {
+            "x": [0],
+            "y": [1],
+            "z": [2],
+            "xy": [0, 1],
+            "xz": [0, 2],
+            "yz": [1, 2],
+            "xyz": [0, 1, 2],
+        }
+
+        self.msd_type = self.msd_type.lower()
+
+        try:
+            self._dim = keys[self.msd_type]
+        except KeyError:
+            raise ValueError(
+                "invalid msd_type: {} specified, please specify one of xyz, "
+                "xy, xz, yz, x, y, z".format(self.msd_type)
+            )
+
+        self.dim_fac = len(self._dim)
+
+    def run(self):
+        print('Calculating MSD...')
+        result = np.zeros((self.n_frames, self.n_particles))
+
+        rd = np.zeros((self.n_frames, self.n_particles, self.dim_fac))
+        for i in range(1, self.n_frames):
+            disp = self._position_array[i, :, :] - self._position_array[i - 1, :, :]
+            # mic by periodic boundary condition
+            disp[np.abs(disp) > 0.5] = disp[np.abs(disp) > 0.5] - np.sign(
+                disp[np.abs(disp) > 0.5]
+            )
+            disp = np.dot(disp, self.lattice.matrix)
+            rd[i, :, :] = disp
+        rd = np.cumsum(rd, axis=0)
+        for n in range(1, self.n_frames):
+            disp = rd[n:, :, :] - rd[:-n, :, :]  # [n:-n] window
+            sqdist = np.square(disp).sum(axis=-1)
+            result[n, :] = sqdist.mean(axis=0)
+
+        return result.mean(axis=1)
+
+
+class RDF:
+    # 用于快速计算径向分布函数的类
+    # Copyright (c) Materials Virtual Lab.
+    # Distributed under the terms of the BSD License.
+    def __init__(
+        self,
+        structures: Union[Structure, List[Structure]],
+        rmin: float = 0.0,
+        rmax: float = 10.0,
+        ngrid: float = 101,
+        sigma: float = 0.0,
+    ):
+        """This method calculates rdf on `np.linspace(rmin, rmax, ngrid)` points
+
+        Parameter
+        ---------
+        structures (list of pymatgen Structures): structures to compute RDF
+        rmin (float): minimal radius
+        rmax (float): maximal radius
+        ngrid (int): number of grid points, defaults to 101
+        sigma (float): smooth parameter
+        """
+        if isinstance(structures, Structure):
+            structures = [structures]
+        self.structures = structures
+        # Number of atoms in all structures should be the same
+        assert len({len(i) for i in self.structures}) == 1, "不同构型的原子数不等！"
+        elements = [[i.specie for i in j.sites] for j in self.structures]
+        unique_elements_on_sites = [len(set(i)) == 1 for i in list(zip(*elements))]
+
+        # For the same site index, all structures should have the same element there
+        if not all(unique_elements_on_sites):
+            raise RuntimeError("Elements are not the same at least for one site")
+
+        self.rmin = rmin
+        self.rmax = rmax
+        self.ngrid = ngrid
+
+        self.dr = (self.rmax - self.rmin) / (self.ngrid - 1)  # end points are on grid
+        self.r = np.linspace(self.rmin, self.rmax, self.ngrid)  # type: ignore
+
+        max_r = self.rmax + self.dr / 2.0  # add a small shell to improve robustness
+
+        self.neighbor_lists = [i.get_neighbor_list(max_r) for i in self.structures]
+        # each neighbor list is a tuple of
+        # center_indices, neighbor_indices, image_vectors, distances
+        (
+            self.center_indices,
+            self.neighbor_indices,
+            self.image_vectors,
+            self.distances,  # 完整的距离列表（遍历体系所有原子）
+        ) = list(
+            zip(*self.neighbor_lists)
+        )
+
+        elements = np.array([str(i.specie) for i in structures[0]])  # type: ignore
+        self.center_elements = [elements[i] for i in self.center_indices]
+        self.neighbor_elements = [elements[i] for i in self.neighbor_indices]
+        self.density = [{}] * len(self.structures)
+
+        self.natoms = [
+            i.composition.to_data_dict["unit_cell_composition"]
+            for i in self.structures  # 抽成单胞化学式
+        ]  # {'H': 2, 'O': 1} 字典构成的列表
+
+        for s_index, natoms in enumerate(self.natoms):  # s_index是结构序号，natoms是单胞化学式字典
+            for i, j in natoms.items():  # i是元素符号，j是原子个数
+                self.density[s_index][i] = (
+                    j / self.structures[s_index].volume
+                )  # 原子数除以体积
+
+        self.volumes = 4.0 * np.pi * self.r**2 * self.dr  # 分母的一部分
+        self.volumes[self.volumes < 1e-8] = 1e8  # avoid divide by zero
+        self.n_structures = len(self.structures)
+        self.sigma = np.ceil(sigma / self.dr)
+        # print(elements)
+
+    def _dist_to_counts(self, d):
+        """Convert a distance array for counts in the bin
+
+        Parameter
+        ---------
+            d: (1D np.array)
+
+        Returns:
+            1D array of counts in the bins centered on self.r
+        """
+        # print(len(d))
+        # print(f'{d=}\n')
+        counts = np.zeros((self.ngrid,))
+        indices = np.array(
+            np.floor((d - self.rmin + 0.5 * self.dr) / self.dr), dtype=int
+        )  # 将找到配对的距离转换为格点序号 (向下取整)
+        # print(len(indices))
+        # print(f'{indices=}\n')
+        # 取整操作，导致格点序号很可能重复，因此需要统计每个格点序号出现的次数并去重
+        unique, val_counts = np.unique(indices, return_counts=True)
+        # print(len(unique))
+        # print(f'{unique=}\n')
+        counts[unique] = val_counts
+        # print(f'{counts=}\n')
+        # raise IndexError
+        return counts
+
+    def get_rdf(
+        self,
+        ref_species: Union[str, List[str]],
+        species: Union[str, List[str]],
+        is_average=True,
+    ):
+        """Wrapper to get the rdf for a given species pair
+
+        Parameter
+        ---------
+        ref_species (list of species or just single specie str):
+            The reference species. The rdfs are calculated with these species at the center
+        species (list of species or just single specie str):
+            the species that we are interested in. The rdfs are calculated on these species.
+        is_average (bool):
+            whether to take the average over all structures
+
+        Returns
+        -------
+        (x, rdf)
+            x is the radial points, and rdf is the rdf value.
+        """
+        print('Calculating RDF...')
+        all_rdfs = [
+                self.get_one_rdf(ref_species, species, i)[1]
+                for i in range(self.n_structures)
+            ]
+        if is_average:
+            all_rdfs = np.mean(all_rdfs, axis=0)
+        return self.r, all_rdfs
+
+    def get_one_rdf(
+        self,
+        ref_species: Union[str, List[str]],
+        species: Union[str, List[str]],
+        index=0,
+    ):
+        """Get the RDF for one structure, indicated by the index of the structure
+        in all structures
+
+        Parameter
+        ---------
+        ref_species (list of species or just single specie str):
+            the reference species. The rdfs are calculated with these species at the center
+        species (list of species or just single specie str):
+            the species that we are interested in. The rdfs are calculated on these species.
+        index (int):
+            structure index in the list
+
+        Returns
+        -------
+            (x, rdf) x is the radial points, and rdf is the rdf value.
+        """
+        if isinstance(ref_species, str):
+            ref_species = [ref_species]
+
+        if isinstance(species, str):
+            species = [species]
+        # print(f'{len(self.center_elements[index])=}')
+        indices = (  # 须同时满足下列条件
+            (np.isin(self.center_elements[index], ref_species))
+            & (np.isin(self.neighbor_elements[index], species))
+            & (self.distances[index] >= self.rmin - self.dr / 2.0)
+            & (self.distances[index] <= self.rmax + self.dr / 2.0)
+            & (self.distances[index] > 1e-8)
+        )
+        # print(f'{len(indices)=}')
+        # raise ValueError
+        # print(f'{indices=}\n')
+        density = sum(self.density[index][i] for i in species)  # 目标元素的原子数密度，单浮点数
+        natoms = sum(self.natoms[index][i] for i in ref_species)  # 中心元素的原子总数，单整数
+        distances = self.distances[index][indices]  # 针对每个中心原子，目标元素的距离列表
+        counts = self._dist_to_counts(distances)  # 统计该距离内目标元素的原子数，列表
+        rdf_temp = (
+            counts / density / self.volumes / natoms
+        )  # counts包含了所有中心元素(ref_species)对应的原子的信息，因此需要除以中心原子总数
+        if self.sigma > 1e-8:
+            rdf_temp = gaussian_filter1d(rdf_temp, self.sigma)
+        return self.r, rdf_temp
+
+    def get_coordination_number(self, ref_species, species, is_average=True):
+        """returns running coordination number
+
+        Parameter
+        ---------
+        ref_species (list of species or just single specie str):
+            the reference species. The rdfs are calculated with these species at the center
+        species (list of species or just single specie str):
+            the species that we are interested in. The rdfs are calculated on these species.
+        is_average (bool): whether to take structural average
+
+        Returns
+        --------
+        numpy array
+        """
+        print('Calculating coordination number...')
+        # Note: The average density from all input structures is used here.
+        all_rdf = self.get_rdf(ref_species, species, is_average=False)[1]
+        if isinstance(species, str):
+            species = [species]
+        density = [sum(i[j] for j in species) for i in self.density]
+        cn = [
+            np.cumsum(rdf * density[i] * 4.0 * np.pi * self.r**2 * self.dr)
+            for i, rdf in enumerate(all_rdf)
+        ]
+        if is_average:
+            cn = np.mean(cn, axis=0)
+        return self.r, cn
+
+
+class RMSD:
+    # 用于计算均方差根（Root Mean Square Deviation）的类，摘自pymatgen开源项目
+    def __init__(self, structures: List[Structure]):
+        self.structures = structures
+
+        self.n_frames = len(self.structures)
+        self.n_particles = len(self.structures[0])
+        self.lattice = self.structures[0].lattice
+
+        self._position_array = np.zeros((self.n_frames, self.n_particles, 3))
+
+        for i, s in enumerate(self.structures):
+            self._position_array[i, :, :] = s.frac_coords
+
+    def run(self, base_index=0):
+        print('Calculating RMSD...')
+        result = np.zeros(self.n_frames)
+        rd = np.zeros((self.n_frames, self.n_particles, 3))
+        for i in range(1, self.n_frames):
+            disp = self._position_array[i, :, :] - self._position_array[i - 1, :, :]
+            # mic by periodic boundary condition
+            disp[np.abs(disp) > 0.5] = disp[np.abs(disp) > 0.5] - np.sign(
+                disp[np.abs(disp) > 0.5]
+            )
+            disp = np.dot(disp, self.lattice.matrix)
+            rd[i, :, :] = disp
+        rd = np.cumsum(rd, axis=0)
+
+        for i in range(self.n_frames):
+            sqdist = np.square(rd[i] - rd[base_index]).sum(axis=-1)
+            result[i] = sqdist.mean()
+
+        return np.sqrt(result)
+
+
+def build_Structures_from_datafile(datafile: Union[str, List[str]]) -> List[Structure]:
+    """读取一/多个h5/json文件，返回pymatgen的Structures列表
+
+    Parameters
+    ----------
+    datafile : 字符串或字符串列表
+        aimd.h5/aimd.json文件或包含任意这些文件文件夹；若给定字符串列表，将依次读取数据并合并成一个Structures列表
+
+    Returns
+    -------
+    List[Structure] : pymatgen structures 列表
+
+    Examples
+    --------
+    >>> from dspawpy.analysis.aimdtools import build_Structures_from_datafile
+    # 读取单个文件
+    >>> pymatgen_Structures = build_Structures_from_datafile(datafile='aimd1.h5')
+    # 给定包含aimd.h5或aimd.json文件的文件夹位置
+    >>> pymatgen_Structures = build_Structures_from_datafile(datafile='my_aimd_task')
+    # 当datafile为列表时，将依次读取多个文件，合并成一个Structures列表
+    >>> pymatgen_Structures = build_Structures_from_datafile(datafile=['aimd1.h5','aimd2.h5'])
+    """
+    dfs = []
+    if isinstance(datafile, list):  # 续算模式，给的是多个文件
+        dfs = datafile
+    else:  # 单次计算模式，处理单个文件
+        if os.path.isdir(datafile):
+            print(f"正在查找读取文件夹 {datafile} 中的aimd.h5或aimd.json文件...")
+            if os.path.exists(os.path.join(datafile, "aimd.h5")):
+                df = os.path.join(datafile, "aimd.h5")
+            elif os.path.exists(os.path.join(datafile, "aimd.json")):
+                df = os.path.join(datafile, "aimd.json")
+            else:
+                raise FileNotFoundError("不存在相应的aimd.h5或aimd.json文件！")
+
+        if datafile.endswith(".h5") or datafile.endswith(".json"):
+            df = datafile
+        else:
+            raise FileNotFoundError("未找到aimd.h5或aimd.json文件！")
+        dfs.append(df)
+
+    # 读取结构数据
+    pymatgen_Structures = []
+    for df in dfs:
+        # TODO 支持选取特定帧
+        structure_list = _get_structure_list(df)
+        pymatgen_Structures.extend(structure_list)
+
+    return pymatgen_Structures
+
+
+def get_lagtime_msd(
+    datafile: Union[str, List[str]],
+    select: Union[str, List[int]] = "all",
+    msd_type: str = "xyz",
+    timestep: float = 1.0,
+):
+    """计算不同时间步长下的均方差
+
+    Parameters
+    ----------
+    datafile : str or list of str
+        aimd.h5或aimd.json文件或包含这两个文件之一的文件夹；
+        写成列表的话将依次读取数据并合并到一起
+    select : str or list of int
+        原子序号列表，原子序号从0开始编号；默认为'all'，计算所有原子
+        暂不支持计算多个元素的MSD
+    msd_type : str
+        计算MSD的类型，可选xyz,xy,xz,yz,x,y,z，默认为'xyz'，即计算所有分量
+    timestep : float
+        时间间隔，单位为fs，默认1.0fs
+
+    Returns
+    -------
+    lagtime : np.ndarray
+        时间序列
+    result : np.ndarray
+        均方差序列
+
+    Examples
+    --------
+    >>> from dspawpy.analysis.aimdtools import get_lagtime_msd
+    >>> lagtime, msd = get_lagtime_msd(datafile='aimd.h5', select='all', msd_type='xyz', timestep=1.0)
+    >>> lagtime
+    array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
+            11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
+            ...,
+            990., 991., 992., 993., 994., 995., 996., 997., 998., 999.])
+    >>> msd
+    array([   0.        ,   67.07025573,  132.46384987,  193.1025821 ,
+            250.1513171 ,  301.71988034,  349.76713326,  397.42586668,
+            ...,
+            1092.833737  , 1067.50385434, 1009.90265319, 1206.1645769 ])
+    """
+    strs = build_Structures_from_datafile(datafile)
+
+    msd = MSD(strs, select, msd_type)
+    result = msd.run()
+
+    nframes = msd.n_frames
+    lagtime = np.arange(nframes) * timestep  # make the lag-time axis
+
+    return lagtime, result
+
+
+def get_lagtime_rmsd(datafile: Union[str, List[str]], timestep: float = 1.0):
+    """
+
+    Parameters
+    ----------
+    datafile : str or list of str
+        aimd.h5或aimd.json文件或包含这两个文件之一的文件夹；
+        写成列表的话将依次读取数据并合并到一起
+    timestep : float
+        时间步长，单位fs，默认1fs
+
+    Returns
+    -------
+    lagtime : numpy.ndarray
+        时间序列
+    rmsd : numpy.ndarray
+        均方根序列
+
+    Examples
+    --------
+    >>> from dspawpy.analysis.aimdtools import get_lagtime_rmsd
+    >>> lagtime, rmsd = get_lagtime_rmsd(datafile='aimd.h5', timestep=1.0)
+    >>> lagtime
+    array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
+            11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,
+            ...,
+            990., 991., 992., 993., 994., 995., 996., 997., 998., 999.])
+    >>> rmsd
+    array([ 0.        , 19.61783543, 19.62557403, 19.63797614, 19.65407193,
+            27.77329091, 27.7898651 , 19.72260788,  2.34196454,  2.62175006,
+            ...,
+            43.97237636, 39.57388473, 39.67579857, 34.61880282, 34.72988017])
+    """
+    strs = build_Structures_from_datafile(datafile)
+
+    rmsd = RMSD(structures=strs)
+    result = rmsd.run()
+
+    # Plot
+    nframes = rmsd.n_frames
+    lagtime = np.arange(nframes) * timestep  # make the lag-time axis
+
+    return lagtime, result
+
+
+def get_rs_rdfs(
+    datafile: Union[str, List[str]],
+    ele1: str,
+    ele2: str,
+    rmin: float = 0,
+    rmax: float = 10,
+    ngrid: float = 101,
+    sigma: float = 0,
+):
+    """计算rdf分布函数
+
+    Parameters
+    ----------
+    datafile : str or list of str
+        aimd.h5或aimd.json文件路径或包含这两个文件之一的文件夹；
+        写成列表的话将依次读取数据并合并到一起
+    ele1 : list
+        中心元素
+    ele2 : list
+        相邻元素
+    rmin : float
+        径向分布最小值，默认为0
+    rmax : float
+        径向分布最大值，默认为10
+    ngrid : int
+        径向分布网格数，默认为101
+    sigma : float
+        平滑参数
+
+    Returns
+    -------
+    r : numpy.ndarray
+        径向分布网格点
+    rdf : numpy.ndarray
+        径向分布函数
+
+    Examples
+    --------
+    >>> from dspawpy.analysis.aimdtools import get_rs_rdfs
+    >>> rs, rdfs = get_rs_rdfs(datafile='aimd.h5', ele1='H', ele2='O', rmin=0, rmax=10, ngrid=101, sigma=0)
+    >>> rs
+    array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,
+            1.1,  1.2,  1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ,  2.1,
+            ...,
+            9.9, 10. ])
+    >>> rdfs
+    array([0.        , 0.        , 0.        , 0.        , 0.        ,
+           0.        , 0.        , 0.        , 0.        , 0.        ,
+           ...,
+           0.97097276])]
+    """
+    strs = build_Structures_from_datafile(datafile)
+    # print(strs[0]) # check pbc
+    # raise ValueError
+
+    # 计算rdf并绘制主要曲线
+    obj = RDF(
+        structures=strs, rmin=rmin, rmax=rmax, ngrid=ngrid, sigma=sigma
+    )
+
+    rs, rdfs = obj.get_rdf(ele1, ele2)
+    return rs, rdfs
+
+
+def plot_msd(
+    lagtime: np.ndarray,
+    result: np.ndarray,
+    xlim: List[float] = None,
+    ylim: List[float] = None,
+    figname: str = None,
+    show: bool = True,
+    ax=None,
+    **kwargs,
+):
+    """AIMD任务完成后，计算均方差（MSD）
+
+    Parameters
+    ----------
+    lagtime : np.ndarray
+        时间序列
+    result : np.ndarray
+        均方差序列
+    xlim : list of float
+        x轴的范围，默认为None，自动设置
+    ylim : list of float
+        y轴的范围，默认为None，自动设置
+    figname : str
+        图片名称，默认为None，不保存图片
+    show : bool
+        是否显示图片，默认为True
+    ax: matplotlib axes object
+        用于将图片绘制到matplotlib的子图上
+    **kwargs : dict
+        其他参数，如线条宽度、颜色等，传递给plt.plot函数
+
+    Returns
+    -------
+    MSD分析后的图片
+
+    Examples
+    --------
+    >>> from dspawpy.analysis.aimdtools import get_lagtime_msd, plot_msd
+    # 指定h5文件位置，用 get_lagtime_msd 函数获取数据，select 参数选择第n个原子（不是元素）
+    >>> lagtime, msd = get_lagtime_msd('H2O-aimd1.h5', select=[0])
+    # 用获取的数据画图并保存
+    >>> plot_msd(lagtime, msd, figname='MSD.png')
+    """
+    if ax:
+        ishow = False
+        ax.plot(lagtime, result, c="black", ls="-", **kwargs)
+    else:
+        ishow = True
+        fig, ax = plt.subplots()
+        ax.plot(lagtime, result, c="black", ls="-", **kwargs)
+        ax.set_xlabel("Time (fs)")
+        ax.set_ylabel("MSD (Å)")
+
+    if xlim:
+        ax.set_xlim(xlim)
+    if ylim:
+        ax.set_ylim(ylim)
+
+    if figname:
+        plt.savefig(figname)
+        print("MSD图片保存在", os.path.abspath(figname))
+    if ishow and show:  # 画子图的话，不应每个子图都show
+        plt.show()  # show会自动清空图片
+
+    return ax
+
+
+def plot_rdf(
+    rs: np.ndarray,
+    rdfs: np.ndarray,
+    ele1: str,
+    ele2: str,
+    xlim: list = None,
+    ylim: list = None,
+    figname: str = None,
+    show: bool = True,
+    ax: plt.Axes = None,
+    **kwargs,
+):
+    """AIMD计算后分析rdf并画图
+
+    Parameters
+    ----------
+    rs : numpy.ndarray
+        径向分布网格点
+    rdfs : numpy.ndarray
+        径向分布函数
+    ele1 : list
+        中心元素
+    ele2 : list
+        相邻元素
+    xlim : list
+        x轴范围，默认为None，即自动设置
+    ylim : list
+        y轴范围，默认为None，即自动设置
+    figname : str
+        图片名称，默认为None，即不保存图片
+    show : bool
+        是否显示图片，默认为True
+    ax: matplotlib.axes.Axes
+        画图的坐标轴，默认为None，即新建坐标轴
+    **kwargs : dict
+        其他参数，如线条宽度、颜色等，传递给plt.plot函数
+
+    Returns
+    -------
+    rdf分析后的图片
+
+    Examples
+    --------
+    >>> from dspawpy.analysis.aimdtools import get_rs_rdfs, plot_rdf
+    # 先获取rs和rdfs数据作为xy轴数据
+    >>> rs, rdfs = get_rs_rdfs(['LiO-aimd1.h5','LiO-aimd2.h5','LiO-aimd3.h5'], 'Li', 'O', rmax=6)
+    # 将xy轴数据传入plot_rdf函数绘图
+    >>> plot_rdf(rs, rdfs, 'Li','O', xlim=[0,6], ylim=[0,35], color='red')
+    """
+    if ax:
+        ax.plot(
+            rs,
+            rdfs,
+            label=r"$g_{\alpha\beta}(r)$" + f"[{ele1},{ele2}]",
+            **kwargs,
+        )
+
+    else:
+        fig, ax = plt.subplots()
+        ax.plot(
+            rs,
+            rdfs,
+            label=r"$g_{\alpha\beta}(r)$" + f"[{ele1},{ele2}]",
+            **kwargs,
+        )
+
+        ax.set_xlabel(r"$r$" + "(Å)")
+        ax.set_ylabel(r"$g(r)$")
+
+    ax.legend()
+
+    # 绘图细节
+    if xlim:
+        ax.set_xlim(xlim)
+    if ylim:
+        ax.set_ylim(ylim)
+
+    if figname:
+        plt.savefig(figname)
+        print(f"图片已保存到 {os.path.abspath(figname)}")
+    if show:  # 画子图的话，不应每个子图都show
+        plt.show()  # show会自动清空图片
+
+
+def plot_rmsd(
+    lagtime: np.ndarray,
+    result: np.ndarray,
+    xlim: list = None,
+    ylim: list = None,
+    figname: str = None,
+    show: bool = True,
+    ax=None,
+    **kwargs,
+):
+    """AIMD计算后分析rmsd并画图
+
+    Parameters
+    ----------
+    lagtime:
+        时间序列
+    result:
+        均方根序列
+    xlim : list
+        x轴范围
+    ylim : list
+        y轴范围
+    figname : str
+        图片保存路径
+    show : bool
+        是否显示图片
+    ax : matplotlib.axes._subplots.AxesSubplot
+        画子图的话，传入子图对象
+    **kwargs : dict
+        传入plt.plot的参数
+
+    Returns
+    -------
+    rmsd分析结构的图片
+
+    Examples
+    --------
+    >>> from dspawpy.analysis.aimdtools import get_lagtime_rmsd, plot_rmsd
+    # timestep 表示时间步长
+    >>> lagtime, rmsd = get_lagtime_rmsd(datafile='H2O-aimd1.h5', timestep=0.1)
+    # 直接保存为RMSD.png图片
+    >>> plot_rmsd(lagtime, rmsd, figname='RMSD.png', show=True)
+    """
+    # 参数初始化
+    if not ax:
+        ishow = True
+    else:
+        ishow = False
+
+    if ax:
+        ax.plot(lagtime, result, **kwargs)
+    else:
+        fig, ax = plt.subplots()
+        ax.plot(lagtime, result, **kwargs)
+        ax.set_xlabel("Time (fs)")
+        ax.set_ylabel("RMSD (Å)")
+
+    if xlim:
+        ax.set_xlim(xlim)
+    if ylim:
+        ax.set_ylim(ylim)
+
+    if figname:
+        plt.savefig(figname)
+    if show and ishow:  # 画子图的话，不应每个子图都show
+        plt.show()  # show会自动清空图片
+
+    return ax
+
+
+def read_h5(
+    hpath: str,
+    index = None,
+    ele = None,
+    ai = None,
+    return_scaled: bool = False,
+):
+    """从hpath指定的路径读取h5文件中的数据
+
+    Parameters
+    ----------
+    hpath: str
+        h5文件路径
+    index: int or list or str
+        运动轨迹中的第几步，从1开始计数
+        如果要切片，用字符串写法： '1, 10:20, 23'
+    ele: str or list or np.array
+        元素，例如 'C'，'H'，'O'，'N'
+    ai: int or list or np.array
+        原子序号（体系中的第几个原子，不是质子数）
+        如果要切片，用字符串写法： '1, 10:20, 23'
+    return_scaled: bool
+        是否返回原子分数坐标，默认为False
+
+    Return
+    -------
+    Nstep: int
+        离子步总数
+    elements: list
+        元素列表, Natom x 1
+    positions: np.ndarray
+        原子位置,  Nstep x Natom x 3
+    lattices: np.ndarray
+        晶胞, Nstep x 3 x 3
+
+    Examples
+    --------
+    >>> from dspawpy.analysis.aimdtools import read_h5
+    >>> Nstep, elements, positions, lattices = read_h5(hpath='aimd.h5', ele='H', index='1:2')
+    >>> Nstep
+    2
+    >>> elements
+    ['H', 'H']
+    >>> positions
+    array([[[0.56037855, 5.60910012, 0.06341764],
+            [0.57622933, 0.78033174, 6.28639689]],
+           [[0.55354018, 5.60627362, 0.12845834],
+            [0.57012995, 0.80246543, 6.22272366]]])
+    >>> lattices
+    array([[[6.35016, 0.     , 0.     ],
+            [0.     , 6.35016, 0.     ],
+            [0.     , 0.     , 6.35016]],
+           [[6.35016, 0.     , 0.     ],
+            [0.     , 6.35016, 0.     ],
+            [0.     , 0.     , 6.35016]]])
+    """
+    import h5py
+
+    print(f"Reading {os.path.abspath(hpath)} ...")
+    hf = h5py.File(hpath)  # 加载h5文件
+    Total_step = len(np.array(hf.get("/Structures"))) - 2  # 总步数
+
+    if ele and ai:
+        raise ValueError("暂不支持同时指定元素和原子序号")
+    # 步数
+    if index:
+        if isinstance(index, int):  # 1
+            indices = [index]
+
+        elif isinstance(index, list) or isinstance(ai, np.ndarray):  # [1,2,3]
+            indices = index
+
+        elif isinstance(index, str):  # ':', '-3:'
+            indices = _parse_indices(index, Total_step)
+
+        else:
+            raise ValueError("请输入正确格式的index")
+
+        Nstep = len(indices)
+    else:
+        Nstep = Total_step
+        indices = list(range(1, Nstep + 1))
+
+    # 读取元素列表，这个列表不会随步数改变，也不会“合并同类项”
+    from dspawpy.io.utils import get_ele_from_h5
+
+    Elements = np.array(get_ele_from_h5(hpath), dtype="str")
+
+    # 开始读取晶胞和原子位置
+    lattices = np.empty((Nstep, 3, 3))  # Nstep x 3 x 3
+    location = []
+    if ele:  # 如果用户指定元素
+        if isinstance(ele, str):  # 单个元素符号，例如 'Fe'
+            ele_list = np.array(ele, dtype="str")
+            location = np.where(Elements == ele_list)[0]
+        # 多个元素符号组成的列表，例如 ['Fe', 'O']
+        elif isinstance(ele, list) or isinstance(ele, np.ndarray):
+            for e in ele:
+                loc = np.where(Elements == e)[0]
+                location.append(loc)
+            location = np.concatenate(location)
+        else:
+            raise TypeError("请输入正确的元素或元素列表")
+        elements = Elements[location]
+
+    elif ai:  # 如果用户指定原子序号
+        if isinstance(ai, int):  # 1
+            ais = [ai]
+        elif isinstance(ai, list) or isinstance(ai, np.ndarray):  # [1,2,3]
+            ais = ai
+        elif isinstance(ai, str):  # ':', '-3:'
+            ais = _parse_indices(ai, Total_step)
+        else:
+            raise ValueError("请输入正确格式的ai")
+        ais = [i - 1 for i in ais]  # python从0开始计数，但是用户从1开始计数
+        elements = Elements[ais]
+        location = ais
+
+    else:  # 如果都没指定
+        elements = Elements
+        location = list(range(len(Elements)))
+
+    elements = elements.tolist()  # for pretty output
+
+    if return_scaled:
+        scaled_positions = np.empty(shape=(len(indices), len(elements), 3))
+        for i, index in enumerate(indices):  # 步数
+            lats = np.array(hf.get("/Structures/Step-" + str(index) + "/Lattice"))
+            lattices[i] = lats
+            # [x1,y1,z1,x2,y2,z2,x3,y3,z3], ...
+            # 结构优化时输出的都是分数坐标，不管CoordinateType写的是啥！
+            spos = np.array(hf.get("/Structures/Step-" + str(index) + "/Position"))
+            wrapped_spos = spos - np.floor(spos)  # wrap into [0,1)
+            wrapped_spos = wrapped_spos.flatten().reshape(-1, 3).T  # reshape
+            for j, sli in enumerate(location):
+                scaled_positions[i, j, :] = np.dot(wrapped_spos[:, sli], np.eye(3, 3))
+        return Nstep, elements, scaled_positions, lattices
+
+    else:
+        # Nstep x Natom x 3
+        positions = np.empty(shape=(len(indices), len(elements), 3))
+        for i, index in enumerate(indices):  # 步数
+            lats = np.array(hf.get("/Structures/Step-" + str(index) + "/Lattice"))
+            lattices[i] = lats
+            # [x1,y1,z1,x2,y2,z2,x3,y3,z3], ...
+            # 结构优化时输出的都是分数坐标，不管CoordinateType写的是啥！
+            spos = np.array(hf.get("/Structures/Step-" + str(index) + "/Position"))
+            wrapped_spos = spos - np.floor(spos)  # wrap into [0,1)
+            wrapped_spos = wrapped_spos.flatten().reshape(-1, 3).T  # reshape
+            for j, sli in enumerate(location):
+                positions[i, j, :] = np.dot(wrapped_spos[:, sli], lats)
+
+        return Nstep, elements, positions, lattices
+
+
+# 2. 写轨迹文件
+def write_xyz_traj(
+    datafile: str = "aimd.h5",
+    ai=None,
+    ele=None,
+    index=None,
+    xyzfile="aimdTraj.xyz",
+):
+    """保存xyz格式的轨迹文件
+
+    Parameters
+    ----------
+    datafile : str
+        DSPAW计算完成后保存的h5/json文件或包含它们的文件夹路径
+    ai : int
+        原子编号列表（体系中的第几号原子，不是质子数）
+    ele : str
+        元素，例如 'C'，'H'，'O'，'N'
+    index : int
+        优化过程中的第几步
+
+    Returns
+    -------
+    xyzfile: str
+        写入xyz格式的轨迹文件，默认为aimdTraj.xyz
+
+    Example
+    -------
+    >>> from dspawpy.analysis.aimdtools import write_xyz_traj
+    >>> write_xyz_traj(datafile='aimd.h5', ai=[1,2,3], index=1, xyzfile='aimdTraj.xyz')
+    """
+    # search datafile in the given directory
+    if os.path.isdir(datafile):
+        directory = datafile  # specified datafile is actually a directory
+        print("您指定了一个文件夹，正在查找相关h5或json文件...")
+        if os.path.exists(os.path.join(directory, "aimd.h5")):
+            datafile = os.path.join(directory, "aimd.h5")
+            print("Reading aimd.h5...")
+        elif os.path.exists(os.path.join(directory, "aimd.json")):
+            datafile = os.path.join(directory, "aimd.json")
+            print("Reading aimd.json...")
+        else:
+            raise FileNotFoundError("未找到aimd.h5/aimd.json文件！")
+    if datafile.endswith(".h5"):
+        Nstep, eles, poses, lats = read_h5(datafile, index, ele, ai)
+    elif datafile.endswith(".json"):
+        raise NotImplementedError("json文件暂不支持写入xyz文件！")
+    else:
+        raise TypeError("仅支持读取h5或json文件！")
+    
+    # 写入文件
+    with open(xyzfile, "w") as f:
+        # Nstep
+        for n in range(Nstep):
+            # 原子数不会变，就是不合并的元素总数
+            f.write("%d\n" % len(eles))
+            # lattice
+            f.write(
+                'Lattice="%f %f %f %f %f %f %f %f %f" Properties=species:S:1:pos:R:3 pbc="T T T"\n'
+                % (
+                    lats[n, 0, 0],
+                    lats[n, 0, 1],
+                    lats[n, 0, 2],
+                    lats[n, 1, 0],
+                    lats[n, 1, 1],
+                    lats[n, 1, 2],
+                    lats[n, 2, 0],
+                    lats[n, 2, 1],
+                    lats[n, 2, 2],
+                )
+            )
+            # position and element
+            for i in range(len(eles)):
+                f.write(
+                    "%s %f %f %f\n"
+                    % (eles[i], poses[n, i, 0], poses[n, i, 1], poses[n, i, 2])
+                )
+
+
+def write_dump_traj(
+    datafile="aimd.h5",
+    ai=None,
+    ele=None,
+    index=None,
+    dumpfile="aimdTraj.dump",
+):
+    """保存为lammps的dump格式的轨迹文件，暂时只支持正交晶胞
+
+    Parameters
+    ----------
+    datafile : str
+        DSPAW计算完成后保存的h5/json文件或包含它们的文件夹路径
+    ai : int
+        原子编号列表（体系中的第几号原子，不是质子数）
+    ele : str
+        元素，例如 'C'，'H'，'O'，'N'
+    index : int
+        优化过程中的第几步
+
+    Returns
+    -------
+    dumpfile: str
+        写入xyz格式的轨迹文件，默认为aimdTraj.xyz
+
+    Example
+    -------
+    >>> from dspawpy.analysis.aimdtools import write_dump_traj
+    >>> write_dump_traj(datafile='aimd.h5', ai=[1,2,3], index=1, dumpfile='aimdTraj.dump')
+    """
+    # search datafile in the given directory
+    if os.path.isdir(datafile):
+        directory = datafile  # specified datafile is actually a directory
+        print("您指定了一个文件夹，正在查找相关h5或json文件...")
+        if os.path.exists(os.path.join(directory, "aimd.h5")):
+            datafile = os.path.join(directory, "aimd.h5")
+            print("Reading aimd.h5...")
+        elif os.path.exists(os.path.join(directory, "aimd.json")):
+            datafile = os.path.join(directory, "aimd.json")
+            print("Reading aimd.json...")
+        else:
+            raise FileNotFoundError("未找到aimd.h5/aimd.json文件！")
+    if datafile.endswith(".h5"):
+        Nstep, eles, poses, lats = read_h5(datafile, index, ele, ai)
+    elif datafile.endswith(".json"):
+        raise NotImplementedError("json文件暂不支持写入xyz文件！")
+    else:
+        raise TypeError("仅支持读取h5或json文件！")
+
+    # 写入文件
+    with open(dumpfile, "w") as f:
+        for n in range(Nstep):
+            box_bounds = _get_lammps_non_orthogonal_box(lats[n])
+            f.write("ITEM: TIMESTEP\n%d\n" % n)
+            f.write("ITEM: NUMBER OF ATOMS\n%d\n" % (len(eles)))
+            f.write("ITEM: BOX BOUNDS xy xz yz xx yy zz\n")
+            f.write(
+                "%f %f %f\n%f %f %f\n %f %f %f\n"
+                % (
+                    box_bounds[0][0],
+                    box_bounds[0][1],
+                    box_bounds[0][2],
+                    box_bounds[1][0],
+                    box_bounds[1][1],
+                    box_bounds[1][2],
+                    box_bounds[2][0],
+                    box_bounds[2][1],
+                    box_bounds[2][2],
+                )
+            )
+            f.write("ITEM: ATOMS type x y z id\n")
+            for i in range(len(eles)):
+                f.write(
+                    "%s %f %f %f %d\n"
+                    % (eles[i], poses[n, i, 0], poses[n, i, 1], poses[n, i, 2], i + 1)
+                )
+
+
+def _get_structure_list(df: str = "aimd.h5") -> List[Structure]:
+    """get pymatgen structures from single datafile
+
+    Parameters
+    ----------
+    df : str, optional
+        datafile, by default "aimd.h5"
+
+    Returns
+    -------
+    List[Structure] : list of pymatgen structures
+
+    Examples
+    --------
+    >>> from dspawpy.analysis.aimdtools import get_structure_list
+    >>> structure_list = get_structure_list(df='aimd.h5')
+    """
+    if df.endswith(".h5"):
+        # create Structure structure_list from aimd.h5
+        Nstep, elements, positions, lattices = read_h5(df)
+        strs = []
+        for i in range(Nstep):
+            strs.append(
+                Structure(
+                    lattices[i], elements, positions[i], coords_are_cartesian=False
+                )
+            )
+    elif df.endswith(".json"):
+        from dspawpy.io.read import json2structures
+
+        strs = json2structures(df)
+    else:
+        raise ValueError(f"{df} file format not supported")
+
+    return strs
+
+
+def _get_neighbor_list(structure, r) -> Tuple:
+    """Thin wrapper to enable parallel calculations
+
+    Parameter
+    ---------
+    structure (pymatgen Structure): pymatgen structure
+    r (float): cutoff radius
+
+    Returns
+    --------
+    tuple of neighbor list
+    """
+    return structure.get_neighbor_list(r)
+
+
+def _parse_indices(index: str, total_step) -> list:
+    """解析用户输入的原子序号字符串
+
+    输入：
+        - index: 用户输入的原子序号/元素字符串，例如 '1:3,5,7:10'
+    输出：
+        - indices: 解析后的原子序号列表，例如 [1,2,3,4,5,6,7,8,9,10]
+    """
+    assert ":" in index, "如果不想切片索引，请输入整数或者列表"
+    blcs = index.split(",")
+    indices = []
+    for blc in blcs:
+        if ":" in blc:  # 切片
+            low = blc.split(":")[0]
+            if not low:
+                low = 1  # 从1开始
+            else:
+                low = int(low)
+                assert low > 0, "索引从1开始！"
+            high = blc.split(":")[1]
+            if not high:
+                high = total_step
+            else:
+                high = int(high)
+                assert high <= total_step, "索引超出范围！"
+
+            for i in range(low, high + 1):
+                indices.append(i)
+        else:  # 单个数字
+            indices.append(int(blc))
+    return indices
+
+
+def _read_json(
+    jpath: str,
+    index= None,
+    ele = None,
+    ai= None,
+):
+    """从json指定的路径读取数据
+
+    输入:
+    - jpath: json文件路径
+    - ai: 原子序号（体系中的第几个原子，不是质子数）
+    - ele: 元素，例如 'C'，'H'，'O'，'N'
+    - index: 运动轨迹中的第几步，从1开始
+
+    输出：
+    - Nstep: 总共要保存多少步的信息, int
+    - elements: 元素列表, list, Natom x 1
+    - positions: 原子位置, list, Nstep x Natom x 3
+    - lattices: 晶胞, list, Nstep x 3 x 3
+    """
+    import json
+
+    with open(jpath, "r") as f:
+        data = json.load(f)  # 加载json文件
+
+    Total_step = len(data["Structures"])  # 总步数
+
+    if ele and ai:
+        raise ValueError("暂不支持同时指定元素和原子序号")
+    # 步数
+    if index:
+        if isinstance(index, int):  # 1
+            indices = [index]
+
+        elif isinstance(index, list) or isinstance(ai, np.ndarray):  # [1,2,3]
+            indices = index
+
+        elif isinstance(index, str):  # ':', '-3:'
+            indices = _parse_indices(index, Total_step)
+
+        else:
+            raise ValueError("请输入正确格式的index")
+
+        Nstep = len(indices)
+    else:
+        Nstep = Total_step
+        indices = list(range(1, Nstep + 1))  # [1,Nstep+1)
+
+    # 预先读取全部元素的总列表，这个列表不会随步数改变，也不会“合并同类项”
+    # 这样可以避免在循环内部频繁判断元素是否符合用户需要
+
+    Nele = len(data["Structures"][0]["Atoms"])  # 总元素数量
+    total_elements = np.empty(shape=(Nele), dtype="str")  # 未合并的元素列表
+    for i in range(Nele):
+        element = data["Structures"][0]["Atoms"][i]["Element"]
+        total_elements[i] = element
+
+    # 开始读取晶胞和原子位置
+    # 在data['Structures']['%d' % index]['Atoms']中根据元素所在序号选择结构
+    if ele:  # 用户指定要某些元素
+        location = []
+        if isinstance(ele, str):  # 单个元素符号，例如 'Fe'
+            ele_list = list(ele)
+        # 多个元素符号组成的列表，例如 ['Fe', 'O']
+        elif isinstance(ele, list) or isinstance(ele, np.ndarray):
+            ele_list = ele
+        else:
+            raise TypeError("请输入正确的元素或元素列表")
+        for e in ele_list:
+            location.append(np.where(total_elements == e)[0])
+        location = np.concatenate(location)
+
+    elif ai:  # 如果用户指定原子序号，也要据此筛选元素列表
+        if isinstance(ai, int):  # 1
+            ais = [ai]
+        elif isinstance(ai, list) or isinstance(ai, np.ndarray):  # [1,2,3]
+            ais = ai
+        elif isinstance(ai, str):  # ':', '-3:'
+            ais = _parse_indices(ai, Total_step)
+        else:
+            raise ValueError("请输入正确格式的ai")
+        ais = [i - 1 for i in ais]  # python从0开始计数，但是用户从1开始计数
+        location = ais
+        # read lattices and poses
+
+    else:  # 如果都没指定
+        location = list(range(Total_step))
+
+    # 满足用户需要的elements列表
+    elements = np.empty(shape=len(location), dtype="str")
+    for i in range(len(location)):
+        elements[i] = total_elements[location[i]]
+
+    # Nstep x Natom x 3
+    positions = np.empty(shape=(len(indices), len(elements), 3))
+    lattices = np.empty(shape=(Nstep, 3, 3))  # Nstep x 3 x 3
+    for i, index in enumerate(indices):  # 步数
+        lat = data["Structures"][index - 1]["Lattice"]
+        lattices[i] = np.array(lat).reshape(3, 3)
+        for j, sli in enumerate(location):
+            positions[i, j, :] = data["Structures"][index - 1]["Atoms"][sli][
+                "Position"
+            ][:]
+
+    return Nstep, elements, positions, lattices
+
+
+def _get_lammps_non_orthogonal_box(lat: np.ndarray):
+    """计算用于输入lammps的盒子边界参数
+
+    Parameters
+    ----------
+    lat : np.ndarray
+        常见的非三角3x3矩阵
+
+    Returns
+    -------
+    box_bounds:
+        用于输入lammps的盒子边界
+    """
+    # https://docs.lammps.org/Howto_triclinic.html
+    A = lat[0]
+    B = lat[1]
+    C = lat[2]
+    assert np.cross(A, B).dot(C) > 0, "Lat is not right handed"
+
+    # 将常规3x3矩阵转成标准的上三角矩阵
+    alpha = np.arccos(np.dot(B, C) / (np.linalg.norm(B) * np.linalg.norm(C)))
+    beta = np.arccos(np.dot(A, C) / (np.linalg.norm(A) * np.linalg.norm(C)))
+    gamma = np.arccos(np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B)))
+
+    ax = np.linalg.norm(A)
+    a = np.array([ax, 0, 0])
+
+    bx = np.linalg.norm(B) * np.cos(gamma)
+    by = np.linalg.norm(B) * np.sin(gamma)
+    b = np.array([bx, by, 0])
+
+    cx = np.linalg.norm(C) * np.cos(beta)
+    cy = (np.linalg.norm(B) * np.linalg.norm(C) - bx * cx) / by
+    cz = np.sqrt(abs(np.linalg.norm(C) ** 2 - cx**2 - cy**2))
+    c = np.array([cx, cy, cz])
+
+    # triangluar matrix in lammmps cell format
+    # note that in OVITO, it will be down-triangular one
+    # lammps_lattice = np.array([a,b,c]).T
+
+    # write lammps box parameters
+    # https://docs.lammps.org/Howto_triclinic.html#:~:text=The%20inverse%20relationship%20can%20be%20written%20as%20follows
+    lx = np.linalg.norm(a)
+    xy = np.linalg.norm(b) * np.cos(gamma)
+    xz = np.linalg.norm(c) * np.cos(beta)
+    ly = np.sqrt(np.linalg.norm(b) ** 2 - xy**2)
+    yz = (np.linalg.norm(b) * np.linalg.norm(c) * np.cos(alpha) - xy * xz) / ly
+    lz = np.sqrt(np.linalg.norm(c) ** 2 - xz**2 - yz**2)
+
+    # "The parallelepiped has its “origin” at (xlo,ylo,zlo) and is defined by 3 edge vectors starting from the origin given by a = (xhi-xlo,0,0); b = (xy,yhi-ylo,0); c = (xz,yz,zhi-zlo)."
+    # 令原点在(0,0,0)，则 xlo = ylo = zlo = 0
+    xlo = ylo = zlo = 0
+    # https://docs.lammps.org/Howto_triclinic.html#:~:text=the%20LAMMPS%20box%20sizes%20(lx%2Cly%2Clz)%20%3D%20(xhi%2Dxlo%2Cyhi%2Dylo%2Czhi%2Dzlo)
+    xhi = lx + xlo
+    yhi = ly + ylo
+    zhi = lz + zlo
+    # https://docs.lammps.org/Howto_triclinic.html#:~:text=This%20bounding%20box%20is%20convenient%20for%20many%20visualization%20programs%20and%20is%20calculated%20from%20the%209%20triclinic%20box%20parameters%20(xlo%2Cxhi%2Cylo%2Cyhi%2Czlo%2Czhi%2Cxy%2Cxz%2Cyz)%20as%20follows%3A
+    xlo_bound = xlo + np.min([0, xy, xz, xy + xz])
+    xhi_bound = xhi + np.max([0, xy, xz, xy + xz])
+    ylo_bound = ylo + np.min([0, yz])
+    yhi_bound = yhi + np.max([0, yz])
+    zlo_bound = zlo
+    zhi_bound = zhi
+    box_bounds = np.array(
+        [
+            [xlo_bound, xhi_bound, xy],
+            [ylo_bound, yhi_bound, xz],
+            [zlo_bound, zhi_bound, yz],
+        ]
+    )
+
+    return box_bounds
```

## dspawpy/analysis/vacf.py

 * *Ordering differences only*

```diff
@@ -1,555 +1,555 @@
-"""
-This module has not been tested yet, use at your own risk!
-"""
-
-import numpy as np
-from scipy.fftpack import fft
-from scipy.integrate import simps
-
-
-def welch(M, sym=1):
-    """Welch window. Function skeleton shamelessly stolen from
-    scipy.signal.bartlett() and others."""
-    if M < 1:
-        return np.array([])
-    if M == 1:
-        return np.ones(1, dtype=float)
-    odd = M % 2
-    if not sym and not odd:
-        M = M + 1
-    n = np.arange(0, M)
-    w = 1.0 - ((n - 0.5 * (M - 1)) / (0.5 * (M - 1))) ** 2.0
-    if not sym and not odd:
-        w = w[:-1]
-    return w
-
-
-def mirror(arr, axis=0):
-    """Mirror array `arr` at index 0 along `axis`.
-    The length of the returned array is 2*arr.shape[axis]-1 ."""
-    return np.concatenate((arr[::-1], arr[1:]), axis=axis)
-
-
-def pad_zeros(
-    arr, axis=0, where="end", nadd=None, upto=None, tonext=None, tonext_min=None
-):
-    """Pad an nd-array with zeros. Default is to append an array of zeros of
-    the same shape as `arr` to arr's end along `axis`.
-    Parameters
-    ----------
-    arr :  nd array
-    axis : the axis along which to pad
-    where : string {'end', 'start'}, pad at the end ("append to array") or
-        start ("prepend to array") of `axis`
-    nadd : number of items to padd (i.e. nadd=3 means padd w/ 3 zeros in case
-        of an 1d array)
-    upto : pad until arr.shape[axis] == upto
-    tonext : bool, pad up to the next power of two (pad so that the padded
-        array has a length of power of two)
-    tonext_min : int, when using `tonext`, pad the array to the next possible
-        power of two for which the resulting array length along `axis` is at
-        least `tonext_min`; the default is tonext_min = arr.shape[axis]
-    Use only one of nadd, upto, tonext.
-    Returns
-    -------
-    padded array
-    Examples
-    --------
-    >>> # 1d
-    >>> pad_zeros(a)
-    array([1, 2, 3, 0, 0, 0])
-    >>> pad_zeros(a, nadd=3)
-    array([1, 2, 3, 0, 0, 0])
-    >>> pad_zeros(a, upto=6)
-    array([1, 2, 3, 0, 0, 0])
-    >>> pad_zeros(a, nadd=1)
-    array([1, 2, 3, 0])
-    >>> pad_zeros(a, nadd=1, where='start')
-    array([0, 1, 2, 3])
-    >>> # 2d
-    >>> a=arange(9).reshape(3,3)
-    >>> pad_zeros(a, nadd=1, axis=0)
-    array([[0, 1, 2],
-           [3, 4, 5],
-           [6, 7, 8],
-           [0, 0, 0]])
-    >>> pad_zeros(a, nadd=1, axis=1)
-    array([[0, 1, 2, 0],
-           [3, 4, 5, 0],
-           [6, 7, 8, 0]])
-    >>> # up to next power of two
-    >>> 2**arange(10)
-    array([  1,   2,   4,   8,  16,  32,  64, 128, 256, 512])
-    >>> pydos.pad_zeros(arange(9), tonext=True).shape
-    (16,)
-    """
-    if tonext == False:
-        tonext = None
-    lst = [nadd, upto, tonext]
-    assert lst.count(None) in [2, 3], (
-        "`nadd`, `upto` and `tonext` must be " + "all None or only one of them not None"
-    )
-    if nadd is None:
-        if upto is None:
-            if (tonext is None) or (not tonext):
-                # default
-                nadd = arr.shape[axis]
-            else:
-                tonext_min = arr.shape[axis] if (tonext_min is None) else tonext_min
-                # beware of int overflows starting w/ 2**arange(64), but we
-                # will never have such long arrays anyway
-                two_powers = 2 ** np.arange(30)
-                assert tonext_min <= two_powers[-1], (
-                    "tonext_min exceeds " "max power of 2"
-                )
-                power = two_powers[np.searchsorted(two_powers, tonext_min)]
-                nadd = power - arr.shape[axis]
-        else:
-            nadd = upto - arr.shape[axis]
-    if nadd == 0:
-        return arr
-    add_shape = list(arr.shape)
-    add_shape[axis] = nadd
-    add_shape = tuple(add_shape)
-    if where == "end":
-        return np.concatenate((arr, np.zeros(add_shape, dtype=arr.dtype)), axis=axis)
-    elif where == "start":
-        return np.concatenate((np.zeros(add_shape, dtype=arr.dtype), arr), axis=axis)
-    else:
-        raise Exception("illegal `where` arg: %s" % where)
-
-
-def slicetake(a, sl, axis=None, copy=False):
-    """The equivalent of numpy.take(a, ..., axis=<axis>), but accepts slice
-    objects instead of an index array. Also by default, it returns a *view* and
-    no copy.
-    Parameters
-    ----------
-    a : numpy ndarray
-    sl : slice object, list or tuple of slice objects
-        axis=<int>
-            one slice object for *that* axis
-        axis=None
-            `sl` is a list or tuple of slice objects, one for each axis.
-            It must index the whole array, i.e. len(sl) == len(a.shape).
-    axis : {None, int}
-    copy : bool, return a copy instead of a view
-    Returns
-    -------
-    A view into `a` or copy of a slice of `a`.
-    Examples
-    --------
-    >>> from numpy import s_
-    >>> a = np.random.rand(20,20,20)
-    >>> b1 = a[:,:,10:]
-    >>> # single slice for axis 2
-    >>> b2 = slicetake(a, s_[10:], axis=2)
-    >>> # tuple of slice objects
-    >>> b3 = slicetake(a, s_[:,:,10:])
-    >>> (b2 == b1).all()
-    True
-    >>> (b3 == b1).all()
-    True
-    >>> # simple extraction too, sl = integer
-    >>> (a[...,5] == slicetake(a, 5, axis=-1))
-    True
-    """
-    # The long story
-    # --------------
-    #
-    # 1) Why do we need that:
-    #
-    # # no problem
-    # a[5:10:2]
-    #
-    # # the same, more general
-    # sl = slice(5,10,2)
-    # a[sl]
-    #
-    # But we want to:
-    #  - Define (type in) a slice object only once.
-    #  - Take the slice of different arrays along different axes.
-    # Since numpy.take() and a.take() don't handle slice objects, one would
-    # have to use direct slicing and pay attention to the shape of the array:
-    #
-    #     a[sl], b[:,:,sl,:], etc ...
-    #
-    # We want to use an 'axis' keyword instead. np.r_() generates index arrays
-    # from slice objects (e.g r_[1:5] == r_[s_[1:5] ==r_[slice(1,5,None)]).
-    # Since we need index arrays for numpy.take(), maybe we can use that? Like
-    # so:
-    #
-    #     a.take(r_[sl], axis=0)
-    #     b.take(r_[sl], axis=2)
-    #
-    # Here we have what we want: slice object + axis kwarg.
-    # But r_[slice(...)] does not work for all slice types. E.g. not for
-    #
-    #     r_[s_[::5]] == r_[slice(None, None, 5)] == array([], dtype=int32)
-    #     r_[::5]                                 == array([], dtype=int32)
-    #     r_[s_[1:]]  == r_[slice(1, None, None)] == array([0])
-    #     r_[1:]
-    #         ValueError: dimensions too large.
-    #
-    # The returned index arrays are wrong (or we even get an exception).
-    # The reason is given below.
-    # Bottom line: We need this function.
-    #
-    # The reason for r_[slice(...)] gererating sometimes wrong index arrays is
-    # that s_ translates a fancy index (1:, ::5, 1:10:2, ...) to a slice
-    # object. This *always* works. But since take() accepts only index arrays,
-    # we use r_[s_[<fancy_index>]], where r_ translates the slice object
-    # prodced by s_ to an index array. THAT works only if start and stop of the
-    # slice are known. r_ has no way of knowing the dimensions of the array to
-    # be sliced and so it can't transform a slice object into a correct index
-    # array in case of slice(<number>, None, None) or slice(None, None,
-    # <number>).
-    #
-    # 2) Slice vs. copy
-    #
-    # numpy.take(a, array([0,1,2,3])) or a[array([0,1,2,3])] return a copy of
-    # `a` b/c that's "fancy indexing". But a[slice(0,4,None)], which is the
-    # same as indexing (slicing) a[:4], return *views*.
-
-    if axis is None:
-        slices = sl
-    else:
-        # Note that these are equivalent:
-        #   a[:]
-        #   a[s_[:]]
-        #   a[slice(None)]
-        #   a[slice(None, None, None)]
-        #   a[slice(0, None, None)]
-        slices = [slice(None)] * a.ndim
-        slices[axis] = sl
-    # a[...] can take a tuple or list of slice objects
-    # a[x:y:z, i:j:k] is the same as
-    # a[(slice(x,y,z), slice(i,j,k))] == a[[slice(x,y,z), slice(i,j,k)]]
-    slices = tuple(slices)
-    if copy:
-        return a[slices].copy()
-    else:
-        return a[slices]
-
-
-def sum(arr, axis=None, keepdims=False, **kwds):
-    """This numpy.sum() with some features implemented which can be found in
-    numpy v1.7 and later: `axis` can be a tuple to select arbitrary axes to sum
-    over.
-    We also have a `keepdims` keyword, which however works completely different
-    from numpy. Docstrings shamelessly stolen from numpy and adapted here
-    and there.
-    Parameters
-    ----------
-    arr : nd array
-    axis : None or int or tuple of ints, optional
-        Axis or axes along which a sum is performed. The default (`axis` =
-        `None`) is to perform a sum over all the dimensions of the input array.
-        `axis` may be negative, in which case it counts from the last to the
-        first axis.
-        If this is a tuple of ints, a sum is performed on multiple
-        axes, instead of a single axis or all the axes as before.
-    keepdims : bool, optional
-        If this is set to True, the axes from `axis` are left in the result
-        and the reduction (sum) is performed for all remaining axes. Therefore,
-        it reverses the `axis` to be summed over.
-    **kwds : passed to np.sum().
-    Examples
-    --------
-    >>> a=rand(2,3,4)
-    >>> num.sum(a)
-    12.073636268676152
-    >>> a.sum()
-    12.073636268676152
-    >>> num.sum(a, axis=1).shape
-    (2, 4)
-    >>> num.sum(a, axis=(1,)).shape
-    (2, 4)
-    >>> # same as axis=1, i.e. it inverts the axis over which we sum
-    >>> num.sum(a, axis=(0,2), keepdims=True).shape
-    (2, 4)
-    >>> # numpy's keepdims has another meaning: it leave the summed axis (0,2)
-    >>> # as dimension of size 1 to allow broadcasting
-    >>> numpy.sum(a, axis=(0,2), keepdims=True).shape
-    (1, 3, 1)
-    >>> num.sum(a, axis=(1,)) - num.sum(a, axis=1)
-    array([[ 0.,  0.,  0.,  0.],
-           [ 0.,  0.,  0.,  0.]])
-    >>> num.sum(a, axis=(0,2)).shape
-    (3,)
-    >>> num.sum(a, axis=(0,2)) - a.sum(axis=0).sum(axis=1)
-    array([ 0.,  0.,  0.])
-    """
-
-    # Recursion rocks!
-    def _sum(arr, tosum):
-        if len(tosum) > 0:
-            # Choose axis to sum over, remove from list w/ remaining axes.
-            axis = tosum.pop(0)
-            _arr = arr.sum(axis=axis)
-            # arr has one dim less now. Rename remaining axes accordingly.
-            _tosum = [xx - 1 if xx > axis else xx for xx in tosum]
-            return _sum(_arr, _tosum)
-        else:
-            return arr
-
-    axis_is_int = isinstance(axis, int)
-    if axis is None:
-        if keepdims:
-            raise Exception("axis=None + keepdims=True makes no sense")
-        else:
-            return np.sum(arr, axis=axis, **kwds)
-    elif axis_is_int and not keepdims:
-        return np.sum(arr, axis=axis, **kwds)
-    else:
-        if axis_is_int:
-            tosum = [axis]
-        elif isinstance(axis, tuple) or isinstance(axis, list):
-            tosum = list(axis)
-        else:
-            raise Exception("illegal type for axis: %s" % str(type(axis)))
-        if keepdims:
-            alldims = range(arr.ndim)
-            tosum = [xx for xx in alldims if xx not in tosum]
-        return _sum(arr, tosum)
-
-
-def norm_int(y, x, area=1.0, scale=True, func=simps):
-    """Normalize integral area of y(x) to `area`.
-    Parameters
-    ----------
-    x,y : numpy 1d arrays
-    area : float
-    scale : bool, optional
-        Scale x and y to the same order of magnitude before integration.
-        This may be necessary to avoid numerical trouble if x and y have very
-        different scales.
-    func : callable
-        Function to do integration (like scipy.integrate.{simps,trapz,...}
-        Called as ``func(y,x)``. Default: simps
-    Returns
-    -------
-    scaled y
-    Notes
-    -----
-    The argument order y,x might be confusing. x,y would be more natural but we
-    stick to the order used in the scipy.integrate routines.
-    """
-    if scale:
-        fx = np.abs(x).max()
-        fy = np.abs(y).max()
-        sx = x / fx
-        sy = y / fy
-    else:
-        fx = fy = 1.0
-        sx, sy = x, y
-    # Area under unscaled y(x).
-    _area = func(sy, sx) * fx * fy
-    return y * area / _area
-
-
-def vacf(vel, m=None, method=3):
-    """Reference implementation for calculating the VACF of velocities in 3d
-    array `vel`. This is slow. Use for debugging only. For production, use
-    fvacf().
-
-    Parameters
-    ----------
-    vel : 3d array, (nstep, natoms, 3)
-        Atomic velocities.
-    m : 1d array (natoms,)
-        Atomic masses.
-    method : int
-        | 1 : 3 loops
-        | 2 : replace 1 inner loop
-        | 3 : replace 2 inner loops
-
-    Returns
-    -------
-    c : 1d array (nstep,)
-        VACF
-    """
-    natoms = vel.shape[1]
-    nstep = vel.shape[0]
-    c = np.zeros((nstep,), dtype=float)
-    if m is None:
-        m = np.ones((natoms,), dtype=float)
-    if method == 1:
-        # c(t) = <v(t0) v(t0 + t)> / <v(t0)**2> = C(t) / C(0)
-        #
-        # "displacements" `t'
-        for t in range(nstep):
-            # time origins t0 == j
-            for j in range(nstep - t):
-                for i in range(natoms):
-                    c[t] += np.dot(vel[j, i, :], vel[j + t, i, :]) * m[i]
-    elif method == 2:
-        # replace 1 inner loop
-        for t in range(nstep):
-            for j in range(nstep - t):
-                # (natoms, 3) * (natoms, 1) -> (natoms, 3)
-                c[t] += (vel[j, ...] * vel[j + t, ...] * m[:, None]).sum()
-    elif method == 3:
-        # replace 2 inner loops:
-        # (xx, natoms, 3) * (1, natoms, 1) -> (xx, natoms, 3)
-        for t in range(nstep):
-            c[t] = (vel[: (nstep - t), ...] * vel[t:, ...] * m[None, :, None]).sum()
-    else:
-        raise ValueError("unknown method: %s" % method)
-    # normalize to unity
-    c = c / c[0]
-    return c
-
-
-def pdos(
-    vel,
-    dt=1.0,
-    m=None,
-    full_out=False,
-    area=1.0,
-    window=True,
-    npad=None,
-    tonext=False,
-    mirr=False,
-    method="direct",
-):
-    """Phonon DOS by FFT of the VACF or direct FFT of atomic velocities.
-
-    Integral area is normalized to `area`. It is possible (and recommended) to
-    zero-padd the velocities (see `npad`).
-
-    Parameters
-    ----------
-    vel : 3d array (nstep, natoms, 3)
-        atomic velocities
-    dt : time step
-    m : 1d array (natoms,),
-        atomic mass array, if None then mass=1.0 for all atoms is used
-    full_out : bool
-    area : float
-        normalize area under frequency-PDOS curve to this value
-    window : bool
-        use Welch windowing on data before FFT (reduces leaking effect,
-        recommended)
-    npad : {None, int}
-        method='direct' only: Length of zero padding along `axis`. `npad=None`
-        = no padding, `npad > 0` = pad by a length of ``(nstep-1)*npad``. `npad
-        > 5` usually results in sufficient interpolation.
-    tonext : bool
-        method='direct' only: Pad `vel` with zeros along `axis` up to the next
-        power of two after the array length determined by `npad`. This gives
-        you speed, but variable (better) frequency resolution.
-    mirr : bool
-        method='vacf' only: mirror one-sided VACF at t=0 before fft
-
-    Returns
-    -------
-    if full_out = False
-        | ``(faxis, pdos)``
-        | faxis : 1d array [1/unit(dt)]
-        | pdos : 1d array, the phonon DOS, normalized to `area`
-    if full_out = True
-        | if method == 'direct':
-        |     ``(faxis, pdos, (full_faxis, full_pdos, split_idx))``
-        | if method == 'vavcf':
-        |     ``(faxis, pdos, (full_faxis, full_pdos, split_idx, vacf, fft_vacf))``
-        |     fft_vacf : 1d complex array, result of fft(vacf) or fft(mirror(vacf))
-        |     vacf : 1d array, the VACF
-
-    Notes
-    -----
-    padding (only method='direct'): With `npad` we pad the velocities `vel`
-    with ``npad*(nstep-1)`` zeros along `axis` (the time axis) before FFT
-    b/c the signal is not periodic. For `npad=1`, this gives us the exact
-    same spectrum and frequency resolution as with ``pdos(...,
-    method='vacf',mirr=True)`` b/c the array to be fft'ed has length
-    ``2*nstep-1`` along the time axis in both cases (remember that the
-    array length = length of the time axis influences the freq.
-    resolution). FFT is only fast for arrays with length = a power of two.
-    Therefore, you may get very different fft speeds depending on whether
-    ``2*nstep-1`` is a power of two or not (in most cases it won't). Try
-    using `tonext` but remember that you get another (better) frequency
-    resolution.
-
-    References
-    ----------
-    [1] Phys Rev B 47(9) 4863, 1993
-
-    See Also
-    --------
-    :func:`pwtools.signal.fftsample`
-    :func:`pwtools.signal.acorr`
-    :func:`direct_pdos`
-    :func:`vacf_pdos`
-
-    """
-    mass = m
-    # assume vel.shape = (nstep,natoms,3)
-    axis = 0
-    assert vel.shape[-1] == 3
-    if mass is not None:
-        assert len(mass) == vel.shape[1], "len(mass) != vel.shape[1]"
-        # define here b/c may be used twice below
-        mass_bc = mass[None, :, None]
-    if window:
-        sl = [None] * vel.ndim
-        sl[axis] = slice(None)  # ':'
-        vel2 = vel * (welch(vel.shape[axis])[tuple(sl)])
-    else:
-        vel2 = vel
-    # handle options which are mutually exclusive
-    if method == "vacf":
-        assert npad in [0, None], "use npad={0,None} for method='vacf'"
-    # padding
-    if npad is not None:
-        nadd = (vel2.shape[axis] - 1) * npad
-        if tonext:
-            vel2 = pad_zeros(
-                vel2, tonext=True, tonext_min=vel2.shape[axis] + nadd, axis=axis
-            )
-        else:
-            vel2 = pad_zeros(vel2, tonext=False, nadd=nadd, axis=axis)
-    if method == "direct":
-        full_fft_vel = np.abs(fft(vel2, axis=axis)) ** 2.0
-        full_faxis = np.fft.fftfreq(vel2.shape[axis], dt)
-        split_idx = len(full_faxis) // 2
-        faxis = full_faxis[:split_idx]
-        # First split the array, then multiply by `mass` and average. If
-        # full_out, then we need full_fft_vel below, so copy before slicing.
-        arr = full_fft_vel.copy() if full_out else full_fft_vel
-        # fft_vel = num.slicetake(arr, slice(0, split_idx), axis=axis, copy=False)
-        fft_vel = slicetake(arr, slice(0, split_idx), axis=axis, copy=False)
-        if mass is not None:
-            fft_vel *= mass_bc
-        # average remaining axes, summing is enough b/c normalization is done below
-        # sums: (nstep, natoms, 3) -> (nstep, natoms) -> (nstep,)
-        pdos = sum(fft_vel, axis=axis, keepdims=True)
-        default_out = (faxis, norm_int(pdos, faxis, area=area))
-        if full_out:
-            # have to re-calculate this here b/c we never calculate the full_pdos
-            # normally
-            if mass is not None:
-                full_fft_vel *= mass_bc
-            full_pdos = sum(full_fft_vel, axis=axis, keepdims=True)
-            extra_out = (full_faxis, full_pdos, split_idx)
-            return default_out + extra_out
-        else:
-            return default_out
-    elif method == "vacf":
-        # vacf = fvacf(vel2, m=mass)
-        vacf_ = vacf(vel2, m=mass)
-        if mirr:
-            fft_vacf = fft(mirror(vacf_))
-        else:
-            fft_vacf = fft(vacf_)
-        full_faxis = np.fft.fftfreq(fft_vacf.shape[axis], dt)
-        full_pdos = np.abs(fft_vacf)
-        split_idx = len(full_faxis) // 2
-        faxis = full_faxis[:split_idx]
-        pdos = full_pdos[:split_idx]
-        default_out = (faxis, norm_int(pdos, faxis, area=area))
-        extra_out = (full_faxis, full_pdos, split_idx, vacf_, fft_vacf)
-        if full_out:
-            return default_out + extra_out
-        else:
-            return default_out
+"""
+This module has not been tested yet, use at your own risk!
+"""
+
+import numpy as np
+from scipy.fftpack import fft
+from scipy.integrate import simps
+
+
+def welch(M, sym=1):
+    """Welch window. Function skeleton shamelessly stolen from
+    scipy.signal.bartlett() and others."""
+    if M < 1:
+        return np.array([])
+    if M == 1:
+        return np.ones(1, dtype=float)
+    odd = M % 2
+    if not sym and not odd:
+        M = M + 1
+    n = np.arange(0, M)
+    w = 1.0 - ((n - 0.5 * (M - 1)) / (0.5 * (M - 1))) ** 2.0
+    if not sym and not odd:
+        w = w[:-1]
+    return w
+
+
+def mirror(arr, axis=0):
+    """Mirror array `arr` at index 0 along `axis`.
+    The length of the returned array is 2*arr.shape[axis]-1 ."""
+    return np.concatenate((arr[::-1], arr[1:]), axis=axis)
+
+
+def pad_zeros(
+    arr, axis=0, where="end", nadd=None, upto=None, tonext=None, tonext_min=None
+):
+    """Pad an nd-array with zeros. Default is to append an array of zeros of
+    the same shape as `arr` to arr's end along `axis`.
+    Parameters
+    ----------
+    arr :  nd array
+    axis : the axis along which to pad
+    where : string {'end', 'start'}, pad at the end ("append to array") or
+        start ("prepend to array") of `axis`
+    nadd : number of items to padd (i.e. nadd=3 means padd w/ 3 zeros in case
+        of an 1d array)
+    upto : pad until arr.shape[axis] == upto
+    tonext : bool, pad up to the next power of two (pad so that the padded
+        array has a length of power of two)
+    tonext_min : int, when using `tonext`, pad the array to the next possible
+        power of two for which the resulting array length along `axis` is at
+        least `tonext_min`; the default is tonext_min = arr.shape[axis]
+    Use only one of nadd, upto, tonext.
+    Returns
+    -------
+    padded array
+    Examples
+    --------
+    >>> # 1d
+    >>> pad_zeros(a)
+    array([1, 2, 3, 0, 0, 0])
+    >>> pad_zeros(a, nadd=3)
+    array([1, 2, 3, 0, 0, 0])
+    >>> pad_zeros(a, upto=6)
+    array([1, 2, 3, 0, 0, 0])
+    >>> pad_zeros(a, nadd=1)
+    array([1, 2, 3, 0])
+    >>> pad_zeros(a, nadd=1, where='start')
+    array([0, 1, 2, 3])
+    >>> # 2d
+    >>> a=arange(9).reshape(3,3)
+    >>> pad_zeros(a, nadd=1, axis=0)
+    array([[0, 1, 2],
+           [3, 4, 5],
+           [6, 7, 8],
+           [0, 0, 0]])
+    >>> pad_zeros(a, nadd=1, axis=1)
+    array([[0, 1, 2, 0],
+           [3, 4, 5, 0],
+           [6, 7, 8, 0]])
+    >>> # up to next power of two
+    >>> 2**arange(10)
+    array([  1,   2,   4,   8,  16,  32,  64, 128, 256, 512])
+    >>> pydos.pad_zeros(arange(9), tonext=True).shape
+    (16,)
+    """
+    if tonext == False:
+        tonext = None
+    lst = [nadd, upto, tonext]
+    assert lst.count(None) in [2, 3], (
+        "`nadd`, `upto` and `tonext` must be " + "all None or only one of them not None"
+    )
+    if nadd is None:
+        if upto is None:
+            if (tonext is None) or (not tonext):
+                # default
+                nadd = arr.shape[axis]
+            else:
+                tonext_min = arr.shape[axis] if (tonext_min is None) else tonext_min
+                # beware of int overflows starting w/ 2**arange(64), but we
+                # will never have such long arrays anyway
+                two_powers = 2 ** np.arange(30)
+                assert tonext_min <= two_powers[-1], (
+                    "tonext_min exceeds " "max power of 2"
+                )
+                power = two_powers[np.searchsorted(two_powers, tonext_min)]
+                nadd = power - arr.shape[axis]
+        else:
+            nadd = upto - arr.shape[axis]
+    if nadd == 0:
+        return arr
+    add_shape = list(arr.shape)
+    add_shape[axis] = nadd
+    add_shape = tuple(add_shape)
+    if where == "end":
+        return np.concatenate((arr, np.zeros(add_shape, dtype=arr.dtype)), axis=axis)
+    elif where == "start":
+        return np.concatenate((np.zeros(add_shape, dtype=arr.dtype), arr), axis=axis)
+    else:
+        raise Exception("illegal `where` arg: %s" % where)
+
+
+def slicetake(a, sl, axis=None, copy=False):
+    """The equivalent of numpy.take(a, ..., axis=<axis>), but accepts slice
+    objects instead of an index array. Also by default, it returns a *view* and
+    no copy.
+    Parameters
+    ----------
+    a : numpy ndarray
+    sl : slice object, list or tuple of slice objects
+        axis=<int>
+            one slice object for *that* axis
+        axis=None
+            `sl` is a list or tuple of slice objects, one for each axis.
+            It must index the whole array, i.e. len(sl) == len(a.shape).
+    axis : {None, int}
+    copy : bool, return a copy instead of a view
+    Returns
+    -------
+    A view into `a` or copy of a slice of `a`.
+    Examples
+    --------
+    >>> from numpy import s_
+    >>> a = np.random.rand(20,20,20)
+    >>> b1 = a[:,:,10:]
+    >>> # single slice for axis 2
+    >>> b2 = slicetake(a, s_[10:], axis=2)
+    >>> # tuple of slice objects
+    >>> b3 = slicetake(a, s_[:,:,10:])
+    >>> (b2 == b1).all()
+    True
+    >>> (b3 == b1).all()
+    True
+    >>> # simple extraction too, sl = integer
+    >>> (a[...,5] == slicetake(a, 5, axis=-1))
+    True
+    """
+    # The long story
+    # --------------
+    #
+    # 1) Why do we need that:
+    #
+    # # no problem
+    # a[5:10:2]
+    #
+    # # the same, more general
+    # sl = slice(5,10,2)
+    # a[sl]
+    #
+    # But we want to:
+    #  - Define (type in) a slice object only once.
+    #  - Take the slice of different arrays along different axes.
+    # Since numpy.take() and a.take() don't handle slice objects, one would
+    # have to use direct slicing and pay attention to the shape of the array:
+    #
+    #     a[sl], b[:,:,sl,:], etc ...
+    #
+    # We want to use an 'axis' keyword instead. np.r_() generates index arrays
+    # from slice objects (e.g r_[1:5] == r_[s_[1:5] ==r_[slice(1,5,None)]).
+    # Since we need index arrays for numpy.take(), maybe we can use that? Like
+    # so:
+    #
+    #     a.take(r_[sl], axis=0)
+    #     b.take(r_[sl], axis=2)
+    #
+    # Here we have what we want: slice object + axis kwarg.
+    # But r_[slice(...)] does not work for all slice types. E.g. not for
+    #
+    #     r_[s_[::5]] == r_[slice(None, None, 5)] == array([], dtype=int32)
+    #     r_[::5]                                 == array([], dtype=int32)
+    #     r_[s_[1:]]  == r_[slice(1, None, None)] == array([0])
+    #     r_[1:]
+    #         ValueError: dimensions too large.
+    #
+    # The returned index arrays are wrong (or we even get an exception).
+    # The reason is given below.
+    # Bottom line: We need this function.
+    #
+    # The reason for r_[slice(...)] gererating sometimes wrong index arrays is
+    # that s_ translates a fancy index (1:, ::5, 1:10:2, ...) to a slice
+    # object. This *always* works. But since take() accepts only index arrays,
+    # we use r_[s_[<fancy_index>]], where r_ translates the slice object
+    # prodced by s_ to an index array. THAT works only if start and stop of the
+    # slice are known. r_ has no way of knowing the dimensions of the array to
+    # be sliced and so it can't transform a slice object into a correct index
+    # array in case of slice(<number>, None, None) or slice(None, None,
+    # <number>).
+    #
+    # 2) Slice vs. copy
+    #
+    # numpy.take(a, array([0,1,2,3])) or a[array([0,1,2,3])] return a copy of
+    # `a` b/c that's "fancy indexing". But a[slice(0,4,None)], which is the
+    # same as indexing (slicing) a[:4], return *views*.
+
+    if axis is None:
+        slices = sl
+    else:
+        # Note that these are equivalent:
+        #   a[:]
+        #   a[s_[:]]
+        #   a[slice(None)]
+        #   a[slice(None, None, None)]
+        #   a[slice(0, None, None)]
+        slices = [slice(None)] * a.ndim
+        slices[axis] = sl
+    # a[...] can take a tuple or list of slice objects
+    # a[x:y:z, i:j:k] is the same as
+    # a[(slice(x,y,z), slice(i,j,k))] == a[[slice(x,y,z), slice(i,j,k)]]
+    slices = tuple(slices)
+    if copy:
+        return a[slices].copy()
+    else:
+        return a[slices]
+
+
+def sum(arr, axis=None, keepdims=False, **kwds):
+    """This numpy.sum() with some features implemented which can be found in
+    numpy v1.7 and later: `axis` can be a tuple to select arbitrary axes to sum
+    over.
+    We also have a `keepdims` keyword, which however works completely different
+    from numpy. Docstrings shamelessly stolen from numpy and adapted here
+    and there.
+    Parameters
+    ----------
+    arr : nd array
+    axis : None or int or tuple of ints, optional
+        Axis or axes along which a sum is performed. The default (`axis` =
+        `None`) is to perform a sum over all the dimensions of the input array.
+        `axis` may be negative, in which case it counts from the last to the
+        first axis.
+        If this is a tuple of ints, a sum is performed on multiple
+        axes, instead of a single axis or all the axes as before.
+    keepdims : bool, optional
+        If this is set to True, the axes from `axis` are left in the result
+        and the reduction (sum) is performed for all remaining axes. Therefore,
+        it reverses the `axis` to be summed over.
+    **kwds : passed to np.sum().
+    Examples
+    --------
+    >>> a=rand(2,3,4)
+    >>> num.sum(a)
+    12.073636268676152
+    >>> a.sum()
+    12.073636268676152
+    >>> num.sum(a, axis=1).shape
+    (2, 4)
+    >>> num.sum(a, axis=(1,)).shape
+    (2, 4)
+    >>> # same as axis=1, i.e. it inverts the axis over which we sum
+    >>> num.sum(a, axis=(0,2), keepdims=True).shape
+    (2, 4)
+    >>> # numpy's keepdims has another meaning: it leave the summed axis (0,2)
+    >>> # as dimension of size 1 to allow broadcasting
+    >>> numpy.sum(a, axis=(0,2), keepdims=True).shape
+    (1, 3, 1)
+    >>> num.sum(a, axis=(1,)) - num.sum(a, axis=1)
+    array([[ 0.,  0.,  0.,  0.],
+           [ 0.,  0.,  0.,  0.]])
+    >>> num.sum(a, axis=(0,2)).shape
+    (3,)
+    >>> num.sum(a, axis=(0,2)) - a.sum(axis=0).sum(axis=1)
+    array([ 0.,  0.,  0.])
+    """
+
+    # Recursion rocks!
+    def _sum(arr, tosum):
+        if len(tosum) > 0:
+            # Choose axis to sum over, remove from list w/ remaining axes.
+            axis = tosum.pop(0)
+            _arr = arr.sum(axis=axis)
+            # arr has one dim less now. Rename remaining axes accordingly.
+            _tosum = [xx - 1 if xx > axis else xx for xx in tosum]
+            return _sum(_arr, _tosum)
+        else:
+            return arr
+
+    axis_is_int = isinstance(axis, int)
+    if axis is None:
+        if keepdims:
+            raise Exception("axis=None + keepdims=True makes no sense")
+        else:
+            return np.sum(arr, axis=axis, **kwds)
+    elif axis_is_int and not keepdims:
+        return np.sum(arr, axis=axis, **kwds)
+    else:
+        if axis_is_int:
+            tosum = [axis]
+        elif isinstance(axis, tuple) or isinstance(axis, list):
+            tosum = list(axis)
+        else:
+            raise Exception("illegal type for axis: %s" % str(type(axis)))
+        if keepdims:
+            alldims = range(arr.ndim)
+            tosum = [xx for xx in alldims if xx not in tosum]
+        return _sum(arr, tosum)
+
+
+def norm_int(y, x, area=1.0, scale=True, func=simps):
+    """Normalize integral area of y(x) to `area`.
+    Parameters
+    ----------
+    x,y : numpy 1d arrays
+    area : float
+    scale : bool, optional
+        Scale x and y to the same order of magnitude before integration.
+        This may be necessary to avoid numerical trouble if x and y have very
+        different scales.
+    func : callable
+        Function to do integration (like scipy.integrate.{simps,trapz,...}
+        Called as ``func(y,x)``. Default: simps
+    Returns
+    -------
+    scaled y
+    Notes
+    -----
+    The argument order y,x might be confusing. x,y would be more natural but we
+    stick to the order used in the scipy.integrate routines.
+    """
+    if scale:
+        fx = np.abs(x).max()
+        fy = np.abs(y).max()
+        sx = x / fx
+        sy = y / fy
+    else:
+        fx = fy = 1.0
+        sx, sy = x, y
+    # Area under unscaled y(x).
+    _area = func(sy, sx) * fx * fy
+    return y * area / _area
+
+
+def vacf(vel, m=None, method=3):
+    """Reference implementation for calculating the VACF of velocities in 3d
+    array `vel`. This is slow. Use for debugging only. For production, use
+    fvacf().
+
+    Parameters
+    ----------
+    vel : 3d array, (nstep, natoms, 3)
+        Atomic velocities.
+    m : 1d array (natoms,)
+        Atomic masses.
+    method : int
+        | 1 : 3 loops
+        | 2 : replace 1 inner loop
+        | 3 : replace 2 inner loops
+
+    Returns
+    -------
+    c : 1d array (nstep,)
+        VACF
+    """
+    natoms = vel.shape[1]
+    nstep = vel.shape[0]
+    c = np.zeros((nstep,), dtype=float)
+    if m is None:
+        m = np.ones((natoms,), dtype=float)
+    if method == 1:
+        # c(t) = <v(t0) v(t0 + t)> / <v(t0)**2> = C(t) / C(0)
+        #
+        # "displacements" `t'
+        for t in range(nstep):
+            # time origins t0 == j
+            for j in range(nstep - t):
+                for i in range(natoms):
+                    c[t] += np.dot(vel[j, i, :], vel[j + t, i, :]) * m[i]
+    elif method == 2:
+        # replace 1 inner loop
+        for t in range(nstep):
+            for j in range(nstep - t):
+                # (natoms, 3) * (natoms, 1) -> (natoms, 3)
+                c[t] += (vel[j, ...] * vel[j + t, ...] * m[:, None]).sum()
+    elif method == 3:
+        # replace 2 inner loops:
+        # (xx, natoms, 3) * (1, natoms, 1) -> (xx, natoms, 3)
+        for t in range(nstep):
+            c[t] = (vel[: (nstep - t), ...] * vel[t:, ...] * m[None, :, None]).sum()
+    else:
+        raise ValueError("unknown method: %s" % method)
+    # normalize to unity
+    c = c / c[0]
+    return c
+
+
+def pdos(
+    vel,
+    dt=1.0,
+    m=None,
+    full_out=False,
+    area=1.0,
+    window=True,
+    npad=None,
+    tonext=False,
+    mirr=False,
+    method="direct",
+):
+    """Phonon DOS by FFT of the VACF or direct FFT of atomic velocities.
+
+    Integral area is normalized to `area`. It is possible (and recommended) to
+    zero-padd the velocities (see `npad`).
+
+    Parameters
+    ----------
+    vel : 3d array (nstep, natoms, 3)
+        atomic velocities
+    dt : time step
+    m : 1d array (natoms,),
+        atomic mass array, if None then mass=1.0 for all atoms is used
+    full_out : bool
+    area : float
+        normalize area under frequency-PDOS curve to this value
+    window : bool
+        use Welch windowing on data before FFT (reduces leaking effect,
+        recommended)
+    npad : {None, int}
+        method='direct' only: Length of zero padding along `axis`. `npad=None`
+        = no padding, `npad > 0` = pad by a length of ``(nstep-1)*npad``. `npad
+        > 5` usually results in sufficient interpolation.
+    tonext : bool
+        method='direct' only: Pad `vel` with zeros along `axis` up to the next
+        power of two after the array length determined by `npad`. This gives
+        you speed, but variable (better) frequency resolution.
+    mirr : bool
+        method='vacf' only: mirror one-sided VACF at t=0 before fft
+
+    Returns
+    -------
+    if full_out = False
+        | ``(faxis, pdos)``
+        | faxis : 1d array [1/unit(dt)]
+        | pdos : 1d array, the phonon DOS, normalized to `area`
+    if full_out = True
+        | if method == 'direct':
+        |     ``(faxis, pdos, (full_faxis, full_pdos, split_idx))``
+        | if method == 'vavcf':
+        |     ``(faxis, pdos, (full_faxis, full_pdos, split_idx, vacf, fft_vacf))``
+        |     fft_vacf : 1d complex array, result of fft(vacf) or fft(mirror(vacf))
+        |     vacf : 1d array, the VACF
+
+    Notes
+    -----
+    padding (only method='direct'): With `npad` we pad the velocities `vel`
+    with ``npad*(nstep-1)`` zeros along `axis` (the time axis) before FFT
+    b/c the signal is not periodic. For `npad=1`, this gives us the exact
+    same spectrum and frequency resolution as with ``pdos(...,
+    method='vacf',mirr=True)`` b/c the array to be fft'ed has length
+    ``2*nstep-1`` along the time axis in both cases (remember that the
+    array length = length of the time axis influences the freq.
+    resolution). FFT is only fast for arrays with length = a power of two.
+    Therefore, you may get very different fft speeds depending on whether
+    ``2*nstep-1`` is a power of two or not (in most cases it won't). Try
+    using `tonext` but remember that you get another (better) frequency
+    resolution.
+
+    References
+    ----------
+    [1] Phys Rev B 47(9) 4863, 1993
+
+    See Also
+    --------
+    :func:`pwtools.signal.fftsample`
+    :func:`pwtools.signal.acorr`
+    :func:`direct_pdos`
+    :func:`vacf_pdos`
+
+    """
+    mass = m
+    # assume vel.shape = (nstep,natoms,3)
+    axis = 0
+    assert vel.shape[-1] == 3
+    if mass is not None:
+        assert len(mass) == vel.shape[1], "len(mass) != vel.shape[1]"
+        # define here b/c may be used twice below
+        mass_bc = mass[None, :, None]
+    if window:
+        sl = [None] * vel.ndim
+        sl[axis] = slice(None)  # ':'
+        vel2 = vel * (welch(vel.shape[axis])[tuple(sl)])
+    else:
+        vel2 = vel
+    # handle options which are mutually exclusive
+    if method == "vacf":
+        assert npad in [0, None], "use npad={0,None} for method='vacf'"
+    # padding
+    if npad is not None:
+        nadd = (vel2.shape[axis] - 1) * npad
+        if tonext:
+            vel2 = pad_zeros(
+                vel2, tonext=True, tonext_min=vel2.shape[axis] + nadd, axis=axis
+            )
+        else:
+            vel2 = pad_zeros(vel2, tonext=False, nadd=nadd, axis=axis)
+    if method == "direct":
+        full_fft_vel = np.abs(fft(vel2, axis=axis)) ** 2.0
+        full_faxis = np.fft.fftfreq(vel2.shape[axis], dt)
+        split_idx = len(full_faxis) // 2
+        faxis = full_faxis[:split_idx]
+        # First split the array, then multiply by `mass` and average. If
+        # full_out, then we need full_fft_vel below, so copy before slicing.
+        arr = full_fft_vel.copy() if full_out else full_fft_vel
+        # fft_vel = num.slicetake(arr, slice(0, split_idx), axis=axis, copy=False)
+        fft_vel = slicetake(arr, slice(0, split_idx), axis=axis, copy=False)
+        if mass is not None:
+            fft_vel *= mass_bc
+        # average remaining axes, summing is enough b/c normalization is done below
+        # sums: (nstep, natoms, 3) -> (nstep, natoms) -> (nstep,)
+        pdos = sum(fft_vel, axis=axis, keepdims=True)
+        default_out = (faxis, norm_int(pdos, faxis, area=area))
+        if full_out:
+            # have to re-calculate this here b/c we never calculate the full_pdos
+            # normally
+            if mass is not None:
+                full_fft_vel *= mass_bc
+            full_pdos = sum(full_fft_vel, axis=axis, keepdims=True)
+            extra_out = (full_faxis, full_pdos, split_idx)
+            return default_out + extra_out
+        else:
+            return default_out
+    elif method == "vacf":
+        # vacf = fvacf(vel2, m=mass)
+        vacf_ = vacf(vel2, m=mass)
+        if mirr:
+            fft_vacf = fft(mirror(vacf_))
+        else:
+            fft_vacf = fft(vacf_)
+        full_faxis = np.fft.fftfreq(fft_vacf.shape[axis], dt)
+        full_pdos = np.abs(fft_vacf)
+        split_idx = len(full_faxis) // 2
+        faxis = full_faxis[:split_idx]
+        pdos = full_pdos[:split_idx]
+        default_out = (faxis, norm_int(pdos, faxis, area=area))
+        extra_out = (full_faxis, full_pdos, split_idx, vacf_, fft_vacf)
+        if full_out:
+            return default_out + extra_out
+        else:
+            return default_out
```

## dspawpy/diffusion/neb.py

 * *Ordering differences only*

```diff
@@ -1,295 +1,295 @@
-# -*- coding: utf-8 -*-
-import json
-import os
-from typing import List
-import numpy as np
-import matplotlib.pyplot as plt
-from scipy.interpolate import interp1d
-from pymatgen.core import Structure
-from dspawpy.io.read import load_h5
-from dspawpy.io.structure import to_file
-from dspawpy.diffusion.pathfinder import IDPPSolver
-
-
-class NEB:
-    """
-
-    Parameters
-    ----------
-    initial_structure: Structure
-        初态
-    final_structure: Structure
-        终态
-    nimages: int
-        中间构型数
-
-    Examples
-    --------
-    >>> from pymatgen.core import Structure
-    # 先调用pymatgen写好的from_file方法读取cif构型文件，生成structure对象
-    >>> initial_structure = Structure.from_file("initial.cif")
-    >>> final_structure = Structure.from_file("final.cif")
-    # 初始化对象，后续可以调用两个插值算法，详见下方write_neb_structures
-    >>> from dspawpy.diffusion.neb import NEB
-    >>> neb = NEB(initial_structure,final_structure,10)
-    """
-
-    def __init__(self, initial_structure, final_structure, nimages):
-        """
-
-        Args:
-            initial_structure:
-            final_structure:
-            nimages: number of images,contain initial and final structure
-        """
-
-        self.nimages = nimages
-        self.iddp = IDPPSolver.from_endpoints(
-            endpoints=[initial_structure, final_structure], nimages=self.nimages - 2
-        )
-
-    def linear_interpolate(self):
-        return self.iddp.structures
-
-    def idpp_interpolate(
-        self,
-        maxiter=1000,
-        tol=1e-5,
-        gtol=1e-3,
-        step_size=0.05,
-        max_disp=0.05,
-        spring_const=5.0,
-    ):
-        return self.iddp.run(maxiter, tol, gtol, step_size, max_disp, spring_const)
-
-
-def plot_neb_barrier(
-    datafile: str,
-    ri: float = None,
-    rf: float = None,
-    ei: float = None,
-    ef: float = None,
-    show: bool = True,
-    figname: str = "neb_reaction_coordinate.png",
-):
-    """根据neb.json或者neb.h5绘制能垒图
-
-    如果NEB任务不计算初末态的自洽，这两个文件中将缺失相关信息，需要手动输入
-
-    Parameters
-    ----------
-    datafile : str
-        neb.json或neb.h5文件路径
-    ri : float
-        初态反应坐标
-    rf : float
-        末态反应坐标
-    ei : float
-        初态自洽能量
-    ef : float
-        末态自洽能量
-    show : bool, optional
-        是否展示交互式绘图窗口, 默认展示
-    figname : str
-        保存的图片名称, 默认'neb_reaction_coordinate.png'
-
-    Returns
-    -------
-    NEB能垒图
-
-    Examples
-    --------
-    >>> from dspawpy.diffusion.neb import plot_neb_barrier
-    >>> plot_neb_barrier("neb.h5") # neb.iniFin = true
-    >>> plot_neb_barrier("neb.h5",ri=0.0,rf=1.0,ei=-1.0,ef=-2.0) # neb.iniFin = false
-    """
-    # search datafile in the given directory
-    if os.path.isdir(datafile):
-        directory = datafile  # specified datafile is actually a directory
-        print("您指定了一个文件夹，正在查找相关h5或json文件...")
-        if os.path.exists(os.path.join(directory, "neb.h5")):
-            datafile = os.path.join(directory, "neb.h5")
-            print("Reading neb.h5...")
-        elif os.path.exists(os.path.join(directory, "neb.json")):
-            datafile = os.path.join(directory, "neb.json")
-            print("Reading neb.json...")
-        else:
-            raise FileNotFoundError("未找到neb.h5/neb.json文件！")
-
-    if datafile.endswith(".h5"):
-        neb = load_h5(datafile)
-        if "/BarrierInfo/ReactionCoordinate" in neb.keys():
-            reaction_coordinate = neb["/BarrierInfo/ReactionCoordinate"]
-            energy = neb["/BarrierInfo/TotalEnergy"]
-        else:  # old version
-            reaction_coordinate = neb["/Distance/ReactionCoordinate"]
-            energy = neb["/Energy/TotalEnergy"]
-    elif datafile.endswith(".json"):
-        with open(datafile, "r") as fin:
-            neb = json.load(fin)
-        if "BarrierInfo" in neb.keys():
-            reaction_coordinate = neb["BarrierInfo"]["ReactionCoordinate"]
-            energy = neb["BarrierInfo"]["TotalEnergy"]
-        else:  # old version
-            reaction_coordinate = neb["Distance"]["ReactionCoordinate"]
-            energy = neb["Energy"]["TotalEnergy"]
-    else:
-        raise TypeError("仅支持读取h5或json文件！")
-
-    x = []
-    for c in reaction_coordinate:
-        if len(x) > 0:
-            x.append(x[-1] + c)
-        else:
-            x.append(c)
-
-    y = [x - energy[0] for x in energy]
-    # initial and final info
-    if ri is not None:  # add initial reaction coordinate
-        x.insert(0, ri)
-    if rf is not None:  # add final reaction coordinate
-        x.append(rf)
-
-    if ei is not None:  # add initial energy
-        y.insert(0, ei)
-    if ef is not None:  # add final energy
-        y.append(ef)
-
-    inter_f = interp1d(x, y, kind="cubic")
-    xnew = np.linspace(x[0], x[-1], 100)
-    ynew = inter_f(xnew)
-
-    plt.plot(xnew, ynew, c="b")
-    plt.scatter(x, y, c="r")
-    plt.xlabel("Reaction Coordinate")
-    plt.ylabel("Energy")
-
-    if figname:
-        plt.tight_layout()
-        plt.savefig(figname)
-    if show:
-        plt.show()
-
-
-def write_neb_structures(
-    structures: List[Structure],
-    coords_are_cartesian=True,
-    fmt: str = "json",
-    path: str = ".",
-    prefix="structure",
-):
-    """插值并生成中间构型文件
-
-    Parameters
-    ----------
-    structures: list
-        构型列表
-    coords_are_cartesian: bool
-        坐标是否为笛卡尔坐标
-    fmt: str
-        结构文件类型，支持 "json", "as", "poscar", "hzw"
-    path: str
-        保存路径
-
-    Returns
-    -------
-    file
-        保存构型文件
-
-    Examples
-    --------
-    >>> from pymatgen.core import Structure
-    # 先调用pymatgen写好的from_file方法读取cif构型文件，生成structure对象
-    >>> initial_structure = Structure.from_file("initial.cif")
-    >>> final_structure = Structure.from_file("final.cif")
-    # 插值并生成中间构型文件
-    >>> from dspawpy.diffusion.neb import NEB,write_neb_structures
-    >>> neb = NEB(initial_structure,final_structure,10)
-    >>> neb = NEB(init_struct,final_struct,5) # 设置插点个数
-    >>> structures = neb.idpp_interpolate() # idpp插值，生成中间构型
-    # 可指定保存到neb文件夹下
-    >>> write_neb_structures(structures,coords_are_cartesian=True,fmt="as",path="neb")
-    """
-    N = len(str(len(structures)))
-    if N <= 2:
-        N = 2
-    for i, structure in enumerate(structures):
-        path_name = str(i).zfill(N)
-        os.makedirs(os.path.join(path, path_name), exist_ok=True)
-        if fmt == "poscar":
-            structure.to(fmt="poscar", filename=os.path.join(path, path_name, "POSCAR"))
-        else:
-            filename = os.path.join(
-                path, path_name, "%s%s.%s" % (prefix, path_name, fmt)
-            )
-            to_file(
-                structure, filename, coords_are_cartesian=coords_are_cartesian, fmt=fmt
-            )
-
-
-def plot_neb_converge(neb_dir: str, image_key="01"):
-    # for compatibility
-    if neb_dir.endswith(".h5"):
-        neb_total = load_h5(neb_dir)
-        maxforce = np.array(neb_total["/Iteration/" + image_key + "/MaxForce"])
-        total_energy = np.array(neb_total["/Iteration/" + image_key + "/TotalEnergy"])
-    elif neb_dir.endswith(".json"):
-        with open(neb_dir, "r") as fin:
-            neb_total = json.load(fin)
-        try:
-            neb = neb_total["LoopInfo"][image_key]
-        except:
-            neb = neb_total["Iteration"][image_key]
-        maxforce = []
-        total_energy = []
-        for n in neb:
-            maxforce.append(n["MaxForce"])
-            total_energy.append(n["TotalEnergy"])
-
-    elif os.path.exists(f"{neb_dir}/neb.h5"):
-        neb_total = load_h5(f"{neb_dir}/neb.h5")
-        maxforce = np.array(neb_total["/Iteration/" + image_key + "/MaxForce"])
-        total_energy = np.array(neb_total["/Iteration/" + image_key + "/TotalEnergy"])
-
-    elif os.path.exists(f"{neb_dir}/neb.json"):
-        with open(f"{neb_dir}/neb.json", "r") as fin:
-            neb_total = json.load(fin)
-        try:
-            neb = neb_total["LoopInfo"][image_key]
-        except:
-            neb = neb_total["Iteration"][image_key]
-        maxforce = []
-        total_energy = []
-        for n in neb:
-            print(n)
-            maxforce.append(n["MaxForce"])
-            total_energy.append(n["TotalEnergy"])
-
-        maxforce = np.array(maxforce)
-        total_energy = np.array(total_energy)
-
-    else:
-        print(f"{neb_dir}路径中找不到neb.h5或者neb.json文件")
-
-    x = np.arange(len(maxforce))
-
-    force = maxforce
-    energy = total_energy
-
-    fig = plt.figure()
-    ax1 = fig.add_subplot(111)
-
-    ax1.plot(x, force, label="Max Force", c="black")
-    ax1.set_xlabel("Number of ionic step")
-    ax1.set_ylabel("Force")
-
-    ax2 = ax1.twinx()
-    ax2.plot(x, energy, label="Energy", c="r")
-    ax2.set_xlabel("Number of ionic step")
-    ax2.set_ylabel("Energy")
-    ax2.ticklabel_format(useOffset=False)  # y轴坐标显示绝对值而不是相对值
-
-    fig.legend(loc=1, bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)
-    plt.tight_layout()
-
-    return ax1, ax2
+# -*- coding: utf-8 -*-
+import json
+import os
+from typing import List
+import numpy as np
+import matplotlib.pyplot as plt
+from scipy.interpolate import interp1d
+from pymatgen.core import Structure
+from dspawpy.io.read import load_h5
+from dspawpy.io.structure import to_file
+from dspawpy.diffusion.pathfinder import IDPPSolver
+
+
+class NEB:
+    """
+
+    Parameters
+    ----------
+    initial_structure: Structure
+        初态
+    final_structure: Structure
+        终态
+    nimages: int
+        中间构型数
+
+    Examples
+    --------
+    >>> from pymatgen.core import Structure
+    # 先调用pymatgen写好的from_file方法读取cif构型文件，生成structure对象
+    >>> initial_structure = Structure.from_file("initial.cif")
+    >>> final_structure = Structure.from_file("final.cif")
+    # 初始化对象，后续可以调用两个插值算法，详见下方write_neb_structures
+    >>> from dspawpy.diffusion.neb import NEB
+    >>> neb = NEB(initial_structure,final_structure,10)
+    """
+
+    def __init__(self, initial_structure, final_structure, nimages):
+        """
+
+        Args:
+            initial_structure:
+            final_structure:
+            nimages: number of images,contain initial and final structure
+        """
+
+        self.nimages = nimages
+        self.iddp = IDPPSolver.from_endpoints(
+            endpoints=[initial_structure, final_structure], nimages=self.nimages - 2
+        )
+
+    def linear_interpolate(self):
+        return self.iddp.structures
+
+    def idpp_interpolate(
+        self,
+        maxiter=1000,
+        tol=1e-5,
+        gtol=1e-3,
+        step_size=0.05,
+        max_disp=0.05,
+        spring_const=5.0,
+    ):
+        return self.iddp.run(maxiter, tol, gtol, step_size, max_disp, spring_const)
+
+
+def plot_neb_barrier(
+    datafile: str,
+    ri: float = None,
+    rf: float = None,
+    ei: float = None,
+    ef: float = None,
+    show: bool = True,
+    figname: str = "neb_reaction_coordinate.png",
+):
+    """根据neb.json或者neb.h5绘制能垒图
+
+    如果NEB任务不计算初末态的自洽，这两个文件中将缺失相关信息，需要手动输入
+
+    Parameters
+    ----------
+    datafile : str
+        neb.json或neb.h5文件路径
+    ri : float
+        初态反应坐标
+    rf : float
+        末态反应坐标
+    ei : float
+        初态自洽能量
+    ef : float
+        末态自洽能量
+    show : bool, optional
+        是否展示交互式绘图窗口, 默认展示
+    figname : str
+        保存的图片名称, 默认'neb_reaction_coordinate.png'
+
+    Returns
+    -------
+    NEB能垒图
+
+    Examples
+    --------
+    >>> from dspawpy.diffusion.neb import plot_neb_barrier
+    >>> plot_neb_barrier("neb.h5") # neb.iniFin = true
+    >>> plot_neb_barrier("neb.h5",ri=0.0,rf=1.0,ei=-1.0,ef=-2.0) # neb.iniFin = false
+    """
+    # search datafile in the given directory
+    if os.path.isdir(datafile):
+        directory = datafile  # specified datafile is actually a directory
+        print("您指定了一个文件夹，正在查找相关h5或json文件...")
+        if os.path.exists(os.path.join(directory, "neb.h5")):
+            datafile = os.path.join(directory, "neb.h5")
+            print("Reading neb.h5...")
+        elif os.path.exists(os.path.join(directory, "neb.json")):
+            datafile = os.path.join(directory, "neb.json")
+            print("Reading neb.json...")
+        else:
+            raise FileNotFoundError("未找到neb.h5/neb.json文件！")
+
+    if datafile.endswith(".h5"):
+        neb = load_h5(datafile)
+        if "/BarrierInfo/ReactionCoordinate" in neb.keys():
+            reaction_coordinate = neb["/BarrierInfo/ReactionCoordinate"]
+            energy = neb["/BarrierInfo/TotalEnergy"]
+        else:  # old version
+            reaction_coordinate = neb["/Distance/ReactionCoordinate"]
+            energy = neb["/Energy/TotalEnergy"]
+    elif datafile.endswith(".json"):
+        with open(datafile, "r") as fin:
+            neb = json.load(fin)
+        if "BarrierInfo" in neb.keys():
+            reaction_coordinate = neb["BarrierInfo"]["ReactionCoordinate"]
+            energy = neb["BarrierInfo"]["TotalEnergy"]
+        else:  # old version
+            reaction_coordinate = neb["Distance"]["ReactionCoordinate"]
+            energy = neb["Energy"]["TotalEnergy"]
+    else:
+        raise TypeError("仅支持读取h5或json文件！")
+
+    x = []
+    for c in reaction_coordinate:
+        if len(x) > 0:
+            x.append(x[-1] + c)
+        else:
+            x.append(c)
+
+    y = [x - energy[0] for x in energy]
+    # initial and final info
+    if ri is not None:  # add initial reaction coordinate
+        x.insert(0, ri)
+    if rf is not None:  # add final reaction coordinate
+        x.append(rf)
+
+    if ei is not None:  # add initial energy
+        y.insert(0, ei)
+    if ef is not None:  # add final energy
+        y.append(ef)
+
+    inter_f = interp1d(x, y, kind="cubic")
+    xnew = np.linspace(x[0], x[-1], 100)
+    ynew = inter_f(xnew)
+
+    plt.plot(xnew, ynew, c="b")
+    plt.scatter(x, y, c="r")
+    plt.xlabel("Reaction Coordinate")
+    plt.ylabel("Energy")
+
+    if figname:
+        plt.tight_layout()
+        plt.savefig(figname)
+    if show:
+        plt.show()
+
+
+def write_neb_structures(
+    structures: List[Structure],
+    coords_are_cartesian=True,
+    fmt: str = "json",
+    path: str = ".",
+    prefix="structure",
+):
+    """插值并生成中间构型文件
+
+    Parameters
+    ----------
+    structures: list
+        构型列表
+    coords_are_cartesian: bool
+        坐标是否为笛卡尔坐标
+    fmt: str
+        结构文件类型，支持 "json", "as", "poscar", "hzw"
+    path: str
+        保存路径
+
+    Returns
+    -------
+    file
+        保存构型文件
+
+    Examples
+    --------
+    >>> from pymatgen.core import Structure
+    # 先调用pymatgen写好的from_file方法读取cif构型文件，生成structure对象
+    >>> initial_structure = Structure.from_file("initial.cif")
+    >>> final_structure = Structure.from_file("final.cif")
+    # 插值并生成中间构型文件
+    >>> from dspawpy.diffusion.neb import NEB,write_neb_structures
+    >>> neb = NEB(initial_structure,final_structure,10)
+    >>> neb = NEB(init_struct,final_struct,5) # 设置插点个数
+    >>> structures = neb.idpp_interpolate() # idpp插值，生成中间构型
+    # 可指定保存到neb文件夹下
+    >>> write_neb_structures(structures,coords_are_cartesian=True,fmt="as",path="neb")
+    """
+    N = len(str(len(structures)))
+    if N <= 2:
+        N = 2
+    for i, structure in enumerate(structures):
+        path_name = str(i).zfill(N)
+        os.makedirs(os.path.join(path, path_name), exist_ok=True)
+        if fmt == "poscar":
+            structure.to(fmt="poscar", filename=os.path.join(path, path_name, "POSCAR"))
+        else:
+            filename = os.path.join(
+                path, path_name, "%s%s.%s" % (prefix, path_name, fmt)
+            )
+            to_file(
+                structure, filename, coords_are_cartesian=coords_are_cartesian, fmt=fmt
+            )
+
+
+def plot_neb_converge(neb_dir: str, image_key="01"):
+    # for compatibility
+    if neb_dir.endswith(".h5"):
+        neb_total = load_h5(neb_dir)
+        maxforce = np.array(neb_total["/Iteration/" + image_key + "/MaxForce"])
+        total_energy = np.array(neb_total["/Iteration/" + image_key + "/TotalEnergy"])
+    elif neb_dir.endswith(".json"):
+        with open(neb_dir, "r") as fin:
+            neb_total = json.load(fin)
+        try:
+            neb = neb_total["LoopInfo"][image_key]
+        except:
+            neb = neb_total["Iteration"][image_key]
+        maxforce = []
+        total_energy = []
+        for n in neb:
+            maxforce.append(n["MaxForce"])
+            total_energy.append(n["TotalEnergy"])
+
+    elif os.path.exists(f"{neb_dir}/neb.h5"):
+        neb_total = load_h5(f"{neb_dir}/neb.h5")
+        maxforce = np.array(neb_total["/Iteration/" + image_key + "/MaxForce"])
+        total_energy = np.array(neb_total["/Iteration/" + image_key + "/TotalEnergy"])
+
+    elif os.path.exists(f"{neb_dir}/neb.json"):
+        with open(f"{neb_dir}/neb.json", "r") as fin:
+            neb_total = json.load(fin)
+        try:
+            neb = neb_total["LoopInfo"][image_key]
+        except:
+            neb = neb_total["Iteration"][image_key]
+        maxforce = []
+        total_energy = []
+        for n in neb:
+            print(n)
+            maxforce.append(n["MaxForce"])
+            total_energy.append(n["TotalEnergy"])
+
+        maxforce = np.array(maxforce)
+        total_energy = np.array(total_energy)
+
+    else:
+        print(f"{neb_dir}路径中找不到neb.h5或者neb.json文件")
+
+    x = np.arange(len(maxforce))
+
+    force = maxforce
+    energy = total_energy
+
+    fig = plt.figure()
+    ax1 = fig.add_subplot(111)
+
+    ax1.plot(x, force, label="Max Force", c="black")
+    ax1.set_xlabel("Number of ionic step")
+    ax1.set_ylabel("Force")
+
+    ax2 = ax1.twinx()
+    ax2.plot(x, energy, label="Energy", c="r")
+    ax2.set_xlabel("Number of ionic step")
+    ax2.set_ylabel("Energy")
+    ax2.ticklabel_format(useOffset=False)  # y轴坐标显示绝对值而不是相对值
+
+    fig.legend(loc=1, bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)
+    plt.tight_layout()
+
+    return ax1, ax2
```

## dspawpy/diffusion/nebtools.py

```diff
@@ -1,1270 +1,1271 @@
-import os
-import h5py
-import json
-import pandas as pd
-import numpy as np
-
-np.set_printoptions(suppress=True)  # 不使用科学计数法
-from dspawpy.io.utils import get_pos_ele_lat, get_ele_from_h5
-import matplotlib.pyplot as plt
-
-
-def get_distance(
-    spo1: np.ndarray, spo2: np.ndarray, lat1: np.ndarray, lat2: np.ndarray
-):
-    """根据两个结构的分数坐标和晶胞计算距离
-
-    Parameters
-    ----------
-    spo1 : np.ndarray
-        分数坐标列表1
-    spo2 : np.ndarray
-        分数坐标列表2
-    lat1 : np.ndarray
-        晶胞1
-    lat2 : np.ndarray
-        晶胞2
-
-    Returns
-    -------
-    float
-        距离
-
-    Examples
-    --------
-    >>> from dspawpy.diffusion.nebtools import get_distance
-    >>> from dspawpy.io.utils import get_spo_ele_lat
-    >>> # 先读取两个构型的分数坐标、元素列表和晶胞信息
-    >>> spo1, ele1, lat1 = get_spo_ele_lat('structure01.as')
-    >>> spo1
-    array([[0.25      , 0.25      , 0.11784996],
-           [0.75      , 0.25      , 0.11784996],
-           ...，
-           [0.04062186, 0.45937779, 0.43038044]])
-    >>> ele1
-    ['Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt']
-    >>> lat1
-    array([[ 5.6058,  0.    ,  0.    ],
-           [ 0.    ,  5.6058,  0.    ],
-           [ 0.    ,  0.    , 16.8174]])
-    >>> spo2, ele2, lat2 = get_spo_ele_lat('structure02.as')
-    >>> spo2
-    array([[0.25      , 0.25      , 0.11784996],
-           ...，
-           [0.08124389, 0.41875557, 0.41408303]])
-    >>> ele2
-    ['Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt']
-    >>> lat2
-    array([[ 5.6058,  0.    ,  0.    ],
-           [ 0.    ,  5.6058,  0.    ],
-           [ 0.    ,  0.    , 16.8174]])
-    >>> # 计算两个构型的距离
-    >>> dist = get_distance(spo1, spo2, lat1, lat2)
-    >>> dist
-    0.5987379724194888
-    """
-    diff_spo = spo1 - spo2  # 分数坐标差
-    avglatv = 0.5 * (lat1 + lat2)  # 平均晶格矢量
-    pbc_diff_spo = set_pbc(diff_spo)  # 笛卡尔坐标差
-    # 分数坐标点乘平均晶胞，转回笛卡尔坐标
-    pbc_diff_pos = np.dot(pbc_diff_spo, avglatv)  # 笛卡尔坐标差
-    distance = np.sqrt(np.sum(pbc_diff_pos**2))
-
-    return distance
-
-
-def get_neb_subfolders(directory: str = "."):
-    """将directory路径下的子文件夹名称列表按照数字大小排序
-
-    仅保留形如00，01数字类型的NEB子文件夹路径
-
-    Parameters
-    ----------
-    subfolders : list
-        子文件夹名称列表
-
-    Returns
-    -------
-    subfolders : list
-        排序后的子文件夹名称列表
-
-    Examples
-    --------
-    >>> from dspawpy.diffusion.nebtools import get_neb_subfolders
-    >>> directory = '.' # 默认当前目录
-    >>> get_neb_subfolders()
-    ['00', '01', '02', '03', '04', '05', '06']
-    """
-    raw_subfolders = next(os.walk(directory))[1]
-    subfolders = []
-    for subfolder in raw_subfolders:
-        try:
-            assert 0 <= int(subfolder) < 100
-            subfolders.append(subfolder)
-        except:
-            pass
-    subfolders.sort()  # 从小到大排序
-    return subfolders
-
-
-def plot_barrier(
-    datafile: str = "neb.h5",
-    directory: str = None,
-    ri: float = None,
-    rf: float = None,
-    ei: float = None,
-    ef: float = None,
-    method: str = "PchipInterpolator",
-    figname: str = "neb_barrier.png",
-    show: bool = True,
-    raw=False,
-    **kwargs,
-):
-    """调用 scipy.interpolate 插值算法，拟合NEB能垒并绘图
-
-    Parameters
-    ----------
-    datafile: str
-        neb.h5或neb.json文件路径
-    directory : str
-        NEB计算路径
-    ri : float
-        初态反应坐标
-    rf : float
-        末态反应坐标
-    ei : float
-        初态自洽能量
-    ef : float
-        末态自洽能量
-    method : str, optional
-        插值算法, by default 'interp1d'
-    figname : str, optional
-        能垒图名称, by default 'neb_barrier.png'
-    show : bool, optional
-        是否展示交互界面, by default True
-    raw : bool, optional
-        是否返回原始数据到csv
-
-    Raises
-    ------
-    ImportError
-        指定了scipy.interpolate中不存在的插值算法
-    ValueError
-        传递给插值算法的参数不符合该算法要求
-
-    Examples
-    --------
-    >>> from dspawpy.diffusion.nebtools import plot_barrier
-    >>> import matplotlib.pyplot as plt
-    >>> plot_barrier(directory='source', method='interp1d', kind=2, figname=None, show=False)
-    >>> plot_barrier(directory='source', method='interp1d', kind=3, figname=None, show=False)
-    >>> plot_barrier(directory='source', method='CubicSpline', figname=None, show=False)
-    >>> plot_barrier(directory='source', method='pchip', figname=None, show=False)
-    >>> plt.show()
-    """
-    if directory is not None:
-        # read data
-        subfolders, resort_mfs, rcs, ens, dEs = _getef(directory)
-        dire = directory
-
-    elif datafile:
-        assert os.path.exists(datafile), f"文件{datafile}不存在"
-        if datafile.endswith(".h5"):
-            from dspawpy.io.read import load_h5
-
-            neb = load_h5(datafile)
-            if "/BarrierInfo/ReactionCoordinate" in neb.keys():
-                reaction_coordinate = neb["/BarrierInfo/ReactionCoordinate"]
-                energy = neb["/BarrierInfo/TotalEnergy"]
-            else:  # old version
-                reaction_coordinate = neb["/Distance/ReactionCoordinate"]
-                energy = neb["/Energy/TotalEnergy"]
-        elif datafile.endswith(".json"):
-            with open(datafile, "r") as fin:
-                neb = json.load(fin)
-            if "BarrierInfo" in neb.keys():
-                reaction_coordinate = neb["BarrierInfo"]["ReactionCoordinate"]
-                energy = neb["BarrierInfo"]["TotalEnergy"]
-            else:  # old version
-                reaction_coordinate = neb["Distance"]["ReactionCoordinate"]
-                energy = neb["Energy"]["TotalEnergy"]
-        else:
-            raise TypeError("datafile 必须是 .h5 或 .json 文件格式")
-
-        x = []
-        for c in reaction_coordinate:
-            if len(x) > 0:
-                x.append(x[-1] + c)
-            else:
-                x.append(c)
-
-        y = [x - energy[0] for x in energy]
-        # initial and final info
-        if ri is not None:  # add initial reaction coordinate
-            x.insert(0, ri)
-        if rf is not None:  # add final reaction coordinate
-            x.append(rf)
-
-        if ei is not None:  # add initial energy
-            y.insert(0, ei)
-        if ef is not None:  # add final energy
-            y.append(ef)
-
-        rcs = np.array(x)
-        dEs = np.array(y)
-
-        print(f"如果NEB任务不计算初末态的自洽，{datafile}中将缺失相关信息，需要手动输入")
-        dire = os.getcwd()
-
-    else:
-        raise ValueError("请指定datafile或directory！")
-
-    # import scipy interpolater
-    try:
-        interpolate_method = getattr(
-            __import__("scipy.interpolate", fromlist=[method]), method
-        )
-    except:
-        raise ImportError(f"无法找到 scipy.interpolate.{method} 算法！")
-    # call the interpolater to interpolate with given kwargs
-    try:
-        inter_f = interpolate_method(rcs, dEs, **kwargs)
-    except:
-        raise ValueError(f"插值失败，请检查{kwargs}参数设置是否有误！")
-
-    xnew = np.linspace(rcs[0], rcs[-1], 100)
-    ynew = inter_f(xnew)
-
-    if raw:
-        pd.DataFrame({"x_raw": rcs, "y_raw": dEs}).to_csv("raw_xy.csv", index=False)
-        pd.DataFrame({"x_interpolated": xnew, "y_interpolated": ynew}).to_csv(
-            "raw_interpolated_xy.csv", index=False
-        )
-        
-    # plot
-    if kwargs:
-        plt.plot(xnew, ynew, label=method + str(kwargs))
-    else:
-        plt.plot(xnew, ynew, label=method)
-    plt.scatter(rcs, dEs, c="r")
-    plt.xlabel("Reaction Coordinate (Å)")
-    plt.ylabel("Energy (eV)")
-    plt.legend()
-
-    # save and show
-    if figname:
-        plt.tight_layout()
-        plt.savefig(f"{dire}/{figname}")
-        print("能垒图已保存为", os.path.abspath(f"{dire}/{figname}"))
-    if show:  # 画子图的话，不应每个子图都show
-        plt.show()  # show会自动清空图片
-
-
-def plot_neb_converge(
-    neb_dir: str,
-    image_key: str = "01",
-    show: bool = True,
-    image_name: str = "neb_conv.png",
-    raw=False,
-):
-    """指定NEB计算路径，绘制NEB收敛过程图
-
-    Parameters
-    ----------
-    neb_dir : str
-        NEB计算路径
-    image_key : str
-        第几个构型，默认 "01"
-    show : bool
-        是否交互绘图
-    image_name : str
-        NEB收敛图名称，默认 "neb_conv.png"
-    raw : bool
-        是否输出原始数据到csv文件
-
-    Returns
-    -------
-    ax1, ax2 : matplotlib.axes.Axes
-        两个子图的Axes对象
-
-    Examples
-    --------
-    >>> from dspawpy.diffusion.nebtools import plot_neb_converge
-    >>> plot_neb_converge(neb_dir='my_neb_task', image_key='01')
-    """
-    assert os.path.isdir(neb_dir), f"目录{neb_dir}不存在"
-
-    if os.path.exists(os.path.join(neb_dir, "neb.h5")):
-        neb_total = h5py.File(os.path.join(neb_dir, "neb.h5"))
-        # new output (>=2022B)
-        if "/LoopInfo/01/MaxForce" in neb_total.keys():
-            maxforce = np.array(neb_total.get("/LoopInfo/" + image_key + "/MaxForce"))
-        else:  # old output
-            maxforce = np.array(neb_total.get("/Iteration/" + image_key + "/MaxForce"))
-
-        if "/LoopInfo/01/TotalEnergy" in neb_total.keys():  # new output (>=2022B)
-            total_energy = np.array(
-                neb_total.get("/LoopInfo/" + image_key + "/TotalEnergy")
-            )
-        else:  # old output
-            total_energy = np.array(
-                neb_total.get("/Iteration/" + image_key + "/TotalEnergy")
-            )
-
-    elif os.path.exists(os.path.join(neb_dir, "neb.json")):
-        with open(os.path.join(neb_dir, "neb.json"), "r") as fin:
-            neb_total = json.load(fin)
-        if "LoopInfo" in neb_total.keys():
-            neb = neb_total["LoopInfo"][image_key]
-        else:
-            neb = neb_total["Iteration"][image_key]
-        maxforce = []
-        total_energy = []
-        for n in neb:
-            maxforce.append(n["MaxForce"])
-            total_energy.append(n["TotalEnergy"])
-
-        maxforce = np.array(maxforce)
-        total_energy = np.array(total_energy)
-
-    else:
-        print(
-            f"请检查{os.path.join(neb_dir, 'neb.h5')}或{os.path.join(neb_dir, 'neb.h5')}是否都存在！"
-        )
-
-    x = np.arange(len(maxforce))
-
-    force = maxforce
-    energy = total_energy
-
-    if raw:
-        pd.DataFrame({"x": x, "force": force, "energy": energy}).to_csv(
-            "neb_conv.csv", index=False
-        )
-
-    fig = plt.figure()
-    ax1 = fig.add_subplot(111)
-    ax1.plot(x, force, label="Max Force", c="black")
-    ax1.set_xlabel("Number of ionic step")
-    ax1.set_ylabel("Force (eV/Å)")
-    ax2 = ax1.twinx()
-    ax2.plot(x, energy, label="Energy", c="r")
-    ax2.set_xlabel("Number of ionic step")
-    ax2.set_ylabel("Energy (eV)")
-    ax2.ticklabel_format(useOffset=False)  # y轴坐标显示绝对值而不是相对值
-    fig.legend(loc=1, bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)
-    if image_name:
-        plt.tight_layout()
-        plt.savefig(image_name)
-    if show:
-        plt.show()
-
-    return ax1, ax2
-
-
-def printef(directory):
-    """打印NEB计算时各构型的能量和受力
-
-    Parameters
-    ----------
-    directory : str
-        NEB计算的目录，默认为当前目录
-
-    Returns
-    -------
-    打印各构型的能量和受力
-
-    Examples
-    --------
-    >>> from dspawpy.diffusion.nebtools import printef
-    >>> printef(directory='.')
-    构型    受力(eV/Å)      反应坐标(Å)     此构型的能量(eV)        与初始构型的能量差(eV)
-    00        0.1869          0.0000         -42922.7554                  0.0000
-    01        0.0874          0.7076         -42922.4959                  0.2595
-    02        0.0399          1.4144         -42921.9633                  0.7921
-    03        0.0618          2.1067         -42922.0385                  0.7169
-    04        0.0425          2.7983         -42921.9606                  0.7948
-    05        0.8643          3.5299         -42922.4642                  0.2912
-    06        0.0352          4.2620         -42922.7012                  0.0542
-    """
-    subfolders, resort_mfs, rcs, ens, dEs = _getef(directory)
-    # printout summary
-    print("构型\t受力(eV/Å)\t反应坐标(Å)\t此构型的能量(eV)\t与初始构型的能量差(eV)")
-    for i in range(len(subfolders)):  # 注意格式化输出，对齐
-        print(
-            "%s\t%8.4f\t%8.4f\t%12.4f\t%20.4f"
-            % (subfolders[i], resort_mfs[i], rcs[i], ens[i][0], dEs[i])
-        )
-
-
-def restart(direrctory: str, inputin: str, output: str):
-    """将旧NEB任务归档压缩，并在原路径下准备续算
-
-    Parameters
-    ----------
-    direrctory : str
-        旧NEB任务所在路径，默认当前路径
-    inputin : str
-        输入参数文件名，默认input.in
-    output : str
-        备份文件夹路径
-
-    Examples
-    ----------
-    >>> from dspawpy.diffusion.nebtools import restart
-    >>> restart(direrctory='source', inputin='input.in', output='backup')
-    """
-    if output == "":
-        raise ValueError("备份文件夹路径不能为空！")
-    elif os.path.isdir(output):
-        raise ValueError("备份文件夹已存在！")
-
-    if direrctory == "":
-        directory = os.getcwd()
-    if inputin == "":
-        inputin = "input.in"
-
-    # 读取子文件夹名称列表，仅保留形如00，01数字文件夹路径
-    subfolders = get_neb_subfolders(directory)
-    # 创建备份文件夹并进入
-    os.makedirs(f"{directory}/{output}")
-    os.chdir(f"{directory}/{output}")
-    # 将-0改成-9可提供极限压缩比
-    os.environ["XZ_OPT"] = "-T0 -0"
-    for subfolder in subfolders:
-        # 备份
-        os.system(f"mv {directory}/{subfolder} ./")
-        # 准备续算用的结构文件
-        os.mkdir(f"{directory}/{subfolder}")
-        latestStructureFile = os.path.join(
-            directory, output, subfolder, "latestStructure%s.as" % subfolder
-        )
-        structureFile = os.path.join(
-            directory, output, subfolder, "structure%s.as" % subfolder
-        )
-        bk_latestStructure = f"{directory}/latestStructure{subfolder}.as"
-        bk_structure = f"{directory}/structure{subfolder}.as"
-
-        if os.path.exists(latestStructureFile):
-            os.system(  # 复制到子文件夹
-                f"cp {latestStructureFile} {directory}/{subfolder}/structure{subfolder}.as"
-            )
-            # 备份latestStructureFile到主目录
-            os.system(f"cp {latestStructureFile} {bk_latestStructure}")
-        elif os.path.exists(structureFile):
-            print(f"未找到{latestStructureFile}，复用{structureFile}续算")
-            os.system(  # 复制到子文件夹
-                f"cp {structureFile} {directory}/{subfolder}/structure{subfolder}.as"
-            )
-        else:
-            raise FileNotFoundError(f"{latestStructureFile}和{structureFile}都不存在！")
-        # 备份structureFile到主目录
-        if os.path.exists(structureFile):
-            os.system(f"cp {structureFile} {bk_structure}")
-
-        # 压缩和移动文件
-        # 若存在latestStructure00.as和structure00.as，则压缩00文件夹并把主结构移入00文件夹
-        if os.path.exists(bk_latestStructure) and os.path.exists(bk_structure):
-            os.system(
-                f"tar -Jcf {subfolder}.tar.xz -C {subfolder} . --remove-files && mkdir {subfolder} && mv {subfolder}.tar.xz {directory}/latestStructure{subfolder}.as {directory}/structure{subfolder}.as {subfolder}/ &"
-            )
-        # 若仅存在latestStructure00.as，则压缩00文件夹并把主结构移入00文件夹
-        elif os.path.exists(bk_latestStructure):
-            os.system(
-                f"tar -Jcf {subfolder}.tar.xz {subfolder} . --remove-files && mkdir {subfolder} && mv {subfolder}.tar.xz {directory}/latestStructure{subfolder}.as {subfolder}/ &"
-            )
-        # 若仅存在structure00.as，则压缩00文件夹并把主结构移入00文件夹
-        elif os.path.exists(bk_structure):
-            os.system(
-                f"tar -Jcf {subfolder}.tar.xz -C {subfolder} . --remove-files && mkdir {subfolder} && mv {subfolder}.tar.xz {directory}/structure{subfolder}.as {subfolder}/ &"
-            )
-        else:  # 如果都不存在，说明备份失败
-            raise FileNotFoundError(f"{bk_latestStructure}和{bk_structure}都不存在！")
-
-    # 备份neb.h5,neb.json和DS-PAW.log
-    if os.path.exists(f"{directory}/neb.json"):
-        os.system(
-            f"mv {directory}/neb.h5 {directory}/neb.json {directory}/DS-PAW.log ./"
-        )
-        os.system(f"tar -Jcf neb.tar.xz neb.h5 neb.json --remove-files &")
-    else:
-        os.system(f"mv {directory}/neb.h5 {directory}/DS-PAW.log ./")
-        os.system(f"tar -Jcf neb.tar.xz neb.h5 --remove-files &")
-
-
-def set_pbc(spo):
-    """根据周期性边界条件将分数坐标分量移入 [-0.5, 0.5) 区间
-
-    Parameters
-    ----------
-    spo : np.ndarray or list
-        分数坐标列表
-
-    Returns
-    -------
-    pbc_spo : np.ndarray
-        符合周期性边界条件的分数坐标列表
-
-    Examples
-    --------
-    >>> from dspawpy.diffusion.nebtools import set_pbc
-    >>> set_pbc([-0.6, 1.2, 2.3])
-    array([0.4, 0.2, 0.3])
-    """
-    # wrap into [-0.5, 0.5)
-    pbc_spo = np.mod(np.array(spo) + 0.5, 1.0) - 0.5
-
-    return pbc_spo
-
-
-def summary(directory: str = ".", raw=False, **kwargs):
-    """NEB任务完成总结，依次执行以下步骤：
-
-    - 1. 打印各构型受力、反应坐标、能量、与初始构型的能量差
-    - 2. 绘制能垒图
-    - 3. 绘制并保存结构优化过程的能量和受力收敛过程图
-
-    Parameters
-    ----------
-    directory : str
-        NEB路径, 默认当前路径
-    raw : bool
-        是否保存原始数据到csv文件
-    **kwargs : dict
-        传递给plot_barrier的参数
-
-    Examples
-    --------
-    >>> from dspawpy.diffusion.nebtools import summary
-    >>> directory = '.' # NEB计算路径，默认当前路径
-    >>> summary(directory)
-    # 若inifin=false，用户必须将自洽的scf.h5或system.json放到初末态子文件夹中
-    """
-    # 1. 绘制能垒图
-    print("--> 1. 打印NEB计算时各构型的能量和受力...")
-    printef(directory)
-
-    # 2. 打印各构型受力、反应坐标、能量、与初始构型的能量差
-    print("\n--> 2. 绘制能垒图...")
-    plot_barrier(directory=directory, raw=raw, **kwargs)
-
-    # 3. 绘制并保存结构优化过程的能量和受力收敛过程图到各构型文件夹中
-    print("\n--> 3. 绘制收敛过程图到各构型文件夹中...")
-    subfolders = get_neb_subfolders(directory)
-    for subfolder in subfolders[1 : len(subfolders) - 1]:
-        print(f"----> {subfolder}/converge.png...")
-        plot_neb_converge(
-            neb_dir=directory,
-            image_key=subfolder,
-            image_name=f"{directory}/{subfolder}/converge.png",
-            raw=raw,
-        )
-    print("\n完成!")
-
-
-def write_movie_json(preview: bool = False, directory: str = ".", step: int = -1):
-    """NEB计算或者初始插值后，读取信息，保存为 neb_movie*.json 文件
-
-    用 Device Studio 打开该文件可以观察结构等信息
-
-    Parameters
-    ----------
-    preview : bool
-        是否预览模式，默认否
-    directory : str
-        计算结果所在目录. 默认当前路径
-    step: int
-        离子步编号. 默认-1，读取整个NEB计算过程信息
-
-    Returns
-    ----------
-    neb_movie*.json文件
-
-    Examples
-    ----------
-    >>> from dspawpy.diffusion.nebtools import write_movie_json
-    # NEB计算完成后要观察轨迹变化全过程，只需指定NEB计算路径即可
-    >>> write_movie_json(directory='source')
-    # NEB计算完成后要观察第n离子步结构，请设置step为n，注意step从1开始计数
-    >>> write_movie_json(directory='source', step=1)
-    # 如果您指定的step数超过NEB实际完成的离子步，将会自动修改为最后一步，实际效果等同于上一行代码
-    >>> write_movie_json(directory='source', step=100)
-    # 另外，如需预览初插结构，请将preview设置为True，并将directory指定为NEB计算主路径
-    >>> write_movie_json(preview=True, directory='source')
-    """
-
-    if preview:  # preview mode, write neb_movie_init.json from structure.as
-        print("正在根据初插结构保存neb_movie_init.json...")
-        try:
-            raw = _from_structures(directory)
-        except FileNotFoundError:
-            print("未找到初始插值结构！")
-        except:
-            print("初始插值结构读取失败！请检查structure*.as文件内容！")
-    else:
-        if step == 0:  # try preview mode to save time
-            try:
-                raw = _from_structures(directory)
-            except:
-                print("未找到初始插值结构，将从计算结果h5或json文件中读取！")
-        else:
-            try:  # read from h5 file
-                raw = _from_h5(directory, step)
-            except FileNotFoundError:
-                try:  # read from json file
-                    raw = _from_json(directory, step)
-                except FileNotFoundError:
-                    print("h5和json文件都不存在！")
-                except json.decoder.JSONDecodeError:
-                    print("json文件格式错误！")
-            except:
-                print("h5文件内容读取失败！")
-
-    _dump_neb_movie_json(raw)
-
-
-def _from_structures(directory: str):
-    """从structure00.as，structure01.as，...，中读取结构信息，
-    写入neb_movie_init，以便用DeviceStudio打开观察
-
-    Parameters
-    ----------
-    directory : str
-        NEB计算路径，默认当前路径
-
-    Returns
-    -------
-    用于json文件的各个数组
-    """
-    output = "neb_movie_init.json"
-    step = 0
-
-    subfolders = get_neb_subfolders(directory)
-    # print(subfolders)
-    nimage = len(subfolders)
-    reactionCoordinates = np.zeros(shape=nimage)  # optional
-    totalEnergies = np.zeros(shape=nimage)  # optional
-    maxForces = np.zeros(shape=nimage - 2)  # optional
-    tangents = np.zeros(shape=nimage - 2)  # optional
-    MaxForces = np.zeros(shape=(nimage - 2, step + 1))  # optional
-    TotalEnergies = np.zeros(shape=(nimage - 2, step + 1))  # optional
-
-    Poses = []  # nimage x Natom x 3 , read
-    Elems = []  # nimage x Natom, read
-    Latvs = []  # nimage x 9, read
-
-    for i, folder in enumerate(subfolders):
-        structure_path = os.path.join(directory, folder, f"structure{folder}.as")
-        if not os.path.exists(structure_path):
-            raise FileNotFoundError(f"请检查{structure_path}是否存在！")
-        pos, ele, lat = get_pos_ele_lat(structure_path)
-        Poses.append(pos)
-        Elems.append(ele)
-        Latvs.append(lat)
-
-    Natom = len(Elems[0])
-
-    # reshape data
-    Poses = np.array(Poses).reshape((nimage, Natom, 3))
-    Elems = np.array(Elems).reshape((nimage, Natom))
-    Latvs = np.array(Latvs).reshape((nimage, 9))
-    Fixs = np.zeros(shape=(Natom, 3))  # optional
-
-    return (
-        output,
-        subfolders,
-        step,
-        MaxForces,
-        TotalEnergies,
-        Poses,
-        Latvs,
-        Elems,
-        Fixs,
-        reactionCoordinates,
-        totalEnergies,
-        maxForces,
-        tangents,
-    )
-
-
-def _from_h5(directory: str, step: int):
-    """从NEB路径下的h5文件读取指定step数的结构和能量信息，
-    写入json文件，以便用DeviceStudio打开观察
-
-    Parameters
-    ----------
-    directory : str
-        NEB路径，默认当前路径
-    step : int
-        step数，默认-1，读取最后一个构型
-
-    Returns
-    -------
-    用于json文件的各个数组
-    """
-    # ^ 前期设置
-    neb_h5 = os.path.join(directory, "01/neb01.h5")
-    data = h5py.File(neb_h5)
-    total_steps = np.array(data.get("/NebSize"))[0]
-
-    if step == -1:
-        output = "neb_movie_last.json"
-        step = total_steps - 1
-        print("正在根据最后一个离子步信息生成neb_movie_last.json...")
-    elif step > total_steps - 1:
-        output = "neb_movie_last.json"
-        step = total_steps - 1
-        print(f"您指定的step数大于NEB计算实际完成的总离子步数{total_steps}")
-        print("正在根据最后一个离子步信息生成neb_movie_last.json...")
-    else:
-        output = "neb_movie_{}.json".format(step)
-        print(f"正在根据第{step}个离子步信息生成{output}...")
-
-    # ^ 读取前，准备好json文件所需数组框架
-    subfolders = get_neb_subfolders(directory)
-    nimage = len(subfolders)
-    reactionCoordinates = np.zeros(shape=nimage)  # optional
-    totalEnergies = np.zeros(shape=nimage)  # optional，每个构型最终能量
-    maxForces = np.zeros(shape=nimage - 2)  # optional
-    tangents = np.zeros(shape=nimage - 2)  # optional
-    MaxForces = np.zeros(shape=(nimage - 2, step + 1))  # optional
-    TotalEnergies = np.zeros(shape=(nimage - 2, step + 1))  # optional，中间构型每个离子步能量
-    Sposes = []  # nimage x Natom x 3 , read
-    Elems = []  # nimage x Natom, read
-    Latvs = []  # nimage x 9, read
-    Fixs = []  # Natom x 3, set
-
-    for folder in subfolders:
-        """如果是首尾两个构型，最多只有scf.h5文件，没有neb.h5文件
-        用户如果算NEB的时候，不计算首尾构型的自洽，
-         或者在别处算完了但是没有复制到首尾文件夹中并命名为scf.h5，
-          便不能使用第一个功能
-        """
-        if folder == subfolders[0] or folder == subfolders[-1]:
-            h5_path = os.path.join(directory, folder, "scf.h5")
-        else:
-            h5_path = os.path.join(directory, folder, f"neb{folder}.h5")
-        assert os.path.exists(h5_path), f"请确认{h5_path}是否存在！"
-
-    # ^ 开始分功能读取数据
-    for i, folder in enumerate(subfolders):
-        if folder == subfolders[0] or folder == subfolders[-1]:
-            h5_path = os.path.join(directory, folder, "scf.h5")
-            data = h5py.File(h5_path)
-            # 不影响可视化，直接定为0
-            if folder == subfolders[0]:
-                reactionCoordinates[i] = 0
-            pos = np.array(data.get("/Structures/Step-1/Position"))  # scaled
-            lat = np.array(data.get("/Structures/Step-1/Lattice"))
-
-        else:
-            h5_path = os.path.join(directory, folder, f"neb{folder}.h5")
-            data = h5py.File(h5_path)
-            # reading...
-            reactionCoordinates[i - 1] = np.array(data.get("/Distance/Previous"))[-1]
-            maxForces[i - 1] = np.array(data.get("/MaxForce"))[-1]
-            tangents[i - 1] = np.array(data.get("/Tangent"))[-1]
-            if folder == subfolders[-2]:
-                reactionCoordinates[i + 1] = np.array(data.get("/Distance/Next"))[-1]
-            # read MaxForces and TotalEnergies
-            nionStep = np.array(data.get("/MaxForce")).shape[0]
-            assert step <= nionStep, f"总共只完成了{nionStep}个离子步!"
-            for j in range(step + 1):
-                MaxForces[i - 1, j] = np.array(data.get("/MaxForce"))[j]
-                TotalEnergies[i - 1, j] = np.array(data.get("/TotalEnergy"))[j]
-
-            # read the latest structure for visualization
-            pos = np.array(data.get(f"/Structures/Step-{step+1}/Position"))  # scaled
-            lat = np.array(data.get(f"/Structures/Step-{step+1}/Lattice"))
-
-        totalEnergies[i] = np.array(data.get("/Energy/TotalEnergy0"))
-
-        elems = get_ele_from_h5(hpath=h5_path)
-        Elems.append(elems)
-
-        Sposes.append(pos)
-        Latvs.append(lat)
-
-    Natom = len(Elems[0])
-    tdata = h5py.File(os.path.join(directory, "neb.h5"))
-    # atom fix, not lattice
-    # ignore this trivial message because it is not necessary for the visualization
-    if "/UnrelaxStructure/Image00/Fix" in tdata:
-        fix_array = np.array(tdata.get("/UnrelaxStructure/Image00/Fix"))
-        for fix in fix_array:
-            if fix == 0.0:
-                F = False
-            elif fix == 1.0:
-                F = True
-            else:
-                raise ValueError("Fix值只能为0或1")
-            Fixs.append(F)
-    else:
-        Fixs = np.full(shape=(Natom, 3), fill_value=False)
-
-    # 累加reactionCoordinates中的元素
-    for i in range(1, len(reactionCoordinates)):
-        reactionCoordinates[i] += reactionCoordinates[i - 1]
-
-    # reshape data
-    Sposes = np.array(Sposes).reshape((nimage, Natom, 3))
-    Elems = np.array(Elems).reshape((nimage, Natom))
-    Latvs = np.array(Latvs).reshape((nimage, 9))
-    Fixs = np.array(Fixs).reshape((Natom, 3))
-
-    return (
-        output,
-        subfolders,
-        step,
-        MaxForces,
-        TotalEnergies,  #
-        Sposes,
-        Latvs,
-        Elems,
-        Fixs,
-        reactionCoordinates,
-        totalEnergies,
-        maxForces,
-        tangents,
-    )
-
-
-def _from_json(directory: str, step: int):
-    """从NEB路径下的json文件读取指定step数的结构和能量信息，
-    写入json文件，以便用DeviceStudio打开观察
-
-    Parameters
-    ----------
-    directory : str
-        NEB路径，默认当前路径
-    step : int
-        step数，默认-1，读取最后一个构型
-
-    Returns
-    -------
-    用于json文件的各个数组
-    """
-
-    # ^ 前期设置
-    neb_js = os.path.join(directory, "01/neb01.json")
-    with open(neb_js, "r") as f:
-        data = json.load(f)
-    total_steps = len(data)
-
-    if step == -1:
-        output = "neb_movie_last.json"
-        step = total_steps - 1
-        print("正在根据最后一个离子步信息生成neb_movie_last.json（h5文件不存在，尝试从json文件读取数据）")
-    elif step > total_steps - 1:
-        output = "neb_movie_last.json"
-        step = total_steps - 1
-        print(f"您指定的step数大于NEB计算实际完成的总离子步数{total_steps}")
-        print("正在根据最后一个离子步信息生成neb_movie_last.json（h5文件不存在，尝试从json文件读取数据）")
-    else:
-        output = f"neb_movie_{step}.json"
-        print(f"正在根据第{step}个离子步信息生成{output}...")
-
-    # ^ 读取前，准备好json文件所需数组框架
-    subfolders = get_neb_subfolders(directory)
-    nimage = len(subfolders)
-    reactionCoordinates = np.zeros(shape=nimage)  # optional
-    totalEnergies = np.zeros(shape=nimage)  # optional，每个构型最终能量
-    maxForces = np.zeros(shape=nimage - 2)  # optional
-    tangents = np.zeros(shape=nimage - 2)  # optional
-    MaxForces = np.zeros(shape=(nimage - 2, step + 1))  # optional
-    TotalEnergies = np.zeros(shape=(nimage - 2, step + 1))  # optional，中间构型每个离子步能量
-    Sposes = []  # nimage x Natom x 3 , read
-    Elems = []  # nimage x Natom, read
-    Latvs = []  # nimage x 9, read
-    Fixs = []  # Natom x 3, set
-
-    for folder in subfolders:
-        """如果是首尾两个构型，最多只有system.json文件，没有neb*.json文件
-        用户如果算NEB的时候，不计算首尾构型的自洽，
-         或者在别处算完了但是没有复制到首尾文件夹中并命名为system.json，
-          便不能使用第一个功能
-        """
-        if folder == subfolders[0] or folder == subfolders[-1]:
-            js_path = os.path.join(directory, folder, "system.json")
-        else:
-            js_path = os.path.join(directory, folder, f"neb{folder}.json")
-        assert os.path.exists(js_path), f"请确认{js_path}是否存在！"
-
-    # ^ 开始分功能读取数据
-    for i, folder in enumerate(subfolders):
-        if i == 0:  # 初末态在NEB计算过程中不会优化结构
-            # 1. 外部自洽后移动system.json
-            js_path = os.path.join(directory, folder, "system.json")
-            # 2. 直接NEB计算，得到system00.json
-            neb_js_path = os.path.join(directory, folder, f"system{folder}.json")
-            if os.path.exists(neb_js_path):  # 优先读取neb计算得到的system00.json
-                with open(neb_js_path, "r") as f:
-                    data = json.load(f)
-
-            elif os.path.exists(js_path):
-                with open(js_path, "r") as f:
-                    data = json.load(f)
-
-            else:
-                raise FileNotFoundError(
-                    f"{os.path.abspath(js_path)}和{os.path.abspath(neb_js_path)}均不存在！"
-                )
-
-            lat = data["AtomInfo"]["Lattice"]
-            Latvs.append(lat)
-
-            Natom = len(data["AtomInfo"]["Atoms"])  # 读取原子数
-            for j in range(Natom):
-                pos = data["AtomInfo"]["Atoms"][j]["Position"]  # scaled
-                Sposes.append(pos)
-
-            totalEnergies[i] = data["Energy"]["TotalEnergy0"]
-            reactionCoordinates[i] = 0.0
-
-        elif i > 0 and i < nimage - 1:  # 中间构型会优化结构
-            # 读取晶胞矢量、原子坐标
-            relax_json = os.path.join(directory, folder, "relax.json")
-            assert os.path.exists(relax_json), f"{relax_json}不存在！"
-
-            with open(relax_json, "r") as f:
-                rdata = json.load(f)
-
-            lat = rdata[step]["Lattice"]  # 第step步优化后的晶胞
-            Latvs.append(lat)
-
-            Natom = len(rdata[0]["Atoms"])
-            for j in range(Natom):  # for each atom
-                pos = rdata[step]["Atoms"][j]["Position"]  # 第step步优化后的原子坐标
-                Sposes.append(pos)  # ! 输出的都是分数坐标
-
-            # 读取能量和反应坐标
-            nj = os.path.join(directory, folder, f"neb{folder}.json")
-            with open(nj, "r") as f:
-                ndata = json.load(f)
-
-            totalEnergies[i] = ndata[step]["TotalEnergy"]  # 读取第step步优化后的能量
-
-            # 读取与前一个构型相比的反应坐标
-            reactionCoordinates[i - 1] = ndata[step]["ReactionCoordinate"][-2]
-            tangents[i - 1] = ndata[step]["Tangent"]
-            if folder == subfolders[-2]:  # 末态前一个构型的计算结果中读取反应坐标
-                reactionCoordinates[i + 1] = ndata[step]["ReactionCoordinate"][-1]
-            for j in range(step + 1):
-                MaxForces[i - 1, j] = ndata[j]["MaxForce"]
-                # TODO neb01.json中不存在TotalEnergy0，暂时读取TotalEnergy
-                TotalEnergies[i - 1, j] = ndata[j]["TotalEnergy"]
-
-        else:  # 末态构型
-            js_path = os.path.join(directory, folder, "system.json")
-            neb_js_path = os.path.join(directory, folder, f"system{folder}.json")
-            if os.path.exists(neb_js_path):  # 优先读取neb计算得到的json文件
-                with open(neb_js_path, "r") as f:
-                    data = json.load(f)
-
-            elif os.path.exists(js_path):
-                with open(js_path, "r") as f:
-                    data = json.load(f)
-
-            else:
-                raise FileNotFoundError(
-                    f"{os.path.abspath(js_path)}和{os.path.abspath(neb_js_path)}均不存在！"
-                )
-
-            lat = data["AtomInfo"]["Lattice"]
-            Latvs.append(lat)
-
-            Natom = len(data["AtomInfo"]["Atoms"])  # 读取原子数
-            for j in range(Natom):
-                pos = data["AtomInfo"]["Atoms"][j]["Position"]  # scaled
-                Sposes.append(pos)
-
-            energy = data["Energy"]["TotalEnergy0"]
-            totalEnergies[i] = energy
-
-    # 读取原子元素
-    tneb_js = os.path.join(directory, "neb.json")
-    with open(tneb_js, "r") as f:
-        tdata = json.load(f)
-
-    Natom = len(tdata["UnrelaxStructure"][0]["Atoms"])
-    elems = []
-    for k in range(Natom):
-        ele = tdata["UnrelaxStructure"][0]["Atoms"][k]["Element"]
-        elems.append(ele)
-
-    for ni in range(nimage):
-        Elems.append(elems)  # 重复nimage次，保持Elems结构一致
-
-    for atom in range(Natom):
-        fix_array = tdata["UnrelaxStructure"][1]["Atoms"][atom]["Fix"]  # (1,3)
-        if fix_array == []:  # empty list
-            fix_array = [0.0, 0.0, 0.0]
-        for fix in fix_array:
-            if fix == 0.0:
-                F = False
-            elif fix == 1.0:
-                F = True
-            else:
-                raise ValueError("Fix值只能为 0.0 或 1.0")
-            Fixs.append(F)
-
-    # 累加reactionCoordinates中的元素
-    for i in range(1, len(reactionCoordinates)):
-        reactionCoordinates[i] += reactionCoordinates[i - 1]
-
-    # reshape data
-    Sposes = np.array(Sposes).reshape((nimage, Natom, 3))
-    Elems = np.array(Elems).reshape((nimage, Natom))
-    Latvs = np.array(Latvs).reshape((nimage, 9))
-    Fixs = np.array(Fixs).reshape((Natom, 3))
-
-    return (
-        output,
-        subfolders,
-        step,
-        MaxForces,
-        TotalEnergies,
-        Sposes,
-        Latvs,
-        Elems,
-        Fixs,
-        reactionCoordinates,
-        totalEnergies,
-        maxForces,
-        tangents,
-    )
-
-
-def _dump_neb_movie_json(raw):
-    """根据之前收集到的各数据列表，dump json文件到output"""
-    (
-        output,
-        subfolders,
-        step,
-        MaxForces,
-        TotalEnergies,
-        Poses,
-        Latvs,
-        Elems,
-        Fixs,
-        reactionCoordinates,
-        totalEnergies,
-        maxForces,
-        tangents,
-    ) = raw
-
-    IterDict = {}
-    for s, sf in enumerate(subfolders):
-        if sf == subfolders[0] or sf == subfolders[-1]:
-            continue
-        else:
-            Eflist = []
-            for l in range(step + 1):
-                ef = {
-                    "MaxForce": MaxForces[s - 1, l],
-                    "TotalEnergy": TotalEnergies[s - 1, l],
-                }
-                Eflist.append(ef)
-                iterDict = {sf: Eflist}  # construct sub-dict
-                IterDict.update(iterDict)  # append sub-dict
-
-    RSList = []
-    """
-    从外到内依次遍历 构型、原子（子字典）
-    原子的键值对为：'Atoms': 原子信息列表
-    原子信息列表是一个由字典组成的列表，每个字典对应一个原子的信息
-    """
-    for s, sf in enumerate(subfolders):
-        pos = Poses[s]
-        lat = Latvs[s]
-        elem = Elems[s]
-        atoms = []
-        for i in range(len(elem)):
-            atom = {
-                "Element": elem[i],
-                "Fix": Fixs[i].tolist(),
-                "Mag": [],  # empty
-                "Position": pos[i].tolist(),
-                "Pot": "",
-            }  # empty
-            atoms.append(atom)
-        rs = {"Atoms": atoms, "CoordinateType": "Direct", "Lattice": lat.tolist()}
-        RSList.append(rs)
-
-    URSList = []  # DS似乎并不读取这部分信息，空置即可
-
-    data = {
-        "Distance": {"ReactionCoordinate": reactionCoordinates.tolist()},
-        "Energy": {"TotalEnergy": totalEnergies.tolist()},
-        "Force": {"MaxForce": maxForces.tolist(), "Tangent": tangents.tolist()},
-        "Iteration": IterDict,
-        "RelaxedStructure": RSList,
-        "UnrelaxedStructure": URSList,
-    }
-
-    # ^ 将字典写入json文件
-    with open(output, "w") as f:
-        json.dump(data, f, indent=4)
-
-    print(f"--> {os.path.abspath(output)} 写入成功！\n")
-
-
-def _getef(directory: str = "."):
-    """读取NEB计算时各构型的能量和受力，NEB计算可以未收敛
-    但如果初末态自洽在别处完成，请手动将其移入00等文件夹中！
-
-    Parameters
-    ----------
-    directory: str
-        NEB计算的路径，默认当前路径
-
-    Returns
-    -------
-    subfolders: list
-        构型文件夹名列表
-    resort_mfs: list
-        构型受力的最大分量列表
-    rcs: list
-        反应坐标列表
-    ens: list
-        电子总能列表
-    dEs: list
-        与初始构型的能量差列表
-
-    Examples
-    --------
-    >>> from dspawpy.diffusion.nebtools import getef
-    >>> directory = '.'  # NEB计算的路径，默认当前路径
-    >>> subfolders, resort_mfs, rcs, ens, dEs = getef(directory)
-    >>> print(subfolders)
-    ['00', '01', '02', '03', '04', '05', '06']
-    >>> print(resort_mfs)
-    [0.186876802632, 0.087370747812, 0.039940517914, 0.061764168138, 0.042459986924, 0.864285419627, 0.035218915742]
-    >>> print(rcs)
-    [0.         0.70762104 1.41440524 2.10669668 2.79829788 3.52986152 4.26191179]
-    >>> print(ens)
-    [-42922.75544084902, -42922.495919381065, -42921.96332356572, -42922.038481743206, -42921.960565486996, -42922.464230317, -42922.70115500509]
-    >>> print(dEs)
-    [0.         0.25952147 0.79211728 0.71695911 0.79487536 0.29121053 0.05428584]
-    """
-
-    subfolders = get_neb_subfolders(directory)
-    Nimage = len(subfolders)
-
-    ens = []
-    dEs = np.zeros(Nimage)
-    rcs = [0]
-    mfs = []
-
-    # read energies
-    count = 1
-    for i, subfolder in enumerate(subfolders):
-        if i == 0 or i == Nimage - 1:
-            jsf = os.path.join(directory, subfolder, f"system{subfolder}.json")
-            old_jsf = os.path.join(directory, subfolder, "system.json")
-            hf = os.path.join(directory, subfolder, "scf.h5")
-
-            if os.path.exists(hf):  # 优先读取h5文件内容
-                data = h5py.File(hf)
-                en = np.array(data.get("/Energy/TotalEnergy0"))
-                if i == 0 or i == Nimage - 1:
-                    mf = np.max(np.abs(np.array(data.get("/Force/ForceOnAtoms"))))
-                    mfs.append(mf)
-
-            elif os.path.exists(jsf):  # 其次读取json文件内容
-                with open(jsf, "r") as f:
-                    data = json.load(f)
-                en = data["Energy"]["TotalEnergy0"]
-                if i == 0 or i == Nimage - 1:
-                    mf = np.max(np.abs(data["Force"]["ForceOnAtoms"]))
-                    mfs.append(mf)
-
-            elif os.path.exists(old_jsf):  # 兼容老json
-                with open(old_jsf, "r") as f:
-                    data = json.load(f)
-                en = data["Energy"]["TotalEnergy0"]
-                if i == 0 or i == Nimage - 1:
-                    mf = np.max(np.abs(data["Force"]["ForceOnAtoms"]))
-                    mfs.append(mf)
-
-            else:
-                raise FileNotFoundError(
-                    "无法找到记录构型%s的能量和受力的system.json或scf.h5文件" % subfolder
-                )
-            ens.append(en)
-
-        else:
-            jsf = os.path.join(directory, subfolder, f"neb{subfolder}.json")
-            sysjsf = os.path.join(directory, subfolder, f"system{subfolder}.json")
-            old_sysjsf = os.path.join(directory, subfolder, "system.json")
-            hf = os.path.join(directory, subfolder, f"neb{subfolder}.h5")
-
-            if os.path.exists(hf):  # 优先读取h5文件内容
-                data = h5py.File(hf)
-                en = np.array(data.get("/Energy/TotalEnergy0"))
-                mf = np.array(data.get("/MaxForce"))[-1]
-                # the key may change depends on your DS-PAW version
-                if "/Distance/Previous" in data:
-                    rc = np.array(data.get("/Distance/Previous"))[-1]
-                elif "/ReactionCoordinate" in data:
-                    rc = np.array(data.get("/ReactionCoordinate"))[-2]
-                else:
-                    raise KeyError("找不到/Distance/Previous或/ReactionCoordinate键！")
-                rcs.append(rc)
-                if count == Nimage - 2:  # before final image
-                    if "/Distance/Next" in data:
-                        rc = np.array(data.get("/Distance/Next"))[-1]
-                    elif "/ReactionCoordinate" in data:
-                        rc = np.array(data.get("/ReactionCoordinate"))[-1]
-                    else:
-                        raise KeyError("找不到/Distance/Next或/ReactionCoordinate键！")
-                    rcs.append(rc)
-
-            elif os.path.exists(jsf):
-                if os.path.exists(sysjsf):
-                    with open(sysjsf, "r") as f:
-                        data = json.load(f)
-                    en = data["Energy"]["TotalEnergy0"]
-                elif os.path.exists(old_sysjsf):  # 兼容旧版DS-PAW
-                    with open(old_sysjsf, "r") as f:
-                        data = json.load(f)
-                    en = data["Energy"]["TotalEnergy0"]
-                else:
-                    raise FileNotFoundError(f"无法找到{sysjsf}或{old_sysjsf}")
-
-                with open(jsf, "r") as f:
-                    data = json.load(f)
-                Nion_step = len(data)
-                # en = data[Nion_step - 1]["TotalEnergy"] # invalid
-                mf = data[Nion_step - 1]["MaxForce"]  # 最后一步的最大受力
-                rc = data[Nion_step - 1]["ReactionCoordinate"][0]  # 最后一步的反应坐标
-                rcs.append(rc)
-                if count == Nimage - 2:  # before final image
-                    rc = data[Nion_step - 1]["ReactionCoordinate"][1]  # 最后一步的反应坐标
-                    rcs.append(rc)
-
-            else:
-                raise FileNotFoundError(f"无法找到{hf}或{jsf}")
-
-            ens.append(en)
-            mfs.append(mf)
-
-            # get dE
-            dE = ens[count] - ens[0]
-            dEs[i] = dE
-            count += 1
-    dEs[-1] = ens[Nimage - 1] - ens[0]
-
-    # rcs 改成累加值
-    for i in range(1, len(rcs)):
-        rcs[i] += rcs[i - 1]
-
-    rcs = np.array(rcs)
-
-    resort_mfs = [mfs[0]]
-    final_mf = mfs[1]
-    for j in range(2, len(mfs)):
-        resort_mfs.append(mfs[j])
-    resort_mfs.append(final_mf)
-
-    return subfolders, resort_mfs, rcs, ens, dEs
+import os
+import h5py
+import json
+import pandas as pd
+import numpy as np
+
+np.set_printoptions(suppress=True)  # 不使用科学计数法
+from dspawpy.io.utils import get_pos_ele_lat, get_ele_from_h5
+import matplotlib.pyplot as plt
+
+
+def get_distance(
+    spo1: np.ndarray, spo2: np.ndarray, lat1: np.ndarray, lat2: np.ndarray
+):
+    """根据两个结构的分数坐标和晶胞计算距离
+
+    Parameters
+    ----------
+    spo1 : np.ndarray
+        分数坐标列表1
+    spo2 : np.ndarray
+        分数坐标列表2
+    lat1 : np.ndarray
+        晶胞1
+    lat2 : np.ndarray
+        晶胞2
+
+    Returns
+    -------
+    float
+        距离
+
+    Examples
+    --------
+    >>> from dspawpy.diffusion.nebtools import get_distance
+    >>> from dspawpy.io.utils import get_spo_ele_lat
+    >>> # 先读取两个构型的分数坐标、元素列表和晶胞信息
+    >>> spo1, ele1, lat1 = get_spo_ele_lat('structure01.as')
+    >>> spo1
+    array([[0.25      , 0.25      , 0.11784996],
+           [0.75      , 0.25      , 0.11784996],
+           ...，
+           [0.04062186, 0.45937779, 0.43038044]])
+    >>> ele1
+    ['Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt']
+    >>> lat1
+    array([[ 5.6058,  0.    ,  0.    ],
+           [ 0.    ,  5.6058,  0.    ],
+           [ 0.    ,  0.    , 16.8174]])
+    >>> spo2, ele2, lat2 = get_spo_ele_lat('structure02.as')
+    >>> spo2
+    array([[0.25      , 0.25      , 0.11784996],
+           ...，
+           [0.08124389, 0.41875557, 0.41408303]])
+    >>> ele2
+    ['Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt', 'Pt']
+    >>> lat2
+    array([[ 5.6058,  0.    ,  0.    ],
+           [ 0.    ,  5.6058,  0.    ],
+           [ 0.    ,  0.    , 16.8174]])
+    >>> # 计算两个构型的距离
+    >>> dist = get_distance(spo1, spo2, lat1, lat2)
+    >>> dist
+    0.5987379724194888
+    """
+    diff_spo = spo1 - spo2  # 分数坐标差
+    avglatv = 0.5 * (lat1 + lat2)  # 平均晶格矢量
+    pbc_diff_spo = set_pbc(diff_spo)  # 笛卡尔坐标差
+    # 分数坐标点乘平均晶胞，转回笛卡尔坐标
+    pbc_diff_pos = np.dot(pbc_diff_spo, avglatv)  # 笛卡尔坐标差
+    distance = np.sqrt(np.sum(pbc_diff_pos**2))
+
+    return distance
+
+
+def get_neb_subfolders(directory: str = "."):
+    """将directory路径下的子文件夹名称列表按照数字大小排序
+
+    仅保留形如00，01数字类型的NEB子文件夹路径
+
+    Parameters
+    ----------
+    subfolders : list
+        子文件夹名称列表
+
+    Returns
+    -------
+    subfolders : list
+        排序后的子文件夹名称列表
+
+    Examples
+    --------
+    >>> from dspawpy.diffusion.nebtools import get_neb_subfolders
+    >>> directory = '.' # 默认当前目录
+    >>> get_neb_subfolders()
+    ['00', '01', '02', '03', '04', '05', '06']
+    """
+    raw_subfolders = next(os.walk(directory))[1]
+    subfolders = []
+    for subfolder in raw_subfolders:
+        try:
+            assert 0 <= int(subfolder) < 100
+            subfolders.append(subfolder)
+        except:
+            pass
+    subfolders.sort()  # 从小到大排序
+    return subfolders
+
+
+def plot_barrier(
+    datafile: str = "neb.h5",
+    directory: str = None,
+    ri: float = None,
+    rf: float = None,
+    ei: float = None,
+    ef: float = None,
+    method: str = "PchipInterpolator",
+    figname: str = "neb_barrier.png",
+    show: bool = True,
+    raw=False,
+    **kwargs,
+):
+    """调用 scipy.interpolate 插值算法，拟合NEB能垒并绘图
+
+    Parameters
+    ----------
+    datafile: str
+        neb.h5或neb.json文件路径
+    directory : str
+        NEB计算路径
+    ri : float
+        初态反应坐标
+    rf : float
+        末态反应坐标
+    ei : float
+        初态自洽能量
+    ef : float
+        末态自洽能量
+    method : str, optional
+        插值算法, by default 'interp1d'
+    figname : str, optional
+        能垒图名称, by default 'neb_barrier.png'
+    show : bool, optional
+        是否展示交互界面, by default True
+    raw : bool, optional
+        是否返回原始数据到csv
+
+    Raises
+    ------
+    ImportError
+        指定了scipy.interpolate中不存在的插值算法
+    ValueError
+        传递给插值算法的参数不符合该算法要求
+
+    Examples
+    --------
+    >>> from dspawpy.diffusion.nebtools import plot_barrier
+    >>> import matplotlib.pyplot as plt
+    >>> plot_barrier(directory='source', method='interp1d', kind=2, figname=None, show=False)
+    >>> plot_barrier(directory='source', method='interp1d', kind=3, figname=None, show=False)
+    >>> plot_barrier(directory='source', method='CubicSpline', figname=None, show=False)
+    >>> plot_barrier(directory='source', method='pchip', figname=None, show=False)
+    >>> plt.show()
+    """
+    if directory is not None:
+        # read data
+        subfolders, resort_mfs, rcs, ens, dEs = _getef(directory)
+        dire = directory
+
+    elif datafile:
+        assert os.path.exists(datafile), f"文件{datafile}不存在"
+        if datafile.endswith(".h5"):
+            from dspawpy.io.read import load_h5
+
+            neb = load_h5(datafile)
+            if "/BarrierInfo/ReactionCoordinate" in neb.keys():
+                reaction_coordinate = neb["/BarrierInfo/ReactionCoordinate"]
+                energy = neb["/BarrierInfo/TotalEnergy"]
+            else:  # old version
+                reaction_coordinate = neb["/Distance/ReactionCoordinate"]
+                energy = neb["/Energy/TotalEnergy"]
+        elif datafile.endswith(".json"):
+            with open(datafile, "r") as fin:
+                neb = json.load(fin)
+            if "BarrierInfo" in neb.keys():
+                reaction_coordinate = neb["BarrierInfo"]["ReactionCoordinate"]
+                energy = neb["BarrierInfo"]["TotalEnergy"]
+            else:  # old version
+                reaction_coordinate = neb["Distance"]["ReactionCoordinate"]
+                energy = neb["Energy"]["TotalEnergy"]
+        else:
+            raise TypeError("datafile 必须是 .h5 或 .json 文件格式")
+
+        x = []
+        for c in reaction_coordinate:
+            if len(x) > 0:
+                x.append(x[-1] + c)
+            else:
+                x.append(c)
+
+        y = [x - energy[0] for x in energy]
+        # initial and final info
+        if ri is not None:  # add initial reaction coordinate
+            x.insert(0, ri)
+        if rf is not None:  # add final reaction coordinate
+            x.append(rf)
+
+        if ei is not None:  # add initial energy
+            y.insert(0, ei)
+        if ef is not None:  # add final energy
+            y.append(ef)
+
+        rcs = np.array(x)
+        dEs = np.array(y)
+
+        print(f"如果NEB任务不计算初末态的自洽，{datafile}中将缺失相关信息，需要手动输入")
+        dire = os.getcwd()
+
+    else:
+        raise ValueError("请指定datafile或directory！")
+
+    # import scipy interpolater
+    try:
+        interpolate_method = getattr(
+            __import__("scipy.interpolate", fromlist=[method]), method
+        )
+    except:
+        raise ImportError(f"无法找到 scipy.interpolate.{method} 算法！")
+    # call the interpolater to interpolate with given kwargs
+    try:
+        inter_f = interpolate_method(rcs, dEs, **kwargs)
+    except:
+        raise ValueError(f"插值失败，请检查{kwargs}参数设置是否有误！")
+
+    xnew = np.linspace(rcs[0], rcs[-1], 100)
+    ynew = inter_f(xnew)
+
+    if raw:
+        pd.DataFrame({"x_raw": rcs, "y_raw": dEs}).to_csv("raw_xy.csv", index=False)
+        pd.DataFrame({"x_interpolated": xnew, "y_interpolated": ynew}).to_csv(
+            "raw_interpolated_xy.csv", index=False
+        )
+        
+    # plot
+    plt.clf()
+    if kwargs:
+        plt.plot(xnew, ynew, label=method + str(kwargs))
+    else:
+        plt.plot(xnew, ynew, label=method)
+    plt.scatter(rcs, dEs, c="r")
+    plt.xlabel("Reaction Coordinate (Å)")
+    plt.ylabel("Energy (eV)")
+    plt.legend()
+
+    # save and show
+    if figname:
+        plt.tight_layout()
+        plt.savefig(f"{dire}/{figname}")
+        print("能垒图已保存为", os.path.abspath(f"{dire}/{figname}"))
+    if show:  # 画子图的话，不应每个子图都show
+        plt.show()  # show会自动清空图片
+
+
+def plot_neb_converge(
+    neb_dir: str,
+    image_key: str = "01",
+    show: bool = True,
+    image_name: str = "neb_conv.png",
+    raw=False,
+):
+    """指定NEB计算路径，绘制NEB收敛过程图
+
+    Parameters
+    ----------
+    neb_dir : str
+        NEB计算路径
+    image_key : str
+        第几个构型，默认 "01"
+    show : bool
+        是否交互绘图
+    image_name : str
+        NEB收敛图名称，默认 "neb_conv.png"
+    raw : bool
+        是否输出原始数据到csv文件
+
+    Returns
+    -------
+    ax1, ax2 : matplotlib.axes.Axes
+        两个子图的Axes对象
+
+    Examples
+    --------
+    >>> from dspawpy.diffusion.nebtools import plot_neb_converge
+    >>> plot_neb_converge(neb_dir='my_neb_task', image_key='01')
+    """
+    assert os.path.isdir(neb_dir), f"目录{neb_dir}不存在"
+
+    if os.path.exists(os.path.join(neb_dir, "neb.h5")):
+        neb_total = h5py.File(os.path.join(neb_dir, "neb.h5"))
+        # new output (>=2022B)
+        if "/LoopInfo/01/MaxForce" in neb_total.keys():
+            maxforce = np.array(neb_total.get("/LoopInfo/" + image_key + "/MaxForce"))
+        else:  # old output
+            maxforce = np.array(neb_total.get("/Iteration/" + image_key + "/MaxForce"))
+
+        if "/LoopInfo/01/TotalEnergy" in neb_total.keys():  # new output (>=2022B)
+            total_energy = np.array(
+                neb_total.get("/LoopInfo/" + image_key + "/TotalEnergy")
+            )
+        else:  # old output
+            total_energy = np.array(
+                neb_total.get("/Iteration/" + image_key + "/TotalEnergy")
+            )
+
+    elif os.path.exists(os.path.join(neb_dir, "neb.json")):
+        with open(os.path.join(neb_dir, "neb.json"), "r") as fin:
+            neb_total = json.load(fin)
+        if "LoopInfo" in neb_total.keys():
+            neb = neb_total["LoopInfo"][image_key]
+        else:
+            neb = neb_total["Iteration"][image_key]
+        maxforce = []
+        total_energy = []
+        for n in neb:
+            maxforce.append(n["MaxForce"])
+            total_energy.append(n["TotalEnergy"])
+
+        maxforce = np.array(maxforce)
+        total_energy = np.array(total_energy)
+
+    else:
+        print(
+            f"请检查{os.path.join(neb_dir, 'neb.h5')}或{os.path.join(neb_dir, 'neb.h5')}是否都存在！"
+        )
+
+    x = np.arange(len(maxforce))
+
+    force = maxforce
+    energy = total_energy
+
+    if raw:
+        pd.DataFrame({"x": x, "force": force, "energy": energy}).to_csv(
+            "neb_conv.csv", index=False
+        )
+
+    fig = plt.figure()
+    ax1 = fig.add_subplot(111)
+    ax1.plot(x, force, label="Max Force", c="black")
+    ax1.set_xlabel("Number of ionic step")
+    ax1.set_ylabel("Force (eV/Å)")
+    ax2 = ax1.twinx()
+    ax2.plot(x, energy, label="Energy", c="r")
+    ax2.set_xlabel("Number of ionic step")
+    ax2.set_ylabel("Energy (eV)")
+    ax2.ticklabel_format(useOffset=False)  # y轴坐标显示绝对值而不是相对值
+    fig.legend(loc=1, bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)
+    if image_name:
+        plt.tight_layout()
+        plt.savefig(image_name)
+    if show:
+        plt.show()
+
+    return ax1, ax2
+
+
+def printef(directory):
+    """打印NEB计算时各构型的能量和受力
+
+    Parameters
+    ----------
+    directory : str
+        NEB计算的目录，默认为当前目录
+
+    Returns
+    -------
+    打印各构型的能量和受力
+
+    Examples
+    --------
+    >>> from dspawpy.diffusion.nebtools import printef
+    >>> printef(directory='.')
+    构型    受力(eV/Å)      反应坐标(Å)     此构型的能量(eV)        与初始构型的能量差(eV)
+    00        0.1869          0.0000         -42922.7554                  0.0000
+    01        0.0874          0.7076         -42922.4959                  0.2595
+    02        0.0399          1.4144         -42921.9633                  0.7921
+    03        0.0618          2.1067         -42922.0385                  0.7169
+    04        0.0425          2.7983         -42921.9606                  0.7948
+    05        0.8643          3.5299         -42922.4642                  0.2912
+    06        0.0352          4.2620         -42922.7012                  0.0542
+    """
+    subfolders, resort_mfs, rcs, ens, dEs = _getef(directory)
+    # printout summary
+    print("构型\t受力(eV/Å)\t反应坐标(Å)\t此构型的能量(eV)\t与初始构型的能量差(eV)")
+    for i in range(len(subfolders)):  # 注意格式化输出，对齐
+        print(
+            "%s\t%8.4f\t%8.4f\t%12.4f\t%20.4f"
+            % (subfolders[i], resort_mfs[i], rcs[i], ens[i][0], dEs[i])
+        )
+
+
+def restart(direrctory: str, inputin: str, output: str):
+    """将旧NEB任务归档压缩，并在原路径下准备续算
+
+    Parameters
+    ----------
+    direrctory : str
+        旧NEB任务所在路径，默认当前路径
+    inputin : str
+        输入参数文件名，默认input.in
+    output : str
+        备份文件夹路径
+
+    Examples
+    ----------
+    >>> from dspawpy.diffusion.nebtools import restart
+    >>> restart(direrctory='source', inputin='input.in', output='backup')
+    """
+    if output == "":
+        raise ValueError("备份文件夹路径不能为空！")
+    elif os.path.isdir(output):
+        raise ValueError("备份文件夹已存在！")
+
+    if direrctory == "":
+        directory = os.getcwd()
+    if inputin == "":
+        inputin = "input.in"
+
+    # 读取子文件夹名称列表，仅保留形如00，01数字文件夹路径
+    subfolders = get_neb_subfolders(directory)
+    # 创建备份文件夹并进入
+    os.makedirs(f"{directory}/{output}")
+    os.chdir(f"{directory}/{output}")
+    # 将-0改成-9可提供极限压缩比
+    os.environ["XZ_OPT"] = "-T0 -0"
+    for subfolder in subfolders:
+        # 备份
+        os.system(f"mv {directory}/{subfolder} ./")
+        # 准备续算用的结构文件
+        os.mkdir(f"{directory}/{subfolder}")
+        latestStructureFile = os.path.join(
+            directory, output, subfolder, "latestStructure%s.as" % subfolder
+        )
+        structureFile = os.path.join(
+            directory, output, subfolder, "structure%s.as" % subfolder
+        )
+        bk_latestStructure = f"{directory}/latestStructure{subfolder}.as"
+        bk_structure = f"{directory}/structure{subfolder}.as"
+
+        if os.path.exists(latestStructureFile):
+            os.system(  # 复制到子文件夹
+                f"cp {latestStructureFile} {directory}/{subfolder}/structure{subfolder}.as"
+            )
+            # 备份latestStructureFile到主目录
+            os.system(f"cp {latestStructureFile} {bk_latestStructure}")
+        elif os.path.exists(structureFile):
+            print(f"未找到{latestStructureFile}，复用{structureFile}续算")
+            os.system(  # 复制到子文件夹
+                f"cp {structureFile} {directory}/{subfolder}/structure{subfolder}.as"
+            )
+        else:
+            raise FileNotFoundError(f"{latestStructureFile}和{structureFile}都不存在！")
+        # 备份structureFile到主目录
+        if os.path.exists(structureFile):
+            os.system(f"cp {structureFile} {bk_structure}")
+
+        # 压缩和移动文件
+        # 若存在latestStructure00.as和structure00.as，则压缩00文件夹并把主结构移入00文件夹
+        if os.path.exists(bk_latestStructure) and os.path.exists(bk_structure):
+            os.system(
+                f"tar -Jcf {subfolder}.tar.xz -C {subfolder} . --remove-files && mkdir {subfolder} && mv {subfolder}.tar.xz {directory}/latestStructure{subfolder}.as {directory}/structure{subfolder}.as {subfolder}/ &"
+            )
+        # 若仅存在latestStructure00.as，则压缩00文件夹并把主结构移入00文件夹
+        elif os.path.exists(bk_latestStructure):
+            os.system(
+                f"tar -Jcf {subfolder}.tar.xz {subfolder} . --remove-files && mkdir {subfolder} && mv {subfolder}.tar.xz {directory}/latestStructure{subfolder}.as {subfolder}/ &"
+            )
+        # 若仅存在structure00.as，则压缩00文件夹并把主结构移入00文件夹
+        elif os.path.exists(bk_structure):
+            os.system(
+                f"tar -Jcf {subfolder}.tar.xz -C {subfolder} . --remove-files && mkdir {subfolder} && mv {subfolder}.tar.xz {directory}/structure{subfolder}.as {subfolder}/ &"
+            )
+        else:  # 如果都不存在，说明备份失败
+            raise FileNotFoundError(f"{bk_latestStructure}和{bk_structure}都不存在！")
+
+    # 备份neb.h5,neb.json和DS-PAW.log
+    if os.path.exists(f"{directory}/neb.json"):
+        os.system(
+            f"mv {directory}/neb.h5 {directory}/neb.json {directory}/DS-PAW.log ./"
+        )
+        os.system(f"tar -Jcf neb.tar.xz neb.h5 neb.json --remove-files &")
+    else:
+        os.system(f"mv {directory}/neb.h5 {directory}/DS-PAW.log ./")
+        os.system(f"tar -Jcf neb.tar.xz neb.h5 --remove-files &")
+
+
+def set_pbc(spo):
+    """根据周期性边界条件将分数坐标分量移入 [-0.5, 0.5) 区间
+
+    Parameters
+    ----------
+    spo : np.ndarray or list
+        分数坐标列表
+
+    Returns
+    -------
+    pbc_spo : np.ndarray
+        符合周期性边界条件的分数坐标列表
+
+    Examples
+    --------
+    >>> from dspawpy.diffusion.nebtools import set_pbc
+    >>> set_pbc([-0.6, 1.2, 2.3])
+    array([0.4, 0.2, 0.3])
+    """
+    # wrap into [-0.5, 0.5)
+    pbc_spo = np.mod(np.array(spo) + 0.5, 1.0) - 0.5
+
+    return pbc_spo
+
+
+def summary(directory: str = ".", raw=False, **kwargs):
+    """NEB任务完成总结，依次执行以下步骤：
+
+    - 1. 打印各构型受力、反应坐标、能量、与初始构型的能量差
+    - 2. 绘制能垒图
+    - 3. 绘制并保存结构优化过程的能量和受力收敛过程图
+
+    Parameters
+    ----------
+    directory : str
+        NEB路径, 默认当前路径
+    raw : bool
+        是否保存原始数据到csv文件
+    **kwargs : dict
+        传递给plot_barrier的参数
+
+    Examples
+    --------
+    >>> from dspawpy.diffusion.nebtools import summary
+    >>> directory = '.' # NEB计算路径，默认当前路径
+    >>> summary(directory)
+    # 若inifin=false，用户必须将自洽的scf.h5或system.json放到初末态子文件夹中
+    """
+    # 1. 绘制能垒图
+    print("--> 1. 打印NEB计算时各构型的能量和受力...")
+    printef(directory)
+
+    # 2. 打印各构型受力、反应坐标、能量、与初始构型的能量差
+    print("\n--> 2. 绘制能垒图...")
+    plot_barrier(directory=directory, raw=raw, **kwargs)
+
+    # 3. 绘制并保存结构优化过程的能量和受力收敛过程图到各构型文件夹中
+    print("\n--> 3. 绘制收敛过程图到各构型文件夹中...")
+    subfolders = get_neb_subfolders(directory)
+    for subfolder in subfolders[1 : len(subfolders) - 1]:
+        print(f"----> {subfolder}/converge.png...")
+        plot_neb_converge(
+            neb_dir=directory,
+            image_key=subfolder,
+            image_name=f"{directory}/{subfolder}/converge.png",
+            raw=raw,
+        )
+    print("\n完成!")
+
+
+def write_movie_json(preview: bool = False, directory: str = ".", step: int = -1):
+    """NEB计算或者初始插值后，读取信息，保存为 neb_movie*.json 文件
+
+    用 Device Studio 打开该文件可以观察结构等信息
+
+    Parameters
+    ----------
+    preview : bool
+        是否预览模式，默认否
+    directory : str
+        计算结果所在目录. 默认当前路径
+    step: int
+        离子步编号. 默认-1，读取整个NEB计算过程信息
+
+    Returns
+    ----------
+    neb_movie*.json文件
+
+    Examples
+    ----------
+    >>> from dspawpy.diffusion.nebtools import write_movie_json
+    # NEB计算完成后要观察轨迹变化全过程，只需指定NEB计算路径即可
+    >>> write_movie_json(directory='source')
+    # NEB计算完成后要观察第n离子步结构，请设置step为n，注意step从1开始计数
+    >>> write_movie_json(directory='source', step=1)
+    # 如果您指定的step数超过NEB实际完成的离子步，将会自动修改为最后一步，实际效果等同于上一行代码
+    >>> write_movie_json(directory='source', step=100)
+    # 另外，如需预览初插结构，请将preview设置为True，并将directory指定为NEB计算主路径
+    >>> write_movie_json(preview=True, directory='source')
+    """
+
+    if preview:  # preview mode, write neb_movie_init.json from structure.as
+        print("正在根据初插结构保存neb_movie_init.json...")
+        try:
+            raw = _from_structures(directory)
+        except FileNotFoundError:
+            print("未找到初始插值结构！")
+        except:
+            print("初始插值结构读取失败！请检查structure*.as文件内容！")
+    else:
+        if step == 0:  # try preview mode to save time
+            try:
+                raw = _from_structures(directory)
+            except:
+                print("未找到初始插值结构，将从计算结果h5或json文件中读取！")
+        else:
+            try:  # read from h5 file
+                raw = _from_h5(directory, step)
+            except FileNotFoundError:
+                try:  # read from json file
+                    raw = _from_json(directory, step)
+                except FileNotFoundError:
+                    print("h5和json文件都不存在！")
+                except json.decoder.JSONDecodeError:
+                    print("json文件格式错误！")
+            except:
+                print("h5文件内容读取失败！")
+
+    _dump_neb_movie_json(raw)
+
+
+def _from_structures(directory: str):
+    """从structure00.as，structure01.as，...，中读取结构信息，
+    写入neb_movie_init，以便用DeviceStudio打开观察
+
+    Parameters
+    ----------
+    directory : str
+        NEB计算路径，默认当前路径
+
+    Returns
+    -------
+    用于json文件的各个数组
+    """
+    output = "neb_movie_init.json"
+    step = 0
+
+    subfolders = get_neb_subfolders(directory)
+    # print(subfolders)
+    nimage = len(subfolders)
+    reactionCoordinates = np.zeros(shape=nimage)  # optional
+    totalEnergies = np.zeros(shape=nimage)  # optional
+    maxForces = np.zeros(shape=nimage - 2)  # optional
+    tangents = np.zeros(shape=nimage - 2)  # optional
+    MaxForces = np.zeros(shape=(nimage - 2, step + 1))  # optional
+    TotalEnergies = np.zeros(shape=(nimage - 2, step + 1))  # optional
+
+    Poses = []  # nimage x Natom x 3 , read
+    Elems = []  # nimage x Natom, read
+    Latvs = []  # nimage x 9, read
+
+    for i, folder in enumerate(subfolders):
+        structure_path = os.path.join(directory, folder, f"structure{folder}.as")
+        if not os.path.exists(structure_path):
+            raise FileNotFoundError(f"请检查{structure_path}是否存在！")
+        pos, ele, lat = get_pos_ele_lat(structure_path)
+        Poses.append(pos)
+        Elems.append(ele)
+        Latvs.append(lat)
+
+    Natom = len(Elems[0])
+
+    # reshape data
+    Poses = np.array(Poses).reshape((nimage, Natom, 3))
+    Elems = np.array(Elems).reshape((nimage, Natom))
+    Latvs = np.array(Latvs).reshape((nimage, 9))
+    Fixs = np.zeros(shape=(Natom, 3))  # optional
+
+    return (
+        output,
+        subfolders,
+        step,
+        MaxForces,
+        TotalEnergies,
+        Poses,
+        Latvs,
+        Elems,
+        Fixs,
+        reactionCoordinates,
+        totalEnergies,
+        maxForces,
+        tangents,
+    )
+
+
+def _from_h5(directory: str, step: int):
+    """从NEB路径下的h5文件读取指定step数的结构和能量信息，
+    写入json文件，以便用DeviceStudio打开观察
+
+    Parameters
+    ----------
+    directory : str
+        NEB路径，默认当前路径
+    step : int
+        step数，默认-1，读取最后一个构型
+
+    Returns
+    -------
+    用于json文件的各个数组
+    """
+    # ^ 前期设置
+    neb_h5 = os.path.join(directory, "01/neb01.h5")
+    data = h5py.File(neb_h5)
+    total_steps = np.array(data.get("/NebSize"))[0]
+
+    if step == -1:
+        output = "neb_movie_last.json"
+        step = total_steps - 1
+        print("正在根据最后一个离子步信息生成neb_movie_last.json...")
+    elif step > total_steps - 1:
+        output = "neb_movie_last.json"
+        step = total_steps - 1
+        print(f"您指定的step数大于NEB计算实际完成的总离子步数{total_steps}")
+        print("正在根据最后一个离子步信息生成neb_movie_last.json...")
+    else:
+        output = "neb_movie_{}.json".format(step)
+        print(f"正在根据第{step}个离子步信息生成{output}...")
+
+    # ^ 读取前，准备好json文件所需数组框架
+    subfolders = get_neb_subfolders(directory)
+    nimage = len(subfolders)
+    reactionCoordinates = np.zeros(shape=nimage)  # optional
+    totalEnergies = np.zeros(shape=nimage)  # optional，每个构型最终能量
+    maxForces = np.zeros(shape=nimage - 2)  # optional
+    tangents = np.zeros(shape=nimage - 2)  # optional
+    MaxForces = np.zeros(shape=(nimage - 2, step + 1))  # optional
+    TotalEnergies = np.zeros(shape=(nimage - 2, step + 1))  # optional，中间构型每个离子步能量
+    Sposes = []  # nimage x Natom x 3 , read
+    Elems = []  # nimage x Natom, read
+    Latvs = []  # nimage x 9, read
+    Fixs = []  # Natom x 3, set
+
+    for folder in subfolders:
+        """如果是首尾两个构型，最多只有scf.h5文件，没有neb.h5文件
+        用户如果算NEB的时候，不计算首尾构型的自洽，
+         或者在别处算完了但是没有复制到首尾文件夹中并命名为scf.h5，
+          便不能使用第一个功能
+        """
+        if folder == subfolders[0] or folder == subfolders[-1]:
+            h5_path = os.path.join(directory, folder, "scf.h5")
+        else:
+            h5_path = os.path.join(directory, folder, f"neb{folder}.h5")
+        assert os.path.exists(h5_path), f"请确认{h5_path}是否存在！"
+
+    # ^ 开始分功能读取数据
+    for i, folder in enumerate(subfolders):
+        if folder == subfolders[0] or folder == subfolders[-1]:
+            h5_path = os.path.join(directory, folder, "scf.h5")
+            data = h5py.File(h5_path)
+            # 不影响可视化，直接定为0
+            if folder == subfolders[0]:
+                reactionCoordinates[i] = 0
+            pos = np.array(data.get("/Structures/Step-1/Position"))  # scaled
+            lat = np.array(data.get("/Structures/Step-1/Lattice"))
+
+        else:
+            h5_path = os.path.join(directory, folder, f"neb{folder}.h5")
+            data = h5py.File(h5_path)
+            # reading...
+            reactionCoordinates[i - 1] = np.array(data.get("/Distance/Previous"))[-1]
+            maxForces[i - 1] = np.array(data.get("/MaxForce"))[-1]
+            tangents[i - 1] = np.array(data.get("/Tangent"))[-1]
+            if folder == subfolders[-2]:
+                reactionCoordinates[i + 1] = np.array(data.get("/Distance/Next"))[-1]
+            # read MaxForces and TotalEnergies
+            nionStep = np.array(data.get("/MaxForce")).shape[0]
+            assert step <= nionStep, f"总共只完成了{nionStep}个离子步!"
+            for j in range(step + 1):
+                MaxForces[i - 1, j] = np.array(data.get("/MaxForce"))[j]
+                TotalEnergies[i - 1, j] = np.array(data.get("/TotalEnergy"))[j]
+
+            # read the latest structure for visualization
+            pos = np.array(data.get(f"/Structures/Step-{step+1}/Position"))  # scaled
+            lat = np.array(data.get(f"/Structures/Step-{step+1}/Lattice"))
+
+        totalEnergies[i] = np.array(data.get("/Energy/TotalEnergy0"))
+
+        elems = get_ele_from_h5(hpath=h5_path)
+        Elems.append(elems)
+
+        Sposes.append(pos)
+        Latvs.append(lat)
+
+    Natom = len(Elems[0])
+    tdata = h5py.File(os.path.join(directory, "neb.h5"))
+    # atom fix, not lattice
+    # ignore this trivial message because it is not necessary for the visualization
+    if "/UnrelaxStructure/Image00/Fix" in tdata:
+        fix_array = np.array(tdata.get("/UnrelaxStructure/Image00/Fix"))
+        for fix in fix_array:
+            if fix == 0.0:
+                F = False
+            elif fix == 1.0:
+                F = True
+            else:
+                raise ValueError("Fix值只能为0或1")
+            Fixs.append(F)
+    else:
+        Fixs = np.full(shape=(Natom, 3), fill_value=False)
+
+    # 累加reactionCoordinates中的元素
+    for i in range(1, len(reactionCoordinates)):
+        reactionCoordinates[i] += reactionCoordinates[i - 1]
+
+    # reshape data
+    Sposes = np.array(Sposes).reshape((nimage, Natom, 3))
+    Elems = np.array(Elems).reshape((nimage, Natom))
+    Latvs = np.array(Latvs).reshape((nimage, 9))
+    Fixs = np.array(Fixs).reshape((Natom, 3))
+
+    return (
+        output,
+        subfolders,
+        step,
+        MaxForces,
+        TotalEnergies,  #
+        Sposes,
+        Latvs,
+        Elems,
+        Fixs,
+        reactionCoordinates,
+        totalEnergies,
+        maxForces,
+        tangents,
+    )
+
+
+def _from_json(directory: str, step: int):
+    """从NEB路径下的json文件读取指定step数的结构和能量信息，
+    写入json文件，以便用DeviceStudio打开观察
+
+    Parameters
+    ----------
+    directory : str
+        NEB路径，默认当前路径
+    step : int
+        step数，默认-1，读取最后一个构型
+
+    Returns
+    -------
+    用于json文件的各个数组
+    """
+
+    # ^ 前期设置
+    neb_js = os.path.join(directory, "01/neb01.json")
+    with open(neb_js, "r") as f:
+        data = json.load(f)
+    total_steps = len(data)
+
+    if step == -1:
+        output = "neb_movie_last.json"
+        step = total_steps - 1
+        print("正在根据最后一个离子步信息生成neb_movie_last.json（h5文件不存在，尝试从json文件读取数据）")
+    elif step > total_steps - 1:
+        output = "neb_movie_last.json"
+        step = total_steps - 1
+        print(f"您指定的step数大于NEB计算实际完成的总离子步数{total_steps}")
+        print("正在根据最后一个离子步信息生成neb_movie_last.json（h5文件不存在，尝试从json文件读取数据）")
+    else:
+        output = f"neb_movie_{step}.json"
+        print(f"正在根据第{step}个离子步信息生成{output}...")
+
+    # ^ 读取前，准备好json文件所需数组框架
+    subfolders = get_neb_subfolders(directory)
+    nimage = len(subfolders)
+    reactionCoordinates = np.zeros(shape=nimage)  # optional
+    totalEnergies = np.zeros(shape=nimage)  # optional，每个构型最终能量
+    maxForces = np.zeros(shape=nimage - 2)  # optional
+    tangents = np.zeros(shape=nimage - 2)  # optional
+    MaxForces = np.zeros(shape=(nimage - 2, step + 1))  # optional
+    TotalEnergies = np.zeros(shape=(nimage - 2, step + 1))  # optional，中间构型每个离子步能量
+    Sposes = []  # nimage x Natom x 3 , read
+    Elems = []  # nimage x Natom, read
+    Latvs = []  # nimage x 9, read
+    Fixs = []  # Natom x 3, set
+
+    for folder in subfolders:
+        """如果是首尾两个构型，最多只有system.json文件，没有neb*.json文件
+        用户如果算NEB的时候，不计算首尾构型的自洽，
+         或者在别处算完了但是没有复制到首尾文件夹中并命名为system.json，
+          便不能使用第一个功能
+        """
+        if folder == subfolders[0] or folder == subfolders[-1]:
+            js_path = os.path.join(directory, folder, "system.json")
+        else:
+            js_path = os.path.join(directory, folder, f"neb{folder}.json")
+        assert os.path.exists(js_path), f"请确认{js_path}是否存在！"
+
+    # ^ 开始分功能读取数据
+    for i, folder in enumerate(subfolders):
+        if i == 0:  # 初末态在NEB计算过程中不会优化结构
+            # 1. 外部自洽后移动system.json
+            js_path = os.path.join(directory, folder, "system.json")
+            # 2. 直接NEB计算，得到system00.json
+            neb_js_path = os.path.join(directory, folder, f"system{folder}.json")
+            if os.path.exists(neb_js_path):  # 优先读取neb计算得到的system00.json
+                with open(neb_js_path, "r") as f:
+                    data = json.load(f)
+
+            elif os.path.exists(js_path):
+                with open(js_path, "r") as f:
+                    data = json.load(f)
+
+            else:
+                raise FileNotFoundError(
+                    f"{os.path.abspath(js_path)}和{os.path.abspath(neb_js_path)}均不存在！"
+                )
+
+            lat = data["AtomInfo"]["Lattice"]
+            Latvs.append(lat)
+
+            Natom = len(data["AtomInfo"]["Atoms"])  # 读取原子数
+            for j in range(Natom):
+                pos = data["AtomInfo"]["Atoms"][j]["Position"]  # scaled
+                Sposes.append(pos)
+
+            totalEnergies[i] = data["Energy"]["TotalEnergy0"]
+            reactionCoordinates[i] = 0.0
+
+        elif i > 0 and i < nimage - 1:  # 中间构型会优化结构
+            # 读取晶胞矢量、原子坐标
+            relax_json = os.path.join(directory, folder, "relax.json")
+            assert os.path.exists(relax_json), f"{relax_json}不存在！"
+
+            with open(relax_json, "r") as f:
+                rdata = json.load(f)
+
+            lat = rdata[step]["Lattice"]  # 第step步优化后的晶胞
+            Latvs.append(lat)
+
+            Natom = len(rdata[0]["Atoms"])
+            for j in range(Natom):  # for each atom
+                pos = rdata[step]["Atoms"][j]["Position"]  # 第step步优化后的原子坐标
+                Sposes.append(pos)  # ! 输出的都是分数坐标
+
+            # 读取能量和反应坐标
+            nj = os.path.join(directory, folder, f"neb{folder}.json")
+            with open(nj, "r") as f:
+                ndata = json.load(f)
+
+            totalEnergies[i] = ndata[step]["TotalEnergy"]  # 读取第step步优化后的能量
+
+            # 读取与前一个构型相比的反应坐标
+            reactionCoordinates[i - 1] = ndata[step]["ReactionCoordinate"][-2]
+            tangents[i - 1] = ndata[step]["Tangent"]
+            if folder == subfolders[-2]:  # 末态前一个构型的计算结果中读取反应坐标
+                reactionCoordinates[i + 1] = ndata[step]["ReactionCoordinate"][-1]
+            for j in range(step + 1):
+                MaxForces[i - 1, j] = ndata[j]["MaxForce"]
+                # TODO neb01.json中不存在TotalEnergy0，暂时读取TotalEnergy
+                TotalEnergies[i - 1, j] = ndata[j]["TotalEnergy"]
+
+        else:  # 末态构型
+            js_path = os.path.join(directory, folder, "system.json")
+            neb_js_path = os.path.join(directory, folder, f"system{folder}.json")
+            if os.path.exists(neb_js_path):  # 优先读取neb计算得到的json文件
+                with open(neb_js_path, "r") as f:
+                    data = json.load(f)
+
+            elif os.path.exists(js_path):
+                with open(js_path, "r") as f:
+                    data = json.load(f)
+
+            else:
+                raise FileNotFoundError(
+                    f"{os.path.abspath(js_path)}和{os.path.abspath(neb_js_path)}均不存在！"
+                )
+
+            lat = data["AtomInfo"]["Lattice"]
+            Latvs.append(lat)
+
+            Natom = len(data["AtomInfo"]["Atoms"])  # 读取原子数
+            for j in range(Natom):
+                pos = data["AtomInfo"]["Atoms"][j]["Position"]  # scaled
+                Sposes.append(pos)
+
+            energy = data["Energy"]["TotalEnergy0"]
+            totalEnergies[i] = energy
+
+    # 读取原子元素
+    tneb_js = os.path.join(directory, "neb.json")
+    with open(tneb_js, "r") as f:
+        tdata = json.load(f)
+
+    Natom = len(tdata["UnrelaxStructure"][0]["Atoms"])
+    elems = []
+    for k in range(Natom):
+        ele = tdata["UnrelaxStructure"][0]["Atoms"][k]["Element"]
+        elems.append(ele)
+
+    for ni in range(nimage):
+        Elems.append(elems)  # 重复nimage次，保持Elems结构一致
+
+    for atom in range(Natom):
+        fix_array = tdata["UnrelaxStructure"][1]["Atoms"][atom]["Fix"]  # (1,3)
+        if fix_array == []:  # empty list
+            fix_array = [0.0, 0.0, 0.0]
+        for fix in fix_array:
+            if fix == 0.0:
+                F = False
+            elif fix == 1.0:
+                F = True
+            else:
+                raise ValueError("Fix值只能为 0.0 或 1.0")
+            Fixs.append(F)
+
+    # 累加reactionCoordinates中的元素
+    for i in range(1, len(reactionCoordinates)):
+        reactionCoordinates[i] += reactionCoordinates[i - 1]
+
+    # reshape data
+    Sposes = np.array(Sposes).reshape((nimage, Natom, 3))
+    Elems = np.array(Elems).reshape((nimage, Natom))
+    Latvs = np.array(Latvs).reshape((nimage, 9))
+    Fixs = np.array(Fixs).reshape((Natom, 3))
+
+    return (
+        output,
+        subfolders,
+        step,
+        MaxForces,
+        TotalEnergies,
+        Sposes,
+        Latvs,
+        Elems,
+        Fixs,
+        reactionCoordinates,
+        totalEnergies,
+        maxForces,
+        tangents,
+    )
+
+
+def _dump_neb_movie_json(raw):
+    """根据之前收集到的各数据列表，dump json文件到output"""
+    (
+        output,
+        subfolders,
+        step,
+        MaxForces,
+        TotalEnergies,
+        Poses,
+        Latvs,
+        Elems,
+        Fixs,
+        reactionCoordinates,
+        totalEnergies,
+        maxForces,
+        tangents,
+    ) = raw
+
+    IterDict = {}
+    for s, sf in enumerate(subfolders):
+        if sf == subfolders[0] or sf == subfolders[-1]:
+            continue
+        else:
+            Eflist = []
+            for l in range(step + 1):
+                ef = {
+                    "MaxForce": MaxForces[s - 1, l],
+                    "TotalEnergy": TotalEnergies[s - 1, l],
+                }
+                Eflist.append(ef)
+                iterDict = {sf: Eflist}  # construct sub-dict
+                IterDict.update(iterDict)  # append sub-dict
+
+    RSList = []
+    """
+    从外到内依次遍历 构型、原子（子字典）
+    原子的键值对为：'Atoms': 原子信息列表
+    原子信息列表是一个由字典组成的列表，每个字典对应一个原子的信息
+    """
+    for s, sf in enumerate(subfolders):
+        pos = Poses[s]
+        lat = Latvs[s]
+        elem = Elems[s]
+        atoms = []
+        for i in range(len(elem)):
+            atom = {
+                "Element": elem[i],
+                "Fix": Fixs[i].tolist(),
+                "Mag": [],  # empty
+                "Position": pos[i].tolist(),
+                "Pot": "",
+            }  # empty
+            atoms.append(atom)
+        rs = {"Atoms": atoms, "CoordinateType": "Direct", "Lattice": lat.tolist()}
+        RSList.append(rs)
+
+    URSList = []  # DS似乎并不读取这部分信息，空置即可
+
+    data = {
+        "Distance": {"ReactionCoordinate": reactionCoordinates.tolist()},
+        "Energy": {"TotalEnergy": totalEnergies.tolist()},
+        "Force": {"MaxForce": maxForces.tolist(), "Tangent": tangents.tolist()},
+        "Iteration": IterDict,
+        "RelaxedStructure": RSList,
+        "UnrelaxedStructure": URSList,
+    }
+
+    # ^ 将字典写入json文件
+    with open(output, "w") as f:
+        json.dump(data, f, indent=4)
+
+    print(f"--> {os.path.abspath(output)} 写入成功！\n")
+
+
+def _getef(directory: str = "."):
+    """读取NEB计算时各构型的能量和受力，NEB计算可以未收敛
+    但如果初末态自洽在别处完成，请手动将其移入00等文件夹中！
+
+    Parameters
+    ----------
+    directory: str
+        NEB计算的路径，默认当前路径
+
+    Returns
+    -------
+    subfolders: list
+        构型文件夹名列表
+    resort_mfs: list
+        构型受力的最大分量列表
+    rcs: list
+        反应坐标列表
+    ens: list
+        电子总能列表
+    dEs: list
+        与初始构型的能量差列表
+
+    Examples
+    --------
+    >>> from dspawpy.diffusion.nebtools import getef
+    >>> directory = '.'  # NEB计算的路径，默认当前路径
+    >>> subfolders, resort_mfs, rcs, ens, dEs = getef(directory)
+    >>> print(subfolders)
+    ['00', '01', '02', '03', '04', '05', '06']
+    >>> print(resort_mfs)
+    [0.186876802632, 0.087370747812, 0.039940517914, 0.061764168138, 0.042459986924, 0.864285419627, 0.035218915742]
+    >>> print(rcs)
+    [0.         0.70762104 1.41440524 2.10669668 2.79829788 3.52986152 4.26191179]
+    >>> print(ens)
+    [-42922.75544084902, -42922.495919381065, -42921.96332356572, -42922.038481743206, -42921.960565486996, -42922.464230317, -42922.70115500509]
+    >>> print(dEs)
+    [0.         0.25952147 0.79211728 0.71695911 0.79487536 0.29121053 0.05428584]
+    """
+
+    subfolders = get_neb_subfolders(directory)
+    Nimage = len(subfolders)
+
+    ens = []
+    dEs = np.zeros(Nimage)
+    rcs = [0]
+    mfs = []
+
+    # read energies
+    count = 1
+    for i, subfolder in enumerate(subfolders):
+        if i == 0 or i == Nimage - 1:
+            jsf = os.path.join(directory, subfolder, f"system{subfolder}.json")
+            old_jsf = os.path.join(directory, subfolder, "system.json")
+            hf = os.path.join(directory, subfolder, "scf.h5")
+
+            if os.path.exists(hf):  # 优先读取h5文件内容
+                data = h5py.File(hf)
+                en = np.array(data.get("/Energy/TotalEnergy0"))
+                if i == 0 or i == Nimage - 1:
+                    mf = np.max(np.abs(np.array(data.get("/Force/ForceOnAtoms"))))
+                    mfs.append(mf)
+
+            elif os.path.exists(jsf):  # 其次读取json文件内容
+                with open(jsf, "r") as f:
+                    data = json.load(f)
+                en = data["Energy"]["TotalEnergy0"]
+                if i == 0 or i == Nimage - 1:
+                    mf = np.max(np.abs(data["Force"]["ForceOnAtoms"]))
+                    mfs.append(mf)
+
+            elif os.path.exists(old_jsf):  # 兼容老json
+                with open(old_jsf, "r") as f:
+                    data = json.load(f)
+                en = data["Energy"]["TotalEnergy0"]
+                if i == 0 or i == Nimage - 1:
+                    mf = np.max(np.abs(data["Force"]["ForceOnAtoms"]))
+                    mfs.append(mf)
+
+            else:
+                raise FileNotFoundError(
+                    "无法找到记录构型%s的能量和受力的system.json或scf.h5文件" % subfolder
+                )
+            ens.append(en)
+
+        else:
+            jsf = os.path.join(directory, subfolder, f"neb{subfolder}.json")
+            sysjsf = os.path.join(directory, subfolder, f"system{subfolder}.json")
+            old_sysjsf = os.path.join(directory, subfolder, "system.json")
+            hf = os.path.join(directory, subfolder, f"neb{subfolder}.h5")
+
+            if os.path.exists(hf):  # 优先读取h5文件内容
+                data = h5py.File(hf)
+                en = np.array(data.get("/Energy/TotalEnergy0"))
+                mf = np.array(data.get("/MaxForce"))[-1]
+                # the key may change depends on your DS-PAW version
+                if "/Distance/Previous" in data:
+                    rc = np.array(data.get("/Distance/Previous"))[-1]
+                elif "/ReactionCoordinate" in data:
+                    rc = np.array(data.get("/ReactionCoordinate"))[-2]
+                else:
+                    raise KeyError("找不到/Distance/Previous或/ReactionCoordinate键！")
+                rcs.append(rc)
+                if count == Nimage - 2:  # before final image
+                    if "/Distance/Next" in data:
+                        rc = np.array(data.get("/Distance/Next"))[-1]
+                    elif "/ReactionCoordinate" in data:
+                        rc = np.array(data.get("/ReactionCoordinate"))[-1]
+                    else:
+                        raise KeyError("找不到/Distance/Next或/ReactionCoordinate键！")
+                    rcs.append(rc)
+
+            elif os.path.exists(jsf):
+                if os.path.exists(sysjsf):
+                    with open(sysjsf, "r") as f:
+                        data = json.load(f)
+                    en = data["Energy"]["TotalEnergy0"]
+                elif os.path.exists(old_sysjsf):  # 兼容旧版DS-PAW
+                    with open(old_sysjsf, "r") as f:
+                        data = json.load(f)
+                    en = data["Energy"]["TotalEnergy0"]
+                else:
+                    raise FileNotFoundError(f"无法找到{sysjsf}或{old_sysjsf}")
+
+                with open(jsf, "r") as f:
+                    data = json.load(f)
+                Nion_step = len(data)
+                # en = data[Nion_step - 1]["TotalEnergy"] # invalid
+                mf = data[Nion_step - 1]["MaxForce"]  # 最后一步的最大受力
+                rc = data[Nion_step - 1]["ReactionCoordinate"][0]  # 最后一步的反应坐标
+                rcs.append(rc)
+                if count == Nimage - 2:  # before final image
+                    rc = data[Nion_step - 1]["ReactionCoordinate"][1]  # 最后一步的反应坐标
+                    rcs.append(rc)
+
+            else:
+                raise FileNotFoundError(f"无法找到{hf}或{jsf}")
+
+            ens.append(en)
+            mfs.append(mf)
+
+            # get dE
+            dE = ens[count] - ens[0]
+            dEs[i] = dE
+            count += 1
+    dEs[-1] = ens[Nimage - 1] - ens[0]
+
+    # rcs 改成累加值
+    for i in range(1, len(rcs)):
+        rcs[i] += rcs[i - 1]
+
+    rcs = np.array(rcs)
+
+    resort_mfs = [mfs[0]]
+    final_mf = mfs[1]
+    for j in range(2, len(mfs)):
+        resort_mfs.append(mfs[j])
+    resort_mfs.append(final_mf)
+
+    return subfolders, resort_mfs, rcs, ens, dEs
```

## dspawpy/diffusion/pathfinder.py

 * *Ordering differences only*

```diff
@@ -1,284 +1,284 @@
-# -*- coding: utf-8 -*-
-import warnings
-import itertools
-
-import numpy as np
-from pymatgen.core.periodic_table import get_el_sp
-from pymatgen.core import Structure, PeriodicSite
-
-
-# copy from pymatgen-analysis-diffusion https://github.com/materialsvirtuallab/pymatgen-analysis-diffusion
-class IDPPSolver:
-    """
-    A solver using image dependent pair potential (IDPP) algo to get an improved
-    initial NEB path. For more details about this algo, please refer to
-    Smidstrup et al., J. Chem. Phys. 140, 214106 (2014).
-
-    """
-
-    def __init__(self, structures):
-        """
-        Initialization.
-
-        Args:
-            structures (list of pmg_structure) : Initial guess of the NEB path
-                (including initial and final end-point structures).
-        """
-
-        latt = structures[0].lattice
-        natoms = structures[0].num_sites
-        nimages = len(structures) - 2
-        target_dists = []
-
-        # Initial guess of the path (in Cartesian coordinates) used in the IDPP
-        # algo.
-        init_coords = []
-
-        # Construct the set of target distance matrices via linear interpolation
-        # between those of end-point structures.
-        for i in range(1, nimages + 1):
-            # Interpolated distance matrices
-            dist = structures[0].distance_matrix + i / (nimages + 1) * (
-                structures[-1].distance_matrix - structures[0].distance_matrix
-            )
-
-            target_dists.append(dist)
-
-        target_dists = np.array(target_dists)
-
-        # Set of translational vector matrices (anti-symmetric) for the images.
-        translations = np.zeros((nimages, natoms, natoms, 3), dtype=np.float64)
-
-        # A set of weight functions. It is set as 1/d^4 for each image. Here,
-        # we take d as the average of the target distance matrix and the actual
-        # distance matrix.
-        weights = np.zeros_like(target_dists, dtype=np.float64)
-        for ni in range(nimages):
-            avg_dist = (target_dists[ni] + structures[ni + 1].distance_matrix) / 2.0
-            weights[ni] = 1.0 / (
-                avg_dist**4 + np.eye(natoms, dtype=np.float64) * 1e-8
-            )
-
-        for ni, i in itertools.product(range(nimages + 2), range(natoms)):
-            frac_coords = structures[ni][i].frac_coords
-            init_coords.append(latt.get_cartesian_coords(frac_coords))
-
-            if ni not in [0, nimages + 1]:
-                for j in range(i + 1, natoms):
-                    img = latt.get_distance_and_image(
-                        frac_coords, structures[ni][j].frac_coords
-                    )[1]
-                    translations[ni - 1, i, j] = latt.get_cartesian_coords(img)
-                    translations[ni - 1, j, i] = -latt.get_cartesian_coords(img)
-
-        self.init_coords = np.array(init_coords).reshape(nimages + 2, natoms, 3)
-        self.translations = translations
-        self.weights = weights
-        self.structures = structures
-        self.target_dists = target_dists
-        self.nimages = nimages
-
-    def run(
-        self,
-        maxiter=1000,
-        tol=1e-5,
-        gtol=1e-3,
-        step_size=0.05,
-        max_disp=0.05,
-        spring_const=5.0,
-        species=None,
-    ):
-        """
-        Perform iterative minimization of the set of objective functions in an
-        NEB-like manner. In each iteration, the total force matrix for each
-        image is constructed, which comprises both the spring forces and true
-        forces. For more details about the NEB approach, please see the
-        references, e.g. Henkelman et al., J. Chem. Phys. 113, 9901 (2000).
-
-        Args:
-            maxiter (int): Maximum number of iterations in the minimization
-                process.
-            tol (float): Tolerance of the change of objective functions between
-                consecutive steps.
-            gtol (float): Tolerance of maximum force component (absolute value).
-            step_size (float): Step size associated with the displacement of
-                the atoms during the minimization process.
-            max_disp (float): Maximum allowed atomic displacement in each
-                iteration.
-            spring_const (float): A virtual spring constant used in the NEB-like
-                        relaxation process that yields so-called IDPP path.
-            species (list of string): If provided, only those given species are
-                allowed to move. The atomic positions of other species are
-                obtained via regular linear interpolation approach.
-
-        Returns:
-            [Structure] Complete IDPP path (including end-point structures)
-        """
-
-        coords = self.init_coords.copy()
-        old_funcs = np.zeros((self.nimages,), dtype=np.float64)
-        idpp_structures = [self.structures[0]]
-
-        if species is None:
-            indices = list(range(len(self.structures[0])))
-        else:
-            species = [get_el_sp(sp) for sp in species]
-            indices = [
-                i for i, site in enumerate(self.structures[0]) if site.specie in species
-            ]
-
-            if len(indices) == 0:
-                raise ValueError("The given species are not in the system!")
-
-        # Iterative minimization
-        for n in range(maxiter):
-            # Get the sets of objective functions, true and total force
-            # matrices.
-            funcs, true_forces = self._get_funcs_and_forces(coords)
-            tot_forces = self._get_total_forces(
-                coords, true_forces, spring_const=spring_const
-            )
-
-            # Each atom is allowed to move up to max_disp
-            disp_mat = step_size * tot_forces[:, indices, :]
-            disp_mat = np.where(
-                np.abs(disp_mat) > max_disp, np.sign(disp_mat) * max_disp, disp_mat
-            )
-            coords[1 : (self.nimages + 1), indices] += disp_mat
-
-            max_force = np.abs(tot_forces[:, indices, :]).max()
-            tot_res = np.sum(np.abs(old_funcs - funcs))
-
-            if tot_res < tol and max_force < gtol:
-                break
-
-            old_funcs = funcs
-
-        else:
-            warnings.warn(
-                "Maximum iteration number is reached without convergence!", UserWarning
-            )
-
-        for ni in range(self.nimages):
-            # generate the improved image structure
-            new_sites = []
-
-            for site, cart_coords in zip(self.structures[ni + 1], coords[ni + 1]):
-                new_site = PeriodicSite(
-                    site.species,
-                    coords=cart_coords,
-                    lattice=site.lattice,
-                    coords_are_cartesian=True,
-                    properties=site.properties,
-                )
-                new_sites.append(new_site)
-
-            idpp_structures.append(Structure.from_sites(new_sites))
-
-        # Also include end-point structure.
-        idpp_structures.append(self.structures[-1])
-
-        return idpp_structures
-
-    @classmethod
-    def from_endpoints(cls, endpoints, nimages=5, sort_tol=1.0):
-        """
-        A class method that starts with end-point structures instead. The
-        initial guess for the IDPP algo is then constructed using linear
-        interpolation.
-
-        Args:
-            endpoints (list of Structure objects): The two end-point structures.
-            nimages (int): Number of images between the two end-points.
-            sort_tol (float): Distance tolerance (in Angstrom) used to match the
-                atomic indices between start and end structures. Need
-                to increase the value in some cases.
-        """
-        try:
-            images = endpoints[0].interpolate(
-                endpoints[1], nimages=nimages + 1, autosort_tol=sort_tol
-            )
-        except Exception as e:
-            if "Unable to reliably match structures " in str(e):
-                warnings.warn(
-                    "Auto sorting is turned off because it is unable"
-                    " to match the end-point structures!",
-                    UserWarning,
-                )
-                images = endpoints[0].interpolate(
-                    endpoints[1], nimages=nimages + 1, autosort_tol=0
-                )
-            else:
-                raise e
-
-        return IDPPSolver(images)
-
-    def _get_funcs_and_forces(self, x):
-        """
-        Calculate the set of objective functions as well as their gradients,
-        i.e. "effective true forces"
-        """
-        funcs = []
-        funcs_prime = []
-        trans = self.translations
-        natoms = trans.shape[1]
-        weights = self.weights
-        target_dists = self.target_dists
-
-        for ni in range(len(x) - 2):
-            vec = [x[ni + 1, i] - x[ni + 1] - trans[ni, i] for i in range(natoms)]
-
-            trial_dist = np.linalg.norm(vec, axis=2)
-            aux = (
-                (trial_dist - target_dists[ni])
-                * weights[ni]
-                / (trial_dist + np.eye(natoms, dtype=np.float64))
-            )
-
-            # Objective function
-            func = np.sum((trial_dist - target_dists[ni]) ** 2 * weights[ni])
-
-            # "True force" derived from the objective function.
-            grad = np.sum(aux[:, :, None] * vec, axis=1)
-
-            funcs.append(func)
-            funcs_prime.append(grad)
-
-        return 0.5 * np.array(funcs), -2 * np.array(funcs_prime)
-
-    @staticmethod
-    def get_unit_vector(vec):
-        return vec / np.sqrt(np.sum(vec**2))
-
-    def _get_total_forces(self, x, true_forces, spring_const):
-        """
-        Calculate the total force on each image structure, which is equal to
-        the spring force along the tangent + true force perpendicular to the
-        tangent. Note that the spring force is the modified version in the
-        literature (e.g. Henkelman et al., J. Chem. Phys. 113, 9901 (2000)).
-        """
-
-        total_forces = []
-        natoms = np.shape(true_forces)[1]
-
-        for ni in range(1, len(x) - 1):
-            vec1 = (x[ni + 1] - x[ni]).flatten()
-            vec2 = (x[ni] - x[ni - 1]).flatten()
-
-            # Local tangent
-            tangent = self.get_unit_vector(vec1) + self.get_unit_vector(vec2)
-            tangent = self.get_unit_vector(tangent)
-
-            # Spring force
-            spring_force = (
-                spring_const * (np.linalg.norm(vec1) - np.linalg.norm(vec2)) * tangent
-            )
-
-            # Total force
-            flat_ft = true_forces[ni - 1].copy().flatten()
-            total_force = true_forces[ni - 1] + (
-                spring_force - np.dot(flat_ft, tangent) * tangent
-            ).reshape(natoms, 3)
-            total_forces.append(total_force)
-
-        return np.array(total_forces)
+# -*- coding: utf-8 -*-
+import warnings
+import itertools
+
+import numpy as np
+from pymatgen.core.periodic_table import get_el_sp
+from pymatgen.core import Structure, PeriodicSite
+
+
+# copy from pymatgen-analysis-diffusion https://github.com/materialsvirtuallab/pymatgen-analysis-diffusion
+class IDPPSolver:
+    """
+    A solver using image dependent pair potential (IDPP) algo to get an improved
+    initial NEB path. For more details about this algo, please refer to
+    Smidstrup et al., J. Chem. Phys. 140, 214106 (2014).
+
+    """
+
+    def __init__(self, structures):
+        """
+        Initialization.
+
+        Args:
+            structures (list of pmg_structure) : Initial guess of the NEB path
+                (including initial and final end-point structures).
+        """
+
+        latt = structures[0].lattice
+        natoms = structures[0].num_sites
+        nimages = len(structures) - 2
+        target_dists = []
+
+        # Initial guess of the path (in Cartesian coordinates) used in the IDPP
+        # algo.
+        init_coords = []
+
+        # Construct the set of target distance matrices via linear interpolation
+        # between those of end-point structures.
+        for i in range(1, nimages + 1):
+            # Interpolated distance matrices
+            dist = structures[0].distance_matrix + i / (nimages + 1) * (
+                structures[-1].distance_matrix - structures[0].distance_matrix
+            )
+
+            target_dists.append(dist)
+
+        target_dists = np.array(target_dists)
+
+        # Set of translational vector matrices (anti-symmetric) for the images.
+        translations = np.zeros((nimages, natoms, natoms, 3), dtype=np.float64)
+
+        # A set of weight functions. It is set as 1/d^4 for each image. Here,
+        # we take d as the average of the target distance matrix and the actual
+        # distance matrix.
+        weights = np.zeros_like(target_dists, dtype=np.float64)
+        for ni in range(nimages):
+            avg_dist = (target_dists[ni] + structures[ni + 1].distance_matrix) / 2.0
+            weights[ni] = 1.0 / (
+                avg_dist**4 + np.eye(natoms, dtype=np.float64) * 1e-8
+            )
+
+        for ni, i in itertools.product(range(nimages + 2), range(natoms)):
+            frac_coords = structures[ni][i].frac_coords
+            init_coords.append(latt.get_cartesian_coords(frac_coords))
+
+            if ni not in [0, nimages + 1]:
+                for j in range(i + 1, natoms):
+                    img = latt.get_distance_and_image(
+                        frac_coords, structures[ni][j].frac_coords
+                    )[1]
+                    translations[ni - 1, i, j] = latt.get_cartesian_coords(img)
+                    translations[ni - 1, j, i] = -latt.get_cartesian_coords(img)
+
+        self.init_coords = np.array(init_coords).reshape(nimages + 2, natoms, 3)
+        self.translations = translations
+        self.weights = weights
+        self.structures = structures
+        self.target_dists = target_dists
+        self.nimages = nimages
+
+    def run(
+        self,
+        maxiter=1000,
+        tol=1e-5,
+        gtol=1e-3,
+        step_size=0.05,
+        max_disp=0.05,
+        spring_const=5.0,
+        species=None,
+    ):
+        """
+        Perform iterative minimization of the set of objective functions in an
+        NEB-like manner. In each iteration, the total force matrix for each
+        image is constructed, which comprises both the spring forces and true
+        forces. For more details about the NEB approach, please see the
+        references, e.g. Henkelman et al., J. Chem. Phys. 113, 9901 (2000).
+
+        Args:
+            maxiter (int): Maximum number of iterations in the minimization
+                process.
+            tol (float): Tolerance of the change of objective functions between
+                consecutive steps.
+            gtol (float): Tolerance of maximum force component (absolute value).
+            step_size (float): Step size associated with the displacement of
+                the atoms during the minimization process.
+            max_disp (float): Maximum allowed atomic displacement in each
+                iteration.
+            spring_const (float): A virtual spring constant used in the NEB-like
+                        relaxation process that yields so-called IDPP path.
+            species (list of string): If provided, only those given species are
+                allowed to move. The atomic positions of other species are
+                obtained via regular linear interpolation approach.
+
+        Returns:
+            [Structure] Complete IDPP path (including end-point structures)
+        """
+
+        coords = self.init_coords.copy()
+        old_funcs = np.zeros((self.nimages,), dtype=np.float64)
+        idpp_structures = [self.structures[0]]
+
+        if species is None:
+            indices = list(range(len(self.structures[0])))
+        else:
+            species = [get_el_sp(sp) for sp in species]
+            indices = [
+                i for i, site in enumerate(self.structures[0]) if site.specie in species
+            ]
+
+            if len(indices) == 0:
+                raise ValueError("The given species are not in the system!")
+
+        # Iterative minimization
+        for n in range(maxiter):
+            # Get the sets of objective functions, true and total force
+            # matrices.
+            funcs, true_forces = self._get_funcs_and_forces(coords)
+            tot_forces = self._get_total_forces(
+                coords, true_forces, spring_const=spring_const
+            )
+
+            # Each atom is allowed to move up to max_disp
+            disp_mat = step_size * tot_forces[:, indices, :]
+            disp_mat = np.where(
+                np.abs(disp_mat) > max_disp, np.sign(disp_mat) * max_disp, disp_mat
+            )
+            coords[1 : (self.nimages + 1), indices] += disp_mat
+
+            max_force = np.abs(tot_forces[:, indices, :]).max()
+            tot_res = np.sum(np.abs(old_funcs - funcs))
+
+            if tot_res < tol and max_force < gtol:
+                break
+
+            old_funcs = funcs
+
+        else:
+            warnings.warn(
+                "Maximum iteration number is reached without convergence!", UserWarning
+            )
+
+        for ni in range(self.nimages):
+            # generate the improved image structure
+            new_sites = []
+
+            for site, cart_coords in zip(self.structures[ni + 1], coords[ni + 1]):
+                new_site = PeriodicSite(
+                    site.species,
+                    coords=cart_coords,
+                    lattice=site.lattice,
+                    coords_are_cartesian=True,
+                    properties=site.properties,
+                )
+                new_sites.append(new_site)
+
+            idpp_structures.append(Structure.from_sites(new_sites))
+
+        # Also include end-point structure.
+        idpp_structures.append(self.structures[-1])
+
+        return idpp_structures
+
+    @classmethod
+    def from_endpoints(cls, endpoints, nimages=5, sort_tol=1.0):
+        """
+        A class method that starts with end-point structures instead. The
+        initial guess for the IDPP algo is then constructed using linear
+        interpolation.
+
+        Args:
+            endpoints (list of Structure objects): The two end-point structures.
+            nimages (int): Number of images between the two end-points.
+            sort_tol (float): Distance tolerance (in Angstrom) used to match the
+                atomic indices between start and end structures. Need
+                to increase the value in some cases.
+        """
+        try:
+            images = endpoints[0].interpolate(
+                endpoints[1], nimages=nimages + 1, autosort_tol=sort_tol
+            )
+        except Exception as e:
+            if "Unable to reliably match structures " in str(e):
+                warnings.warn(
+                    "Auto sorting is turned off because it is unable"
+                    " to match the end-point structures!",
+                    UserWarning,
+                )
+                images = endpoints[0].interpolate(
+                    endpoints[1], nimages=nimages + 1, autosort_tol=0
+                )
+            else:
+                raise e
+
+        return IDPPSolver(images)
+
+    def _get_funcs_and_forces(self, x):
+        """
+        Calculate the set of objective functions as well as their gradients,
+        i.e. "effective true forces"
+        """
+        funcs = []
+        funcs_prime = []
+        trans = self.translations
+        natoms = trans.shape[1]
+        weights = self.weights
+        target_dists = self.target_dists
+
+        for ni in range(len(x) - 2):
+            vec = [x[ni + 1, i] - x[ni + 1] - trans[ni, i] for i in range(natoms)]
+
+            trial_dist = np.linalg.norm(vec, axis=2)
+            aux = (
+                (trial_dist - target_dists[ni])
+                * weights[ni]
+                / (trial_dist + np.eye(natoms, dtype=np.float64))
+            )
+
+            # Objective function
+            func = np.sum((trial_dist - target_dists[ni]) ** 2 * weights[ni])
+
+            # "True force" derived from the objective function.
+            grad = np.sum(aux[:, :, None] * vec, axis=1)
+
+            funcs.append(func)
+            funcs_prime.append(grad)
+
+        return 0.5 * np.array(funcs), -2 * np.array(funcs_prime)
+
+    @staticmethod
+    def get_unit_vector(vec):
+        return vec / np.sqrt(np.sum(vec**2))
+
+    def _get_total_forces(self, x, true_forces, spring_const):
+        """
+        Calculate the total force on each image structure, which is equal to
+        the spring force along the tangent + true force perpendicular to the
+        tangent. Note that the spring force is the modified version in the
+        literature (e.g. Henkelman et al., J. Chem. Phys. 113, 9901 (2000)).
+        """
+
+        total_forces = []
+        natoms = np.shape(true_forces)[1]
+
+        for ni in range(1, len(x) - 1):
+            vec1 = (x[ni + 1] - x[ni]).flatten()
+            vec2 = (x[ni] - x[ni - 1]).flatten()
+
+            # Local tangent
+            tangent = self.get_unit_vector(vec1) + self.get_unit_vector(vec2)
+            tangent = self.get_unit_vector(tangent)
+
+            # Spring force
+            spring_force = (
+                spring_const * (np.linalg.norm(vec1) - np.linalg.norm(vec2)) * tangent
+            )
+
+            # Total force
+            flat_ft = true_forces[ni - 1].copy().flatten()
+            total_force = true_forces[ni - 1] + (
+                spring_force - np.dot(flat_ft, tangent) * tangent
+            ).reshape(natoms, 3)
+            total_forces.append(total_force)
+
+        return np.array(total_forces)
```

## dspawpy/io/read.py

 * *Ordering differences only*

```diff
@@ -1,623 +1,623 @@
-# -*- coding: utf-8 -*-
-
-import json
-import h5py
-from typing import List, Dict
-
-import numpy as np
-from pymatgen.core.structure import Structure
-from pymatgen.core.lattice import Lattice
-from pymatgen.electronic_structure.dos import Dos, CompleteDos
-from pymatgen.electronic_structure.core import Spin
-from pymatgen.electronic_structure.core import Orbital
-from pymatgen.electronic_structure.bandstructure import BandStructureSymmLine
-from pymatgen.phonon.bandstructure import PhononBandStructureSymmLine
-from pymatgen.phonon.dos import PhononDos
-
-
-def json2structures(jsonfile):
-    """relax.json/aimd.json -> [pymatgen.Structure]
-
-    Parameters
-    ----------
-    jsonfile : str
-        包含多个离子步的 json 文件路径，例如 "relax.json" 或 "aimd.json"
-
-    Returns
-    -------
-    pymatgen_structures : list
-        pymatgen.Structure 列表
-
-    Examples
-    --------
-    >>> from dspawpy.io.read import json2structures
-    >>> structures = json2structures("relax.json")
-    """
-    with open(jsonfile, "r") as file:
-        j = json.load(file)
-
-    pymatgen_structures = []
-    for step in range(len(j)):
-        atominfo = j[step]["Atoms"]
-        elements = []
-        positions = []
-        for atomindex in range(len(atominfo)):
-            elements.append(atominfo[atomindex]["Element"])
-            positions.append(atominfo[atomindex]["Position"])
-        coords = np.asarray(positions).reshape(-1, 3)
-        lattice = np.asarray(j[step]["Lattice"]).reshape(3, 3)
-        pymatgen_structures.append(
-            Structure(lattice, elements, coords, coords_are_cartesian=True)
-        )
-
-    return pymatgen_structures
-
-
-def load_h5(dir_h5: str) -> dict:
-    """遍历读取h5文件中的数据，保存为字典格式
-
-    慎用此函数，因为会读取很多不需要的数据，耗时很长。
-
-    Parameters
-    ----------
-    dir_h5 : str
-        h5文件路径
-
-    Returns
-    -------
-    datas: dict
-        数据字典
-
-    Examples
-    --------
-    >>> from dspawpy.io.read import load_h5
-    >>> datas = load_h5(dir_h5)
-    """
-
-    def get_names(key, h5_object):
-        names.append(h5_object.name)
-
-    def is_dataset(name):
-        for name_inTheList in names:
-            if name_inTheList.find(name + "/") != -1:
-                return False
-        return True
-
-    def get_datas(key, h5_object):
-        if is_dataset(h5_object.name):
-            data = np.asarray(h5_object)
-            if data.dtype == "|S1":  # 转成字符串 并根据";"分割
-                byte2str = [str(bi, "utf-8") for bi in data]
-                string = ""
-                for char in byte2str:
-                    string += char
-                data = np.array([elem for elem in string.strip().split(";")])
-            # "/group1/group2/.../groupN/dataset" : value
-            datas[h5_object.name] = data.tolist()
-
-    with h5py.File(dir_h5, "r") as fin:
-        names = []
-        datas = {}
-        fin.visititems(get_names)
-        fin.visititems(get_datas)
-
-        return datas
-
-
-def load_h5_todict(dir_h5: str) -> Dict:
-    """与上一个函数区别在于合并了部分同类数据，例如
-
-    /Structures/Step-1/* 和 /Structures/Step-2/* 并入 /Structures/ 组内
-    """
-
-    def create_dict(L: List, D: Dict):
-        if len(L) == 2:
-            D[L[0]] = L[1]
-            return
-        else:
-            if not (L[0] in D.keys()):
-                D[L[0]] = {}
-            create_dict(L[1:], D[L[0]])
-
-    datas = load_h5(dir_h5)
-
-    groups_value_list = []
-    for key in datas.keys():
-        tmp_list = key[1:].strip().split("/")  # [1:] 截去root
-        tmp_list.append(datas[key])
-        # groups_value_list[i]结构: [group1, group2, ..., groupN, dataset, value]
-        groups_value_list.append(tmp_list)
-
-    groups_value_dict = {}
-    for data in groups_value_list:
-        create_dict(data, groups_value_dict)
-
-    return groups_value_dict
-
-
-def get_dos_data(dos_dir: str):
-    if dos_dir.endswith(".h5"):
-        dos = load_h5(dos_dir)
-        if dos["/DosInfo/Project"][0]:
-            return get_complete_dos(dos)
-        else:
-            return get_total_dos(dos)
-
-    elif dos_dir.endswith(".json"):
-        with open(dos_dir, "r") as fin:
-            dos = json.load(fin)
-
-        if dos["DosInfo"]["Project"]:
-            return get_complete_dos_json(dos)
-        else:
-            return get_total_dos_json(dos)
-
-    else:
-        print("file - " + dos_dir + " :  Unsupported format!")
-        return
-
-
-def get_total_dos(dos: Dict) -> Dos:
-    # h5 -> Dos Obj
-    energies = np.asarray(dos["/DosInfo/DosEnergy"])
-    if dos["/DosInfo/SpinType"][0] == "none":
-        densities = {Spin.up: np.asarray(dos["/DosInfo/Spin1/Dos"])}
-    else:
-        densities = {
-            Spin.up: np.asarray(dos["/DosInfo/Spin1/Dos"]),
-            Spin.down: np.asarray(dos["/DosInfo/Spin2/Dos"]),
-        }
-
-    efermi = dos["/DosInfo/EFermi"][0]
-
-    return Dos(efermi, energies, densities)
-
-
-def get_complete_dos(dos: Dict) -> CompleteDos:
-    # h5 -> CompleteDos Obj
-    total_dos = get_total_dos(dos)
-    structure = get_structure(dos, "/AtomInfo")
-    N = len(structure)
-    pdos = [{} for i in range(N)]
-    number_of_spin = 1 if dos["/DosInfo/SpinType"][0] == "none" else 2
-
-    for i in range(number_of_spin):
-        spin_key = "Spin" + str(i + 1)
-        spin = Spin.up if i == 0 else Spin.down
-        atomindexs = dos["/DosInfo/" + spin_key + "/ProjectDos/AtomIndexs"][0]
-        orbitindexs = dos["/DosInfo/" + spin_key + "/ProjectDos/OrbitIndexs"][0]
-        for atom_index in range(atomindexs):
-            for orbit_index in range(orbitindexs):
-                orbit_name = Orbital(orbit_index)
-                Contribution = dos[
-                    "/DosInfo/"
-                    + spin_key
-                    + "/ProjectDos"
-                    + str(atom_index + 1)
-                    + "/"
-                    + str(orbit_index + 1)
-                ]
-                if orbit_name in pdos[atom_index].keys():
-                    pdos[atom_index][orbit_name].update({spin: Contribution})
-                else:
-                    pdos[atom_index][orbit_name] = {spin: Contribution}
-
-    pdoss = {structure[i]: pd for i, pd in enumerate(pdos)}
-
-    return CompleteDos(structure, total_dos, pdoss)
-
-
-def get_total_dos_json(dos: Dict) -> Dos:
-    # json -> Dos Obj
-    energies = np.asarray(dos["DosInfo"]["DosEnergy"])
-    if dos["DosInfo"]["SpinType"] == "none":
-        densities = {Spin.up: np.asarray(dos["DosInfo"]["Spin1"]["Dos"])}
-    else:
-        densities = {
-            Spin.up: np.asarray(dos["DosInfo"]["Spin1"]["Dos"]),
-            Spin.down: np.asarray(dos["DosInfo"]["Spin2"]["Dos"]),
-        }
-    efermi = dos["DosInfo"]["EFermi"]
-    return Dos(efermi, energies, densities)
-
-
-def get_complete_dos_json(dos: Dict) -> CompleteDos:
-    # json -> CompleteDos Obj
-    total_dos = get_total_dos_json(dos)
-    structure = get_structure_json(dos["AtomInfo"])
-    N = len(structure)
-    pdos = [{} for i in range(N)]
-    number_of_spin = 1 if dos["DosInfo"]["SpinType"] == "none" else 2
-
-    for i in range(number_of_spin):
-        spin_key = "Spin" + str(i + 1)
-        spin = Spin.up if i == 0 else Spin.down
-        project = dos["DosInfo"][spin_key]["ProjectDos"]
-        for p in project:
-            atom_index = p["AtomIndex"] - 1
-            o = p["OrbitIndex"] - 1
-            orbit_name = Orbital(o)
-            if orbit_name in pdos[atom_index].keys():
-                pdos[atom_index][orbit_name].update({spin: p["Contribution"]})
-            else:
-                pdos[atom_index][orbit_name] = {spin: p["Contribution"]}
-    pdoss = {structure[i]: pd for i, pd in enumerate(pdos)}
-
-    return CompleteDos(structure, total_dos, pdoss)
-
-
-def get_structure(hdf5: Dict, key: str) -> Structure:
-    # load_h5 -> Structure Obj
-    lattice = np.asarray(hdf5[key + "/Lattice"]).reshape(3, 3)
-    elements = hdf5[key + "/Elements"]
-    positions = hdf5[key + "/Position"]
-    coords = np.asarray(positions).reshape(-1, 3)
-    is_direct = hdf5[key + "/CoordinateType"][0] == "Direct"
-    return Structure(lattice, elements, coords, coords_are_cartesian=(not is_direct))
-
-
-def get_structure_json(atominfo) -> Structure:
-    lattice = np.asarray(atominfo["Lattice"]).reshape(3, 3)
-    elements = []
-    positions = []
-    for atom in atominfo["Atoms"]:
-        elements.append(atom["Element"])
-        positions.extend(atom["Position"])
-
-    coords = np.asarray(positions).reshape(-1, 3)
-    is_direct = atominfo["CoordinateType"] == "Direct"
-    return Structure(lattice, elements, coords, coords_are_cartesian=(not is_direct))
-
-
-def get_structure_from_json(jsonfile: str) -> Structure:
-    with open(jsonfile, "r") as file:
-        j = json.load(file)
-    lattice = np.asarray(j["AtomInfo"]["Lattice"]).reshape(3, 3)
-    elements = j["AtomInfo"]["Elements"]
-    positions = j["AtomInfo"]["Position"]
-    coords = np.asarray(positions).reshape(-1, 3)
-    is_direct = j["AtomInfo"]["CoordinateType"][0] == "Direct"
-    return Structure(lattice, elements, coords, coords_are_cartesian=(not is_direct))
-
-
-def get_band_data_h5(band: Dict, iwan=False):
-    if iwan:
-        bd = "WannBandInfo"
-    else:
-        bd = "BandInfo"
-    number_of_band = band[f"/{bd}/NumberOfBand"][0]
-    number_of_kpoints = band[f"/{bd}/NumberOfKpoints"][0]
-    if (
-        band[f"/{bd}/SpinType"][0] == "none"
-        or band[f"/{bd}/SpinType"][0] == "non-collinear"
-    ):
-        number_of_spin = 1
-    else:
-        number_of_spin = 2
-
-    symmetry_kPoints_index = band[f"/{bd}/SymmetryKPointsIndex"]
-
-    efermi = band[f"/{bd}/EFermi"][0]
-    eigenvals = {}
-    for i in range(number_of_spin):
-        spin_key = "Spin" + str(i + 1)
-        spin = Spin.up if i == 0 else Spin.down
-
-        if f"/{bd}/" + spin_key + "/BandEnergies" in band:
-            data = band[f"/{bd}/" + spin_key + "/BandEnergies"]
-        elif f"/{bd}/" + spin_key + "/Band" in band:
-            data = band[f"/{bd}/" + spin_key + "/Band"]
-        else:
-            print("Band key error")
-            return
-        band_data = np.array(data).reshape((number_of_kpoints, number_of_band)).T
-        eigenvals[spin] = band_data
-
-    kpoints = np.asarray(band[f"/{bd}/CoordinatesOfKPoints"]).reshape(
-        number_of_kpoints, 3
-    )
-
-    structure = get_structure(band, "/AtomInfo")
-    labels_dict = {}
-
-    for i, s in enumerate(band[f"/{bd}/SymmetryKPoints"]):
-        labels_dict[s] = kpoints[symmetry_kPoints_index[i] - 1]
-
-    # read projection data
-    projections = None
-    if f"/{bd}/IsProject" in band.keys():
-        if band[f"/{bd}/IsProject"][0]:
-            projections = {}
-            number_of_orbit = len(band[f"/{bd}/Orbit"])
-            projection = np.zeros(
-                (number_of_band, number_of_kpoints, number_of_orbit, len(structure))
-            )
-
-            for i in range(number_of_spin):
-                spin_key = "Spin" + str(i + 1)
-                spin = Spin.up if i == 0 else Spin.down
-
-                atomindexs = band[f"/{bd}/" + spin_key + "/ProjectBand/AtomIndex"][0]
-                orbitindexs = band[f"/{bd}/" + spin_key + "/ProjectBand/OrbitIndexs"][0]
-                for atom_index in range(atomindexs):
-                    for orbit_index in range(orbitindexs):
-                        project_data = band[
-                            f"/{bd}/"
-                            + spin_key
-                            + "/ProjectBand/1/"
-                            + str(atom_index + 1)
-                            + "/"
-                            + str(orbit_index + 1)
-                        ]
-                        projection[:, :, orbit_index, atom_index] = (
-                            np.asarray(project_data)
-                            .reshape((number_of_kpoints, number_of_band))
-                            .T
-                        )
-                projections[spin] = projection
-
-    return structure, kpoints, eigenvals, efermi, labels_dict, projections
-
-
-def get_band_data_json(band: Dict, iwan=False):
-    if iwan:
-        bd = "WannBandInfo"
-    else:
-        bd = "BandInfo"
-
-    number_of_band = band[f"{bd}"]["NumberOfBand"]
-    number_of_kpoints = band[f"{bd}"]["NumberOfKpoints"]
-    if 'Spin2' in band[f"{bd}"]:
-        number_of_spin = 2
-    else:
-        number_of_spin = 1
-
-    symmetry_kPoints_index = band[f"{bd}"]["SymmetryKPointsIndex"]
-
-    if "EFermi" in band[f"{bd}"]:
-        efermi = band[f"{bd}"]["EFermi"]
-    else:
-        efermi = 0 # for wannier
-        
-    eigenvals = {}
-    for i in range(number_of_spin):
-        spin_key = "Spin" + str(i + 1)
-        spin = Spin.up if i == 0 else Spin.down
-
-        if "BandEnergies" in band[f"{bd}"][spin_key]:
-            data = band[f"{bd}"][spin_key]["BandEnergies"]
-        elif "Band" in band[f"{bd}"][spin_key]:
-            data = band[f"{bd}"][spin_key]["Band"]
-        else:
-            print("Band key error")
-            return
-
-        band_data = np.array(data).reshape((number_of_kpoints, number_of_band)).T
-
-        eigenvals[spin] = band_data
-
-    kpoints = np.asarray(band[f"{bd}"]["CoordinatesOfKPoints"]).reshape(
-        number_of_kpoints, 3
-    )
-
-    structure = get_structure_json(band["AtomInfo"])
-    labels_dict = {}
-
-    for i, s in enumerate(band[f"{bd}"]["SymmetryKPoints"]):
-        labels_dict[s] = kpoints[symmetry_kPoints_index[i] - 1]
-
-    # read projection data
-    projections = None
-    if "IsProject" in band[f"{bd}"].keys():
-        if band[f"{bd}"]["IsProject"]:
-            projections = {}
-            number_of_orbit = len(band[f"{bd}"]["Orbit"])
-            projection = np.zeros(
-                (number_of_band, number_of_kpoints, number_of_orbit, len(structure))
-            )
-
-            for i in range(number_of_spin):
-                spin_key = "Spin" + str(i + 1)
-                spin = Spin.up if i == 0 else Spin.down
-
-                data = band[f"{bd}"][spin_key]["ProjectBand"]
-                for d in data:
-                    orbit_index = d["OrbitIndex"] - 1
-                    atom_index = d["AtomIndex"] - 1
-                    project_data = d["Contribution"]
-                    projection[:, :, orbit_index, atom_index] = (
-                        np.asarray(project_data)
-                        .reshape((number_of_kpoints, number_of_band))
-                        .T
-                    )
-
-                projections[spin] = projection
-
-    return structure, kpoints, eigenvals, efermi, labels_dict, projections
-
-
-def get_band_data(band_dir: str, efermi: float = None) -> BandStructureSymmLine:
-    # modify BandStructureSymmLine.efermi after it was created will cause error
-    if band_dir.endswith(".h5"):
-        band = load_h5(band_dir)
-        raw = h5py.File(band_dir, "r").keys()
-        if "/WannBandInfo/NumberOfBand" in raw:
-            (
-                structure,
-                kpoints,
-                eigenvals,
-                efermi,
-                labels_dict,
-                projections,
-            ) = get_band_data_h5(band, iwan=True)
-        elif "/BandInfo/NumberOfBand" in raw:
-            (
-                structure,
-                kpoints,
-                eigenvals,
-                efermi,
-                labels_dict,
-                projections,
-            ) = get_band_data_h5(band, iwan=False)
-        else:
-            print("BandInfo or WannBandInfo key not found in h5file!")
-            return
-    elif band_dir.endswith(".json"):
-        with open(band_dir, "r") as fin:
-            band = json.load(fin)
-        if "WannBandInfo" in band.keys():
-            (
-                structure,
-                kpoints,
-                eigenvals,
-                efermi,
-                labels_dict,
-                projections,
-            ) = get_band_data_json(band, iwan=True)
-        elif "BandInfo" in band.keys():
-            (
-                structure,
-                kpoints,
-                eigenvals,
-                efermi,
-                labels_dict,
-                projections,
-            ) = get_band_data_json(band, iwan=False)
-        else:
-            print("BandInfo or WannBandInfo key not found in json file!")
-            return
-    else:
-        print("file - " + band_dir + " :  Unsupported format!")
-        return
-
-    if efermi:  # 从h5直接读取的费米能级可能是错的，此时需要用户自行指定
-        efermi = efermi  # 这只是个临时解决方案
-
-    lattice_new = Lattice(structure.lattice.reciprocal_lattice.matrix)
-    return BandStructureSymmLine(
-        kpoints=kpoints,
-        eigenvals=eigenvals,
-        lattice=lattice_new,
-        efermi=efermi,
-        labels_dict=labels_dict,
-        structure=structure,
-        projections=projections,
-    )
-
-
-def get_phonon_band_data_h5(band: Dict):
-    number_of_band = band["/BandInfo/NumberOfBand"][0]
-    number_of_kpoints = band["/BandInfo/NumberOfQPoints"][0]
-    number_of_spin = 1
-    symmmetry_kpoints = band["/BandInfo/SymmetryQPoints"]
-    symmetry_kPoints_index = band["/BandInfo/SymmetryQPointsIndex"]
-    eigenvals = {}
-    for i in range(number_of_spin):
-        spin_key = "Spin" + str(i + 1)
-        spin = Spin.up if i == 0 else Spin.down
-        if "/BandInfo/" + spin_key + "/BandEnergies" in band:
-            data = band["/BandInfo/" + spin_key + "/BandEnergies"]
-        elif "/BandInfo/" + spin_key + "/Band" in band:
-            data = band["/BandInfo/" + spin_key + "/Band"]
-        else:
-            print("Band key error")
-            return
-        frequencies = np.array(data).reshape((number_of_kpoints, number_of_band)).T
-        eigenvals[spin] = frequencies
-    kpoints = np.asarray(band["/BandInfo/CoordinatesOfQPoints"]).reshape(
-        number_of_kpoints, 3
-    )
-    if "/SupercellAtomInfo/CoordinateType" in band.keys():
-        structure = get_structure(band, "/SupercellAtomInfo")
-    else:
-        structure = get_structure(band, "/AtomInfo")
-    return symmmetry_kpoints, symmetry_kPoints_index, kpoints, structure, frequencies
-
-
-def get_phonon_band_data_json(band: Dict):
-    number_of_band = band["BandInfo"]["NumberOfBand"]
-    number_of_kpoints = band["BandInfo"]["NumberOfQPoints"]
-    number_of_spin = 1
-    symmmetry_kpoints = band["BandInfo"]["SymmetryQPoints"]
-    symmetry_kPoints_index = band["BandInfo"]["SymmetryQPointsIndex"]
-
-    eigenvals = {}
-    for i in range(number_of_spin):
-        spin_key = "Spin" + str(i + 1)
-        spin = Spin.up if i == 0 else Spin.down
-        if "BandEnergies" in band["BandInfo"][spin_key]:
-            data = band["BandInfo"][spin_key]["BandEnergies"]
-        elif "Band" in band["BandInfo"][spin_key]:
-            data = band["BandInfo"][spin_key]["Band"]
-        else:
-            print("Band key error")
-            return
-        frequencies = np.array(data).reshape((number_of_kpoints, number_of_band)).T
-        eigenvals[spin] = frequencies
-
-    kpoints = np.asarray(band["BandInfo"]["CoordinatesOfQPoints"]).reshape(
-        number_of_kpoints, 3
-    )
-
-    if "SupercellAtomInfo" in band.keys():
-        structure = get_structure_json(band["SupercellAtomInfo"])
-    else:
-        structure = get_structure_json(band["AtomInfo"])
-
-    return symmmetry_kpoints, symmetry_kPoints_index, kpoints, structure, frequencies
-
-
-def get_phonon_band_data(phonon_band_dir: str) -> PhononBandStructureSymmLine:
-    if phonon_band_dir.endswith(".h5"):
-        band = load_h5(phonon_band_dir)
-        (
-            symmmetry_kpoints,
-            symmetry_kPoints_index,
-            kpoints,
-            structure,
-            frequencies,
-        ) = get_phonon_band_data_h5(band)
-    elif phonon_band_dir.endswith(".json"):
-        with open(phonon_band_dir, "r") as fin:
-            band = json.load(fin)
-        (
-            symmmetry_kpoints,
-            symmetry_kPoints_index,
-            kpoints,
-            structure,
-            frequencies,
-        ) = get_phonon_band_data_json(band)
-    else:
-        print("file - " + phonon_band_dir + " :  Unsupported format!")
-        return
-
-    labels_dict = {}
-    for i, s in enumerate(symmmetry_kpoints):
-        labels_dict[s] = kpoints[symmetry_kPoints_index[i] - 1]
-    lattice_new = Lattice(structure.lattice.reciprocal_lattice.matrix)
-
-    return PhononBandStructureSymmLine(
-        qpoints=kpoints,
-        frequencies=frequencies,
-        lattice=lattice_new,
-        has_nac=False,
-        labels_dict=labels_dict,
-        structure=structure,
-    )
-
-
-def get_phonon_dos_data(phonon_dos_dir: str) -> PhononDos:
-    if phonon_dos_dir.endswith(".h5"):
-        dos = load_h5(phonon_dos_dir)
-        frequencies = np.asarray(dos["/DosInfo/DosEnergy"])
-        densities = dos["/DosInfo/Spin1/Dos"]
-    elif phonon_dos_dir.endswith(".json"):
-        with open(phonon_dos_dir, "r") as fin:
-            dos = json.load(fin)
-        frequencies = np.asarray(dos["DosInfo"]["DosEnergy"])
-        densities = dos["DosInfo"]["Spin1"]["Dos"]
-    else:
-        print("file - " + phonon_dos_dir + " :  Unsupported format!")
-        return
-
-    return PhononDos(frequencies, densities)
+# -*- coding: utf-8 -*-
+
+import json
+import h5py
+from typing import List, Dict
+
+import numpy as np
+from pymatgen.core.structure import Structure
+from pymatgen.core.lattice import Lattice
+from pymatgen.electronic_structure.dos import Dos, CompleteDos
+from pymatgen.electronic_structure.core import Spin
+from pymatgen.electronic_structure.core import Orbital
+from pymatgen.electronic_structure.bandstructure import BandStructureSymmLine
+from pymatgen.phonon.bandstructure import PhononBandStructureSymmLine
+from pymatgen.phonon.dos import PhononDos
+
+
+def json2structures(jsonfile):
+    """relax.json/aimd.json -> [pymatgen.Structure]
+
+    Parameters
+    ----------
+    jsonfile : str
+        包含多个离子步的 json 文件路径，例如 "relax.json" 或 "aimd.json"
+
+    Returns
+    -------
+    pymatgen_structures : list
+        pymatgen.Structure 列表
+
+    Examples
+    --------
+    >>> from dspawpy.io.read import json2structures
+    >>> structures = json2structures("relax.json")
+    """
+    with open(jsonfile, "r") as file:
+        j = json.load(file)
+
+    pymatgen_structures = []
+    for step in range(len(j)):
+        atominfo = j[step]["Atoms"]
+        elements = []
+        positions = []
+        for atomindex in range(len(atominfo)):
+            elements.append(atominfo[atomindex]["Element"])
+            positions.append(atominfo[atomindex]["Position"])
+        coords = np.asarray(positions).reshape(-1, 3)
+        lattice = np.asarray(j[step]["Lattice"]).reshape(3, 3)
+        pymatgen_structures.append(
+            Structure(lattice, elements, coords, coords_are_cartesian=True)
+        )
+
+    return pymatgen_structures
+
+
+def load_h5(dir_h5: str) -> dict:
+    """遍历读取h5文件中的数据，保存为字典格式
+
+    慎用此函数，因为会读取很多不需要的数据，耗时很长。
+
+    Parameters
+    ----------
+    dir_h5 : str
+        h5文件路径
+
+    Returns
+    -------
+    datas: dict
+        数据字典
+
+    Examples
+    --------
+    >>> from dspawpy.io.read import load_h5
+    >>> datas = load_h5(dir_h5)
+    """
+
+    def get_names(key, h5_object):
+        names.append(h5_object.name)
+
+    def is_dataset(name):
+        for name_inTheList in names:
+            if name_inTheList.find(name + "/") != -1:
+                return False
+        return True
+
+    def get_datas(key, h5_object):
+        if is_dataset(h5_object.name):
+            data = np.asarray(h5_object)
+            if data.dtype == "|S1":  # 转成字符串 并根据";"分割
+                byte2str = [str(bi, "utf-8") for bi in data]
+                string = ""
+                for char in byte2str:
+                    string += char
+                data = np.array([elem for elem in string.strip().split(";")])
+            # "/group1/group2/.../groupN/dataset" : value
+            datas[h5_object.name] = data.tolist()
+
+    with h5py.File(dir_h5, "r") as fin:
+        names = []
+        datas = {}
+        fin.visititems(get_names)
+        fin.visititems(get_datas)
+
+        return datas
+
+
+def load_h5_todict(dir_h5: str) -> Dict:
+    """与上一个函数区别在于合并了部分同类数据，例如
+
+    /Structures/Step-1/* 和 /Structures/Step-2/* 并入 /Structures/ 组内
+    """
+
+    def create_dict(L: List, D: Dict):
+        if len(L) == 2:
+            D[L[0]] = L[1]
+            return
+        else:
+            if not (L[0] in D.keys()):
+                D[L[0]] = {}
+            create_dict(L[1:], D[L[0]])
+
+    datas = load_h5(dir_h5)
+
+    groups_value_list = []
+    for key in datas.keys():
+        tmp_list = key[1:].strip().split("/")  # [1:] 截去root
+        tmp_list.append(datas[key])
+        # groups_value_list[i]结构: [group1, group2, ..., groupN, dataset, value]
+        groups_value_list.append(tmp_list)
+
+    groups_value_dict = {}
+    for data in groups_value_list:
+        create_dict(data, groups_value_dict)
+
+    return groups_value_dict
+
+
+def get_dos_data(dos_dir: str):
+    if dos_dir.endswith(".h5"):
+        dos = load_h5(dos_dir)
+        if dos["/DosInfo/Project"][0]:
+            return get_complete_dos(dos)
+        else:
+            return get_total_dos(dos)
+
+    elif dos_dir.endswith(".json"):
+        with open(dos_dir, "r") as fin:
+            dos = json.load(fin)
+
+        if dos["DosInfo"]["Project"]:
+            return get_complete_dos_json(dos)
+        else:
+            return get_total_dos_json(dos)
+
+    else:
+        print("file - " + dos_dir + " :  Unsupported format!")
+        return
+
+
+def get_total_dos(dos: Dict) -> Dos:
+    # h5 -> Dos Obj
+    energies = np.asarray(dos["/DosInfo/DosEnergy"])
+    if dos["/DosInfo/SpinType"][0] == "none":
+        densities = {Spin.up: np.asarray(dos["/DosInfo/Spin1/Dos"])}
+    else:
+        densities = {
+            Spin.up: np.asarray(dos["/DosInfo/Spin1/Dos"]),
+            Spin.down: np.asarray(dos["/DosInfo/Spin2/Dos"]),
+        }
+
+    efermi = dos["/DosInfo/EFermi"][0]
+
+    return Dos(efermi, energies, densities)
+
+
+def get_complete_dos(dos: Dict) -> CompleteDos:
+    # h5 -> CompleteDos Obj
+    total_dos = get_total_dos(dos)
+    structure = get_structure(dos, "/AtomInfo")
+    N = len(structure)
+    pdos = [{} for i in range(N)]
+    number_of_spin = 1 if dos["/DosInfo/SpinType"][0] == "none" else 2
+
+    for i in range(number_of_spin):
+        spin_key = "Spin" + str(i + 1)
+        spin = Spin.up if i == 0 else Spin.down
+        atomindexs = dos["/DosInfo/" + spin_key + "/ProjectDos/AtomIndexs"][0]
+        orbitindexs = dos["/DosInfo/" + spin_key + "/ProjectDos/OrbitIndexs"][0]
+        for atom_index in range(atomindexs):
+            for orbit_index in range(orbitindexs):
+                orbit_name = Orbital(orbit_index)
+                Contribution = dos[
+                    "/DosInfo/"
+                    + spin_key
+                    + "/ProjectDos"
+                    + str(atom_index + 1)
+                    + "/"
+                    + str(orbit_index + 1)
+                ]
+                if orbit_name in pdos[atom_index].keys():
+                    pdos[atom_index][orbit_name].update({spin: Contribution})
+                else:
+                    pdos[atom_index][orbit_name] = {spin: Contribution}
+
+    pdoss = {structure[i]: pd for i, pd in enumerate(pdos)}
+
+    return CompleteDos(structure, total_dos, pdoss)
+
+
+def get_total_dos_json(dos: Dict) -> Dos:
+    # json -> Dos Obj
+    energies = np.asarray(dos["DosInfo"]["DosEnergy"])
+    if dos["DosInfo"]["SpinType"] == "none":
+        densities = {Spin.up: np.asarray(dos["DosInfo"]["Spin1"]["Dos"])}
+    else:
+        densities = {
+            Spin.up: np.asarray(dos["DosInfo"]["Spin1"]["Dos"]),
+            Spin.down: np.asarray(dos["DosInfo"]["Spin2"]["Dos"]),
+        }
+    efermi = dos["DosInfo"]["EFermi"]
+    return Dos(efermi, energies, densities)
+
+
+def get_complete_dos_json(dos: Dict) -> CompleteDos:
+    # json -> CompleteDos Obj
+    total_dos = get_total_dos_json(dos)
+    structure = get_structure_json(dos["AtomInfo"])
+    N = len(structure)
+    pdos = [{} for i in range(N)]
+    number_of_spin = 1 if dos["DosInfo"]["SpinType"] == "none" else 2
+
+    for i in range(number_of_spin):
+        spin_key = "Spin" + str(i + 1)
+        spin = Spin.up if i == 0 else Spin.down
+        project = dos["DosInfo"][spin_key]["ProjectDos"]
+        for p in project:
+            atom_index = p["AtomIndex"] - 1
+            o = p["OrbitIndex"] - 1
+            orbit_name = Orbital(o)
+            if orbit_name in pdos[atom_index].keys():
+                pdos[atom_index][orbit_name].update({spin: p["Contribution"]})
+            else:
+                pdos[atom_index][orbit_name] = {spin: p["Contribution"]}
+    pdoss = {structure[i]: pd for i, pd in enumerate(pdos)}
+
+    return CompleteDos(structure, total_dos, pdoss)
+
+
+def get_structure(hdf5: Dict, key: str) -> Structure:
+    # load_h5 -> Structure Obj
+    lattice = np.asarray(hdf5[key + "/Lattice"]).reshape(3, 3)
+    elements = hdf5[key + "/Elements"]
+    positions = hdf5[key + "/Position"]
+    coords = np.asarray(positions).reshape(-1, 3)
+    is_direct = hdf5[key + "/CoordinateType"][0] == "Direct"
+    return Structure(lattice, elements, coords, coords_are_cartesian=(not is_direct))
+
+
+def get_structure_json(atominfo) -> Structure:
+    lattice = np.asarray(atominfo["Lattice"]).reshape(3, 3)
+    elements = []
+    positions = []
+    for atom in atominfo["Atoms"]:
+        elements.append(atom["Element"])
+        positions.extend(atom["Position"])
+
+    coords = np.asarray(positions).reshape(-1, 3)
+    is_direct = atominfo["CoordinateType"] == "Direct"
+    return Structure(lattice, elements, coords, coords_are_cartesian=(not is_direct))
+
+
+def get_structure_from_json(jsonfile: str) -> Structure:
+    with open(jsonfile, "r") as file:
+        j = json.load(file)
+    lattice = np.asarray(j["AtomInfo"]["Lattice"]).reshape(3, 3)
+    elements = j["AtomInfo"]["Elements"]
+    positions = j["AtomInfo"]["Position"]
+    coords = np.asarray(positions).reshape(-1, 3)
+    is_direct = j["AtomInfo"]["CoordinateType"][0] == "Direct"
+    return Structure(lattice, elements, coords, coords_are_cartesian=(not is_direct))
+
+
+def get_band_data_h5(band: Dict, iwan=False):
+    if iwan:
+        bd = "WannBandInfo"
+    else:
+        bd = "BandInfo"
+    number_of_band = band[f"/{bd}/NumberOfBand"][0]
+    number_of_kpoints = band[f"/{bd}/NumberOfKpoints"][0]
+    if (
+        band[f"/{bd}/SpinType"][0] == "none"
+        or band[f"/{bd}/SpinType"][0] == "non-collinear"
+    ):
+        number_of_spin = 1
+    else:
+        number_of_spin = 2
+
+    symmetry_kPoints_index = band[f"/{bd}/SymmetryKPointsIndex"]
+
+    efermi = band[f"/{bd}/EFermi"][0]
+    eigenvals = {}
+    for i in range(number_of_spin):
+        spin_key = "Spin" + str(i + 1)
+        spin = Spin.up if i == 0 else Spin.down
+
+        if f"/{bd}/" + spin_key + "/BandEnergies" in band:
+            data = band[f"/{bd}/" + spin_key + "/BandEnergies"]
+        elif f"/{bd}/" + spin_key + "/Band" in band:
+            data = band[f"/{bd}/" + spin_key + "/Band"]
+        else:
+            print("Band key error")
+            return
+        band_data = np.array(data).reshape((number_of_kpoints, number_of_band)).T
+        eigenvals[spin] = band_data
+
+    kpoints = np.asarray(band[f"/{bd}/CoordinatesOfKPoints"]).reshape(
+        number_of_kpoints, 3
+    )
+
+    structure = get_structure(band, "/AtomInfo")
+    labels_dict = {}
+
+    for i, s in enumerate(band[f"/{bd}/SymmetryKPoints"]):
+        labels_dict[s] = kpoints[symmetry_kPoints_index[i] - 1]
+
+    # read projection data
+    projections = None
+    if f"/{bd}/IsProject" in band.keys():
+        if band[f"/{bd}/IsProject"][0]:
+            projections = {}
+            number_of_orbit = len(band[f"/{bd}/Orbit"])
+            projection = np.zeros(
+                (number_of_band, number_of_kpoints, number_of_orbit, len(structure))
+            )
+
+            for i in range(number_of_spin):
+                spin_key = "Spin" + str(i + 1)
+                spin = Spin.up if i == 0 else Spin.down
+
+                atomindexs = band[f"/{bd}/" + spin_key + "/ProjectBand/AtomIndex"][0]
+                orbitindexs = band[f"/{bd}/" + spin_key + "/ProjectBand/OrbitIndexs"][0]
+                for atom_index in range(atomindexs):
+                    for orbit_index in range(orbitindexs):
+                        project_data = band[
+                            f"/{bd}/"
+                            + spin_key
+                            + "/ProjectBand/1/"
+                            + str(atom_index + 1)
+                            + "/"
+                            + str(orbit_index + 1)
+                        ]
+                        projection[:, :, orbit_index, atom_index] = (
+                            np.asarray(project_data)
+                            .reshape((number_of_kpoints, number_of_band))
+                            .T
+                        )
+                projections[spin] = projection
+
+    return structure, kpoints, eigenvals, efermi, labels_dict, projections
+
+
+def get_band_data_json(band: Dict, iwan=False):
+    if iwan:
+        bd = "WannBandInfo"
+    else:
+        bd = "BandInfo"
+
+    number_of_band = band[f"{bd}"]["NumberOfBand"]
+    number_of_kpoints = band[f"{bd}"]["NumberOfKpoints"]
+    if 'Spin2' in band[f"{bd}"]:
+        number_of_spin = 2
+    else:
+        number_of_spin = 1
+
+    symmetry_kPoints_index = band[f"{bd}"]["SymmetryKPointsIndex"]
+
+    if "EFermi" in band[f"{bd}"]:
+        efermi = band[f"{bd}"]["EFermi"]
+    else:
+        efermi = 0 # for wannier
+        
+    eigenvals = {}
+    for i in range(number_of_spin):
+        spin_key = "Spin" + str(i + 1)
+        spin = Spin.up if i == 0 else Spin.down
+
+        if "BandEnergies" in band[f"{bd}"][spin_key]:
+            data = band[f"{bd}"][spin_key]["BandEnergies"]
+        elif "Band" in band[f"{bd}"][spin_key]:
+            data = band[f"{bd}"][spin_key]["Band"]
+        else:
+            print("Band key error")
+            return
+
+        band_data = np.array(data).reshape((number_of_kpoints, number_of_band)).T
+
+        eigenvals[spin] = band_data
+
+    kpoints = np.asarray(band[f"{bd}"]["CoordinatesOfKPoints"]).reshape(
+        number_of_kpoints, 3
+    )
+
+    structure = get_structure_json(band["AtomInfo"])
+    labels_dict = {}
+
+    for i, s in enumerate(band[f"{bd}"]["SymmetryKPoints"]):
+        labels_dict[s] = kpoints[symmetry_kPoints_index[i] - 1]
+
+    # read projection data
+    projections = None
+    if "IsProject" in band[f"{bd}"].keys():
+        if band[f"{bd}"]["IsProject"]:
+            projections = {}
+            number_of_orbit = len(band[f"{bd}"]["Orbit"])
+            projection = np.zeros(
+                (number_of_band, number_of_kpoints, number_of_orbit, len(structure))
+            )
+
+            for i in range(number_of_spin):
+                spin_key = "Spin" + str(i + 1)
+                spin = Spin.up if i == 0 else Spin.down
+
+                data = band[f"{bd}"][spin_key]["ProjectBand"]
+                for d in data:
+                    orbit_index = d["OrbitIndex"] - 1
+                    atom_index = d["AtomIndex"] - 1
+                    project_data = d["Contribution"]
+                    projection[:, :, orbit_index, atom_index] = (
+                        np.asarray(project_data)
+                        .reshape((number_of_kpoints, number_of_band))
+                        .T
+                    )
+
+                projections[spin] = projection
+
+    return structure, kpoints, eigenvals, efermi, labels_dict, projections
+
+
+def get_band_data(band_dir: str, efermi: float = None) -> BandStructureSymmLine:
+    # modify BandStructureSymmLine.efermi after it was created will cause error
+    if band_dir.endswith(".h5"):
+        band = load_h5(band_dir)
+        raw = h5py.File(band_dir, "r").keys()
+        if "/WannBandInfo/NumberOfBand" in raw:
+            (
+                structure,
+                kpoints,
+                eigenvals,
+                efermi,
+                labels_dict,
+                projections,
+            ) = get_band_data_h5(band, iwan=True)
+        elif "/BandInfo/NumberOfBand" in raw:
+            (
+                structure,
+                kpoints,
+                eigenvals,
+                efermi,
+                labels_dict,
+                projections,
+            ) = get_band_data_h5(band, iwan=False)
+        else:
+            print("BandInfo or WannBandInfo key not found in h5file!")
+            return
+    elif band_dir.endswith(".json"):
+        with open(band_dir, "r") as fin:
+            band = json.load(fin)
+        if "WannBandInfo" in band.keys():
+            (
+                structure,
+                kpoints,
+                eigenvals,
+                efermi,
+                labels_dict,
+                projections,
+            ) = get_band_data_json(band, iwan=True)
+        elif "BandInfo" in band.keys():
+            (
+                structure,
+                kpoints,
+                eigenvals,
+                efermi,
+                labels_dict,
+                projections,
+            ) = get_band_data_json(band, iwan=False)
+        else:
+            print("BandInfo or WannBandInfo key not found in json file!")
+            return
+    else:
+        print("file - " + band_dir + " :  Unsupported format!")
+        return
+
+    if efermi:  # 从h5直接读取的费米能级可能是错的，此时需要用户自行指定
+        efermi = efermi  # 这只是个临时解决方案
+
+    lattice_new = Lattice(structure.lattice.reciprocal_lattice.matrix)
+    return BandStructureSymmLine(
+        kpoints=kpoints,
+        eigenvals=eigenvals,
+        lattice=lattice_new,
+        efermi=efermi,
+        labels_dict=labels_dict,
+        structure=structure,
+        projections=projections,
+    )
+
+
+def get_phonon_band_data_h5(band: Dict):
+    number_of_band = band["/BandInfo/NumberOfBand"][0]
+    number_of_kpoints = band["/BandInfo/NumberOfQPoints"][0]
+    number_of_spin = 1
+    symmmetry_kpoints = band["/BandInfo/SymmetryQPoints"]
+    symmetry_kPoints_index = band["/BandInfo/SymmetryQPointsIndex"]
+    eigenvals = {}
+    for i in range(number_of_spin):
+        spin_key = "Spin" + str(i + 1)
+        spin = Spin.up if i == 0 else Spin.down
+        if "/BandInfo/" + spin_key + "/BandEnergies" in band:
+            data = band["/BandInfo/" + spin_key + "/BandEnergies"]
+        elif "/BandInfo/" + spin_key + "/Band" in band:
+            data = band["/BandInfo/" + spin_key + "/Band"]
+        else:
+            print("Band key error")
+            return
+        frequencies = np.array(data).reshape((number_of_kpoints, number_of_band)).T
+        eigenvals[spin] = frequencies
+    kpoints = np.asarray(band["/BandInfo/CoordinatesOfQPoints"]).reshape(
+        number_of_kpoints, 3
+    )
+    if "/SupercellAtomInfo/CoordinateType" in band.keys():
+        structure = get_structure(band, "/SupercellAtomInfo")
+    else:
+        structure = get_structure(band, "/AtomInfo")
+    return symmmetry_kpoints, symmetry_kPoints_index, kpoints, structure, frequencies
+
+
+def get_phonon_band_data_json(band: Dict):
+    number_of_band = band["BandInfo"]["NumberOfBand"]
+    number_of_kpoints = band["BandInfo"]["NumberOfQPoints"]
+    number_of_spin = 1
+    symmmetry_kpoints = band["BandInfo"]["SymmetryQPoints"]
+    symmetry_kPoints_index = band["BandInfo"]["SymmetryQPointsIndex"]
+
+    eigenvals = {}
+    for i in range(number_of_spin):
+        spin_key = "Spin" + str(i + 1)
+        spin = Spin.up if i == 0 else Spin.down
+        if "BandEnergies" in band["BandInfo"][spin_key]:
+            data = band["BandInfo"][spin_key]["BandEnergies"]
+        elif "Band" in band["BandInfo"][spin_key]:
+            data = band["BandInfo"][spin_key]["Band"]
+        else:
+            print("Band key error")
+            return
+        frequencies = np.array(data).reshape((number_of_kpoints, number_of_band)).T
+        eigenvals[spin] = frequencies
+
+    kpoints = np.asarray(band["BandInfo"]["CoordinatesOfQPoints"]).reshape(
+        number_of_kpoints, 3
+    )
+
+    if "SupercellAtomInfo" in band.keys():
+        structure = get_structure_json(band["SupercellAtomInfo"])
+    else:
+        structure = get_structure_json(band["AtomInfo"])
+
+    return symmmetry_kpoints, symmetry_kPoints_index, kpoints, structure, frequencies
+
+
+def get_phonon_band_data(phonon_band_dir: str) -> PhononBandStructureSymmLine:
+    if phonon_band_dir.endswith(".h5"):
+        band = load_h5(phonon_band_dir)
+        (
+            symmmetry_kpoints,
+            symmetry_kPoints_index,
+            kpoints,
+            structure,
+            frequencies,
+        ) = get_phonon_band_data_h5(band)
+    elif phonon_band_dir.endswith(".json"):
+        with open(phonon_band_dir, "r") as fin:
+            band = json.load(fin)
+        (
+            symmmetry_kpoints,
+            symmetry_kPoints_index,
+            kpoints,
+            structure,
+            frequencies,
+        ) = get_phonon_band_data_json(band)
+    else:
+        print("file - " + phonon_band_dir + " :  Unsupported format!")
+        return
+
+    labels_dict = {}
+    for i, s in enumerate(symmmetry_kpoints):
+        labels_dict[s] = kpoints[symmetry_kPoints_index[i] - 1]
+    lattice_new = Lattice(structure.lattice.reciprocal_lattice.matrix)
+
+    return PhononBandStructureSymmLine(
+        qpoints=kpoints,
+        frequencies=frequencies,
+        lattice=lattice_new,
+        has_nac=False,
+        labels_dict=labels_dict,
+        structure=structure,
+    )
+
+
+def get_phonon_dos_data(phonon_dos_dir: str) -> PhononDos:
+    if phonon_dos_dir.endswith(".h5"):
+        dos = load_h5(phonon_dos_dir)
+        frequencies = np.asarray(dos["/DosInfo/DosEnergy"])
+        densities = dos["/DosInfo/Spin1/Dos"]
+    elif phonon_dos_dir.endswith(".json"):
+        with open(phonon_dos_dir, "r") as fin:
+            dos = json.load(fin)
+        frequencies = np.asarray(dos["DosInfo"]["DosEnergy"])
+        densities = dos["DosInfo"]["Spin1"]["Dos"]
+    else:
+        print("file - " + phonon_dos_dir + " :  Unsupported format!")
+        return
+
+    return PhononDos(frequencies, densities)
```

## dspawpy/io/read_json.py

 * *Ordering differences only*

```diff
@@ -1,233 +1,233 @@
-# -*- coding: utf-8 -*-
-
-import json
-from typing import Dict
-
-import numpy as np
-from pymatgen.core.structure import Structure
-from pymatgen.core.lattice import Lattice
-from pymatgen.electronic_structure.dos import Dos, CompleteDos
-from pymatgen.electronic_structure.core import Spin
-from pymatgen.electronic_structure.core import Orbital
-from pymatgen.electronic_structure.bandstructure import BandStructureSymmLine
-from pymatgen.phonon.bandstructure import PhononBandStructureSymmLine
-from pymatgen.phonon.dos import PhononDos, CompletePhononDos
-
-
-def get_dos_data(dos_json: str):
-    with open(dos_json, "r") as file:
-        dos = json.load(file)
-
-    if dos["DosInfo"]["Project"]:
-        return get_complete_dos(dos)
-    else:
-        return get_total_dos(dos)
-
-
-def get_total_dos(dos: Dict) -> Dos:
-    energies = np.asarray(dos["DosInfo"]["DosEnergy"])
-    if dos["DosInfo"]["SpinType"] == "none":
-        densities = {Spin.up: np.asarray(dos["DosInfo"]["Spin1"]["Dos"])}
-    else:
-        densities = {
-            Spin.up: np.asarray(dos["DosInfo"]["Spin1"]["Dos"]),
-            Spin.down: np.asarray(dos["DosInfo"]["Spin2"]["Dos"]),
-        }
-
-    efermi = dos["DosInfo"]["EFermi"]
-
-    return Dos(efermi, energies, densities)
-
-
-def get_complete_dos(dos: Dict) -> CompleteDos:
-    total_dos = get_total_dos(dos)
-
-    structure = get_structure(dos["AtomInfo"])
-    orbit = dos["DosInfo"]["Orbit"]
-    N = len(structure)
-
-    pdos = [{} for i in range(N)]
-    number_of_spin = 1 if dos["DosInfo"]["SpinType"] == "none" else 2
-
-    for i in range(number_of_spin):
-        spin_key = "Spin" + str(i + 1)
-        spin = Spin.up if i == 0 else Spin.down
-        project = dos["DosInfo"][spin_key]["ProjectDos"]
-        for p in project:
-            atom_index = p["AtomIndex"] - 1
-            o = p["OrbitIndex"] - 1
-            orbit_name = Orbital(o)
-
-            if orbit_name in pdos[atom_index].keys():
-                pdos[atom_index][orbit_name].update({spin: p["Contribution"]})
-            else:
-                pdos[atom_index][orbit_name] = {spin: p["Contribution"]}
-
-    pdoss = {structure[i]: pd for i, pd in enumerate(pdos)}
-
-    return CompleteDos(structure, total_dos, pdoss)
-
-
-def get_structure(atominfo) -> Structure:
-    lattice = np.asarray(atominfo["Lattice"]).reshape(3, 3)
-    elements = []
-    positions = []
-    for atom in atominfo["Atoms"]:
-        elements.append(atom["Element"])
-        positions.extend(atom["Position"])
-
-    coords = np.asarray(positions).reshape(-1, 3)
-    is_direct = atominfo["CoordinateType"] == "Direct"
-    return Structure(lattice, elements, coords, coords_are_cartesian=(not is_direct))
-
-
-def get_structure_from_json(jsonfile: str) -> Structure:
-    with open(jsonfile, "r") as file:
-        j = json.load(file)
-    return get_structure(j["AtomInfo"])
-
-
-def remove_extra_kpoint(band_data, symmetry_kPoints_index, number_of_band):
-    keep_data = []
-    for i in range(len(symmetry_kPoints_index) - 1):
-        if i == 0:
-            start_index = symmetry_kPoints_index[i] - 1
-            end_index = symmetry_kPoints_index[i + 1]
-        else:
-            start_index = symmetry_kPoints_index[i] + 1
-            end_index = symmetry_kPoints_index[i + 1]
-
-        tmp = band_data[start_index * number_of_band : end_index * number_of_band]
-        keep_data.extend(tmp)
-    return keep_data
-
-
-def get_band_data(band_json: str) -> BandStructureSymmLine:
-    with open(band_json, "r") as file:
-        band = json.load(file)
-
-    number_of_band = band["BandInfo"]["NumberOfBand"]
-    number_of_kpoints = band["BandInfo"]["NumberOfKpoints"]
-    if (
-        band["BandInfo"]["SpinType"] == "none"
-        or band["BandInfo"]["SpinType"] == "non-collinear"
-    ):
-        number_of_spin = 1
-    else:
-        number_of_spin = 2
-
-    symmmetry_kpoints = band["BandInfo"]["SymmetryKPoints"]
-    symmetry_kPoints_index = band["BandInfo"]["SymmetryKPointsIndex"]
-
-    efermi = band["BandInfo"]["EFermi"]
-    eigenvals = {}
-    for i in range(number_of_spin):
-        spin_key = "Spin" + str(i + 1)
-        spin = Spin.up if i == 0 else Spin.down
-
-        data = band["BandInfo"][spin_key]["Band"]
-        band_data = np.array(data).reshape((number_of_kpoints, number_of_band)).T
-
-        eigenvals[spin] = band_data
-
-    kpoints = np.asarray(band["BandInfo"]["CoordinatesOfKPoints"]).reshape(
-        number_of_kpoints, 3
-    )
-
-    structure = get_structure(band["AtomInfo"])
-    labels_dict = {}
-
-    for i, s in enumerate(band["BandInfo"]["SymmetryKPoints"]):
-        labels_dict[s] = kpoints[symmetry_kPoints_index[i] - 1]
-
-    # read projection data
-    projections = None
-    if "IsProject" in band["BandInfo"].keys():
-        if band["BandInfo"]["IsProject"]:
-            projections = {}
-            number_of_orbit = len(band["BandInfo"]["Orbit"])
-            projection = np.zeros(
-                (number_of_band, number_of_kpoints, number_of_orbit, len(structure))
-            )
-
-            for i in range(number_of_spin):
-                spin_key = "Spin" + str(i + 1)
-                spin = Spin.up if i == 0 else Spin.down
-
-                data = band["BandInfo"][spin_key]["ProjectBand"]
-                for d in data:
-                    orbit_index = d["OrbitIndex"] - 1
-                    atom_index = d["AtomIndex"] - 1
-                    project_data = d["Contribution"]
-                    projection[:, :, orbit_index, atom_index] = (
-                        np.asarray(project_data)
-                        .reshape((number_of_kpoints, number_of_band))
-                        .T
-                    )
-
-                projections[spin] = projection
-
-    lattice_new = Lattice(structure.lattice.reciprocal_lattice.matrix)
-    return BandStructureSymmLine(
-        kpoints=kpoints,
-        eigenvals=eigenvals,
-        lattice=lattice_new,
-        efermi=efermi,
-        labels_dict=labels_dict,
-        structure=structure,
-        projections=projections,
-    )
-
-
-def get_phonon_band_data(phonon_band_json: str) -> PhononBandStructureSymmLine:
-    with open(phonon_band_json, "r") as file:
-        band = json.load(file)
-
-    number_of_band = band["BandInfo"]["NumberOfBand"]
-    number_of_kpoints = band["BandInfo"]["NumberOfQPoints"]
-
-    number_of_spin = 1
-
-    symmmetry_kpoints = band["BandInfo"]["SymmetryQPoints"]
-    symmetry_kPoints_index = band["BandInfo"]["SymmetryQPointsIndex"]
-
-    eigenvals = {}
-    for i in range(number_of_spin):
-        spin_key = "Spin" + str(i + 1)
-        spin = Spin.up if i == 0 else Spin.down
-
-        data = band["BandInfo"][spin_key]["Band"]
-        frequencies = np.array(data).reshape((number_of_kpoints, number_of_band)).T
-        eigenvals[spin] = frequencies
-
-    kpoints = np.asarray(band["BandInfo"]["CoordinatesOfQPoints"]).reshape(
-        number_of_kpoints, 3
-    )
-
-    if "SupercellAtomInfo" in band.keys():
-        structure = get_structure(band["SupercellAtomInfo"])
-    else:
-        structure = get_structure(band["AtomInfo"])
-    labels_dict = {}
-
-    for i, s in enumerate(symmmetry_kpoints):
-        labels_dict[s] = kpoints[symmetry_kPoints_index[i] - 1]
-
-    lattice_new = Lattice(structure.lattice.reciprocal_lattice.matrix)
-    return PhononBandStructureSymmLine(
-        qpoints=kpoints,
-        frequencies=frequencies,
-        lattice=lattice_new,
-        has_nac=False,
-        labels_dict=labels_dict,
-        structure=structure,
-    )
-
-
-def get_phonon_dos_data(phonon_dos_json: str) -> PhononDos:
-    with open(phonon_dos_json, "r") as file:
-        dos = json.load(file)
-
-    frequencies = np.asarray(dos["DosInfo"]["DosEnergy"])
-    densities = dos["DosInfo"]["Spin1"]["Dos"]
-    return PhononDos(frequencies, densities)
+# -*- coding: utf-8 -*-
+
+import json
+from typing import Dict
+
+import numpy as np
+from pymatgen.core.structure import Structure
+from pymatgen.core.lattice import Lattice
+from pymatgen.electronic_structure.dos import Dos, CompleteDos
+from pymatgen.electronic_structure.core import Spin
+from pymatgen.electronic_structure.core import Orbital
+from pymatgen.electronic_structure.bandstructure import BandStructureSymmLine
+from pymatgen.phonon.bandstructure import PhononBandStructureSymmLine
+from pymatgen.phonon.dos import PhononDos, CompletePhononDos
+
+
+def get_dos_data(dos_json: str):
+    with open(dos_json, "r") as file:
+        dos = json.load(file)
+
+    if dos["DosInfo"]["Project"]:
+        return get_complete_dos(dos)
+    else:
+        return get_total_dos(dos)
+
+
+def get_total_dos(dos: Dict) -> Dos:
+    energies = np.asarray(dos["DosInfo"]["DosEnergy"])
+    if dos["DosInfo"]["SpinType"] == "none":
+        densities = {Spin.up: np.asarray(dos["DosInfo"]["Spin1"]["Dos"])}
+    else:
+        densities = {
+            Spin.up: np.asarray(dos["DosInfo"]["Spin1"]["Dos"]),
+            Spin.down: np.asarray(dos["DosInfo"]["Spin2"]["Dos"]),
+        }
+
+    efermi = dos["DosInfo"]["EFermi"]
+
+    return Dos(efermi, energies, densities)
+
+
+def get_complete_dos(dos: Dict) -> CompleteDos:
+    total_dos = get_total_dos(dos)
+
+    structure = get_structure(dos["AtomInfo"])
+    orbit = dos["DosInfo"]["Orbit"]
+    N = len(structure)
+
+    pdos = [{} for i in range(N)]
+    number_of_spin = 1 if dos["DosInfo"]["SpinType"] == "none" else 2
+
+    for i in range(number_of_spin):
+        spin_key = "Spin" + str(i + 1)
+        spin = Spin.up if i == 0 else Spin.down
+        project = dos["DosInfo"][spin_key]["ProjectDos"]
+        for p in project:
+            atom_index = p["AtomIndex"] - 1
+            o = p["OrbitIndex"] - 1
+            orbit_name = Orbital(o)
+
+            if orbit_name in pdos[atom_index].keys():
+                pdos[atom_index][orbit_name].update({spin: p["Contribution"]})
+            else:
+                pdos[atom_index][orbit_name] = {spin: p["Contribution"]}
+
+    pdoss = {structure[i]: pd for i, pd in enumerate(pdos)}
+
+    return CompleteDos(structure, total_dos, pdoss)
+
+
+def get_structure(atominfo) -> Structure:
+    lattice = np.asarray(atominfo["Lattice"]).reshape(3, 3)
+    elements = []
+    positions = []
+    for atom in atominfo["Atoms"]:
+        elements.append(atom["Element"])
+        positions.extend(atom["Position"])
+
+    coords = np.asarray(positions).reshape(-1, 3)
+    is_direct = atominfo["CoordinateType"] == "Direct"
+    return Structure(lattice, elements, coords, coords_are_cartesian=(not is_direct))
+
+
+def get_structure_from_json(jsonfile: str) -> Structure:
+    with open(jsonfile, "r") as file:
+        j = json.load(file)
+    return get_structure(j["AtomInfo"])
+
+
+def remove_extra_kpoint(band_data, symmetry_kPoints_index, number_of_band):
+    keep_data = []
+    for i in range(len(symmetry_kPoints_index) - 1):
+        if i == 0:
+            start_index = symmetry_kPoints_index[i] - 1
+            end_index = symmetry_kPoints_index[i + 1]
+        else:
+            start_index = symmetry_kPoints_index[i] + 1
+            end_index = symmetry_kPoints_index[i + 1]
+
+        tmp = band_data[start_index * number_of_band : end_index * number_of_band]
+        keep_data.extend(tmp)
+    return keep_data
+
+
+def get_band_data(band_json: str) -> BandStructureSymmLine:
+    with open(band_json, "r") as file:
+        band = json.load(file)
+
+    number_of_band = band["BandInfo"]["NumberOfBand"]
+    number_of_kpoints = band["BandInfo"]["NumberOfKpoints"]
+    if (
+        band["BandInfo"]["SpinType"] == "none"
+        or band["BandInfo"]["SpinType"] == "non-collinear"
+    ):
+        number_of_spin = 1
+    else:
+        number_of_spin = 2
+
+    symmmetry_kpoints = band["BandInfo"]["SymmetryKPoints"]
+    symmetry_kPoints_index = band["BandInfo"]["SymmetryKPointsIndex"]
+
+    efermi = band["BandInfo"]["EFermi"]
+    eigenvals = {}
+    for i in range(number_of_spin):
+        spin_key = "Spin" + str(i + 1)
+        spin = Spin.up if i == 0 else Spin.down
+
+        data = band["BandInfo"][spin_key]["Band"]
+        band_data = np.array(data).reshape((number_of_kpoints, number_of_band)).T
+
+        eigenvals[spin] = band_data
+
+    kpoints = np.asarray(band["BandInfo"]["CoordinatesOfKPoints"]).reshape(
+        number_of_kpoints, 3
+    )
+
+    structure = get_structure(band["AtomInfo"])
+    labels_dict = {}
+
+    for i, s in enumerate(band["BandInfo"]["SymmetryKPoints"]):
+        labels_dict[s] = kpoints[symmetry_kPoints_index[i] - 1]
+
+    # read projection data
+    projections = None
+    if "IsProject" in band["BandInfo"].keys():
+        if band["BandInfo"]["IsProject"]:
+            projections = {}
+            number_of_orbit = len(band["BandInfo"]["Orbit"])
+            projection = np.zeros(
+                (number_of_band, number_of_kpoints, number_of_orbit, len(structure))
+            )
+
+            for i in range(number_of_spin):
+                spin_key = "Spin" + str(i + 1)
+                spin = Spin.up if i == 0 else Spin.down
+
+                data = band["BandInfo"][spin_key]["ProjectBand"]
+                for d in data:
+                    orbit_index = d["OrbitIndex"] - 1
+                    atom_index = d["AtomIndex"] - 1
+                    project_data = d["Contribution"]
+                    projection[:, :, orbit_index, atom_index] = (
+                        np.asarray(project_data)
+                        .reshape((number_of_kpoints, number_of_band))
+                        .T
+                    )
+
+                projections[spin] = projection
+
+    lattice_new = Lattice(structure.lattice.reciprocal_lattice.matrix)
+    return BandStructureSymmLine(
+        kpoints=kpoints,
+        eigenvals=eigenvals,
+        lattice=lattice_new,
+        efermi=efermi,
+        labels_dict=labels_dict,
+        structure=structure,
+        projections=projections,
+    )
+
+
+def get_phonon_band_data(phonon_band_json: str) -> PhononBandStructureSymmLine:
+    with open(phonon_band_json, "r") as file:
+        band = json.load(file)
+
+    number_of_band = band["BandInfo"]["NumberOfBand"]
+    number_of_kpoints = band["BandInfo"]["NumberOfQPoints"]
+
+    number_of_spin = 1
+
+    symmmetry_kpoints = band["BandInfo"]["SymmetryQPoints"]
+    symmetry_kPoints_index = band["BandInfo"]["SymmetryQPointsIndex"]
+
+    eigenvals = {}
+    for i in range(number_of_spin):
+        spin_key = "Spin" + str(i + 1)
+        spin = Spin.up if i == 0 else Spin.down
+
+        data = band["BandInfo"][spin_key]["Band"]
+        frequencies = np.array(data).reshape((number_of_kpoints, number_of_band)).T
+        eigenvals[spin] = frequencies
+
+    kpoints = np.asarray(band["BandInfo"]["CoordinatesOfQPoints"]).reshape(
+        number_of_kpoints, 3
+    )
+
+    if "SupercellAtomInfo" in band.keys():
+        structure = get_structure(band["SupercellAtomInfo"])
+    else:
+        structure = get_structure(band["AtomInfo"])
+    labels_dict = {}
+
+    for i, s in enumerate(symmmetry_kpoints):
+        labels_dict[s] = kpoints[symmetry_kPoints_index[i] - 1]
+
+    lattice_new = Lattice(structure.lattice.reciprocal_lattice.matrix)
+    return PhononBandStructureSymmLine(
+        qpoints=kpoints,
+        frequencies=frequencies,
+        lattice=lattice_new,
+        has_nac=False,
+        labels_dict=labels_dict,
+        structure=structure,
+    )
+
+
+def get_phonon_dos_data(phonon_dos_json: str) -> PhononDos:
+    with open(phonon_dos_json, "r") as file:
+        dos = json.load(file)
+
+    frequencies = np.asarray(dos["DosInfo"]["DosEnergy"])
+    densities = dos["DosInfo"]["Spin1"]["Dos"]
+    return PhononDos(frequencies, densities)
```

## dspawpy/io/structure.py

 * *Ordering differences only*

```diff
@@ -1,301 +1,301 @@
-# -*- coding: utf-8 -*-
-
-import json
-from typing import Dict, List
-import h5py
-import numpy as np
-
-from pymatgen.core import Structure
-from dspawpy.io.utils import get_lines_without_comment
-from dspawpy.analysis.aimdtools import read_h5
-
-
-def from_dspaw_as(as_file: str = "structure.as") -> Structure:
-    """从DSPAW的as结构文件中读取结构信息
-
-    Parameters
-    ----------
-    as_file : str
-        DSPAW的as结构文件, 默认'structure.as'
-
-    Returns
-    -------
-    Structure
-        pymatgen的Structure对象
-
-    Examples
-    --------
-    >>> from dspawpy.io.structure import from_dspaw_as
-    >>> S1 = from_dspaw_as(as_file='structure00.as')
-    """
-    D = {}
-    lines = get_lines_without_comment(as_file, "#")
-    N = int(lines[1])
-    lattice = []
-    for line in lines[3:6]:
-        vector = line.split()
-        lattice.extend([float(vector[0]), float(vector[1]), float(vector[2])])
-
-    lattice = np.asarray(lattice).reshape(3, 3)
-    is_direct = lines[6].strip().split()[0].startswith("Direct")
-    elements = []
-    positions = []
-    others = []
-    line6s = []
-    for i in range(N):
-        atom = lines[i + 7].strip().split()
-        elements.append(atom[0])
-        positions.extend([float(atom[1]), float(atom[2]), float(atom[3])])
-
-        if len(atom) > 4:
-            other = atom[4:]
-            others.append(other)
-            line6 = lines[6]
-            line6s.append(line6)
-
-    D.setdefault("others", others)
-    D.setdefault("line6s", line6s)
-
-    coords = np.asarray(positions).reshape(-1, 3)
-    if others == [] and line6s == []:
-        return Structure(
-            lattice, elements, coords, coords_are_cartesian=(not is_direct)
-        )
-    else:
-        return Structure(
-            lattice,
-            elements,
-            coords,
-            coords_are_cartesian=(not is_direct),
-            site_properties=D,
-        )
-
-
-def from_hzw(hzw_file) -> Structure:
-    """从hzw结构文件中读取结构信息
-
-    Parameters
-    ----------
-    hzw_file : str
-        hzw结构文件，以 .hzw 结尾
-
-    Returns
-    -------
-    Structure
-        pymatgen的Structure对象
-
-    Examples
-    --------
-    >>> from dspawpy.io.structure import from_hzw
-    >>> S1 = from_hzw(hzw_file='Si.hzw')
-    """
-    lines = get_lines_without_comment(hzw_file, "%")
-    number_of_probes = int(lines[0])
-    if number_of_probes != 0:
-        raise ValueError("dspaw only support 0 probes hzw file")
-    lattice = []
-    for line in lines[1:4]:
-        vector = line.split()
-        lattice.extend([float(vector[0]), float(vector[1]), float(vector[2])])
-
-    lattice = np.asarray(lattice).reshape(3, 3)
-    N = int(lines[4])
-    elements = []
-    positions = []
-    for i in range(N):
-        atom = lines[i + 5].strip().split()
-        elements.append(atom[0])
-        positions.extend([float(atom[1]), float(atom[2]), float(atom[3])])
-
-    coords = np.asarray(positions).reshape(-1, 3)
-    return Structure(lattice, elements, coords, coords_are_cartesian=True)
-
-
-def to_file(structure: Structure, filename: str, fmt, coords_are_cartesian=True):
-    """往结构文件中写入信息
-
-    Parameters
-    ----------
-    structure : Structure
-        pymatgen的Structure对象
-    filename : str
-        结构文件名
-    fmt : str
-        结构文件类型，支持 "json","as","hzw"
-    coords_are_cartesian : bool
-        坐标是否为笛卡尔坐标，默认为True
-
-    Examples
-    --------
-    >>> from dspawpy.io.structure import to_file
-    >>> to_file(structure, filename='Si.json', fmt='json')
-    >>> to_file(structure, filename='Si.as', fmt='as')
-    >>> to_file(structure, filename='Si.hzw', fmt='hzw')
-    """
-    if fmt == "json":
-        to_dspaw_json(structure, filename, coords_are_cartesian)
-    elif fmt == "as":
-        to_dspaw_as(structure, filename, coords_are_cartesian)
-    elif fmt == "hzw":
-        to_hzw(structure, filename)
-
-
-def from_dspaw_atominfo(hpath, index: int = None) -> Structure:
-    if not isinstance(hpath, str):  # for compatibility with aimd2pdb.py
-        atominfo = hpath
-        lattice = np.asarray(atominfo["Lattice"]).reshape(3, 3)
-        elements = []
-        positions = []
-        for atom in atominfo["Atoms"]:
-            elements.append(atom["Element"])
-            positions.extend(atom["Position"])
-
-        coords = np.asarray(positions).reshape(-1, 3)
-        is_direct = atominfo["CoordinateType"] == "Direct"
-        return Structure(
-            lattice, elements, coords, coords_are_cartesian=(not is_direct)
-        )
-    Nstep, elements, positions, lattices = read_h5(hpath, index)
-    return Structure(lattices[0], elements, positions[0], coords_are_cartesian=True)
-
-
-def from_dspaw_atominfo_json(atominfo: dict) -> Structure:
-    lattice = np.asarray(atominfo["Lattice"]).reshape(3, 3)
-    elements = []
-    positions = []
-    for atom in atominfo["Atoms"]:
-        elements.append(atom["Element"])
-        positions.extend(atom["Position"])
-
-    coords = np.asarray(positions).reshape(-1, 3)
-    is_direct = atominfo["CoordinateType"] == "Direct"
-    return Structure(lattice, elements, coords, coords_are_cartesian=(not is_direct))
-
-
-def from_dspaw_atominfos(aimd_dir: str) -> List[Structure]:
-    structures = []
-    if isinstance(aimd_dir, list) or isinstance(aimd_dir, np.ndarray):
-        for atominfo in aimd_dir:
-            structures.append(from_dspaw_atominfo(atominfo))
-    elif aimd_dir.endswith(".h5"):
-        aimd = h5py.File(aimd_dir)
-        steps = int(np.array(aimd.get("/Structures/FinalStep"))[0])
-        for i in range(steps):
-            structures.append(from_dspaw_atominfo(aimd_dir, i + 1))
-    elif aimd_dir.endswith(".json"):
-        with open(aimd_dir, "r") as fin:
-            aimd = json.load(fin)
-        for atominfo in aimd["Structures"]:
-            structures.append(from_dspaw_atominfo_json(atominfo))
-    else:
-        print("file - " + aimd_dir + " :  Unsupported format!")
-        return
-
-    return structures
-
-
-def to_dspaw_as(structure: Structure, filename: str, coords_are_cartesian=True):
-    with open(filename, "w", encoding="utf-8") as file:
-        file.write("Total number of atoms\n")
-        file.write("%d\n" % len(structure))
-
-        file.write("Lattice\n")
-        for v in structure.lattice.matrix:
-            file.write("%.6f %.6f %.6f\n" % (v[0], v[1], v[2]))
-
-        i = 0
-        for site in structure:
-            if i == 0:
-                if 'line6s' in site.properties:
-                    file.write("%s\n" % site.properties["line6s"])
-                else:
-                    if coords_are_cartesian:
-                        file.write("Cartesian\n")
-                    else:
-                        file.write("Direct\n")
-            i += 1
-
-            coords = site.coords if coords_are_cartesian else site.frac_coords
-            if 'others' in site.properties:
-                sp = " ".join(site.properties["others"])  # flatten str list
-                file.write(
-                    "%s %.6f %.6f %.6f %s\n"
-                    % (site.species_string, coords[0], coords[1], coords[2], sp)
-                )
-            else:
-                file.write(
-                    "%s %.6f %.6f %.6f\n"
-                    % (site.species_string, coords[0], coords[1], coords[2])
-                )
-
-
-def to_hzw(structure: Structure, filename: str):
-    with open(filename, "w", encoding="utf-8") as file:
-        file.write("% The number of probes \n")
-        file.write("0\n")
-        file.write("% Uni-cell vector\n")
-
-        for v in structure.lattice.matrix:
-            file.write("%.6f %.6f %.6f\n" % (v[0], v[1], v[2]))
-
-        file.write("% Total number of device_structure\n")
-        file.write("%d\n" % len(structure))
-        file.write("% Atom site\n")
-
-        for site in structure:
-            file.write(
-                "%s %.6f %.6f %.6f\n"
-                % (site.species_string, site.coords[0], site.coords[1], site.coords[2])
-            )
-
-
-def to_dspaw_dict(structure: Structure, coords_are_cartesian=True) -> Dict:
-    lattice = structure.lattice.matrix.flatten().tolist()
-    atoms = []
-    for site in structure:
-        coords = site.coords if coords_are_cartesian else site.frac_coords
-        atoms.append({"Element": site.species_string, "Position": coords.tolist()})
-
-    coordinate_type = "Cartesian" if coords_are_cartesian else "Direct"
-    return {"Lattice": lattice, "CoordinateType": coordinate_type, "Atoms": atoms}
-
-
-def to_dspaw_json(structure: Structure, filename: str, coords_are_cartesian=True):
-    d = to_dspaw_dict(structure, coords_are_cartesian)
-    with open(filename, "w", encoding="utf-8") as file:
-        json.dump(d, file, indent=4)
-
-
-def to_pdb(structures: List[Structure], pdb_filename: str):
-    with open(pdb_filename, "w", encoding="utf-8") as file:
-        for i, s in enumerate(structures):
-            file.write("MODEL         %d\n" % (i + 1))
-            file.write("REMARK   Converted from Structures\n")
-            file.write("REMARK   Converted using dspawpy\n")
-            lengths = s.lattice.lengths
-            angles = s.lattice.angles
-            file.write(
-                "CRYST1{0:9.3f}{1:9.3f}{2:9.3f}{3:7.2f}{4:7.2f}{5:7.2f}\n".format(
-                    lengths[0], lengths[1], lengths[2], angles[0], angles[1], angles[2]
-                )
-            )
-            for j, site in enumerate(s):
-                file.write(
-                    "%4s%7d%4s%5s%6d%4s%8.3f%8.3f%8.3f%6.2f%6.2f%12s\n"
-                    % (
-                        "ATOM",
-                        j + 1,
-                        site.species_string,
-                        "MOL",
-                        1,
-                        "    ",
-                        site.coords[0],
-                        site.coords[1],
-                        site.coords[2],
-                        1.0,
-                        0.0,
-                        site.species_string,
-                    )
-                )
-            file.write("TER\n")
-            file.write("ENDMDL\n")
+# -*- coding: utf-8 -*-
+
+import json
+from typing import Dict, List
+import h5py
+import numpy as np
+
+from pymatgen.core import Structure
+from dspawpy.io.utils import get_lines_without_comment
+from dspawpy.analysis.aimdtools import read_h5
+
+
+def from_dspaw_as(as_file: str = "structure.as") -> Structure:
+    """从DSPAW的as结构文件中读取结构信息
+
+    Parameters
+    ----------
+    as_file : str
+        DSPAW的as结构文件, 默认'structure.as'
+
+    Returns
+    -------
+    Structure
+        pymatgen的Structure对象
+
+    Examples
+    --------
+    >>> from dspawpy.io.structure import from_dspaw_as
+    >>> S1 = from_dspaw_as(as_file='structure00.as')
+    """
+    D = {}
+    lines = get_lines_without_comment(as_file, "#")
+    N = int(lines[1])
+    lattice = []
+    for line in lines[3:6]:
+        vector = line.split()
+        lattice.extend([float(vector[0]), float(vector[1]), float(vector[2])])
+
+    lattice = np.asarray(lattice).reshape(3, 3)
+    is_direct = lines[6].strip().split()[0].startswith("Direct")
+    elements = []
+    positions = []
+    others = []
+    line6s = []
+    for i in range(N):
+        atom = lines[i + 7].strip().split()
+        elements.append(atom[0])
+        positions.extend([float(atom[1]), float(atom[2]), float(atom[3])])
+
+        if len(atom) > 4:
+            other = atom[4:]
+            others.append(other)
+            line6 = lines[6]
+            line6s.append(line6)
+
+    D.setdefault("others", others)
+    D.setdefault("line6s", line6s)
+
+    coords = np.asarray(positions).reshape(-1, 3)
+    if others == [] and line6s == []:
+        return Structure(
+            lattice, elements, coords, coords_are_cartesian=(not is_direct)
+        )
+    else:
+        return Structure(
+            lattice,
+            elements,
+            coords,
+            coords_are_cartesian=(not is_direct),
+            site_properties=D,
+        )
+
+
+def from_hzw(hzw_file) -> Structure:
+    """从hzw结构文件中读取结构信息
+
+    Parameters
+    ----------
+    hzw_file : str
+        hzw结构文件，以 .hzw 结尾
+
+    Returns
+    -------
+    Structure
+        pymatgen的Structure对象
+
+    Examples
+    --------
+    >>> from dspawpy.io.structure import from_hzw
+    >>> S1 = from_hzw(hzw_file='Si.hzw')
+    """
+    lines = get_lines_without_comment(hzw_file, "%")
+    number_of_probes = int(lines[0])
+    if number_of_probes != 0:
+        raise ValueError("dspaw only support 0 probes hzw file")
+    lattice = []
+    for line in lines[1:4]:
+        vector = line.split()
+        lattice.extend([float(vector[0]), float(vector[1]), float(vector[2])])
+
+    lattice = np.asarray(lattice).reshape(3, 3)
+    N = int(lines[4])
+    elements = []
+    positions = []
+    for i in range(N):
+        atom = lines[i + 5].strip().split()
+        elements.append(atom[0])
+        positions.extend([float(atom[1]), float(atom[2]), float(atom[3])])
+
+    coords = np.asarray(positions).reshape(-1, 3)
+    return Structure(lattice, elements, coords, coords_are_cartesian=True)
+
+
+def to_file(structure: Structure, filename: str, fmt, coords_are_cartesian=True):
+    """往结构文件中写入信息
+
+    Parameters
+    ----------
+    structure : Structure
+        pymatgen的Structure对象
+    filename : str
+        结构文件名
+    fmt : str
+        结构文件类型，支持 "json","as","hzw"
+    coords_are_cartesian : bool
+        坐标是否为笛卡尔坐标，默认为True
+
+    Examples
+    --------
+    >>> from dspawpy.io.structure import to_file
+    >>> to_file(structure, filename='Si.json', fmt='json')
+    >>> to_file(structure, filename='Si.as', fmt='as')
+    >>> to_file(structure, filename='Si.hzw', fmt='hzw')
+    """
+    if fmt == "json":
+        to_dspaw_json(structure, filename, coords_are_cartesian)
+    elif fmt == "as":
+        to_dspaw_as(structure, filename, coords_are_cartesian)
+    elif fmt == "hzw":
+        to_hzw(structure, filename)
+
+
+def from_dspaw_atominfo(hpath, index: int = None) -> Structure:
+    if not isinstance(hpath, str):  # for compatibility with aimd2pdb.py
+        atominfo = hpath
+        lattice = np.asarray(atominfo["Lattice"]).reshape(3, 3)
+        elements = []
+        positions = []
+        for atom in atominfo["Atoms"]:
+            elements.append(atom["Element"])
+            positions.extend(atom["Position"])
+
+        coords = np.asarray(positions).reshape(-1, 3)
+        is_direct = atominfo["CoordinateType"] == "Direct"
+        return Structure(
+            lattice, elements, coords, coords_are_cartesian=(not is_direct)
+        )
+    Nstep, elements, positions, lattices = read_h5(hpath, index)
+    return Structure(lattices[0], elements, positions[0], coords_are_cartesian=True)
+
+
+def from_dspaw_atominfo_json(atominfo: dict) -> Structure:
+    lattice = np.asarray(atominfo["Lattice"]).reshape(3, 3)
+    elements = []
+    positions = []
+    for atom in atominfo["Atoms"]:
+        elements.append(atom["Element"])
+        positions.extend(atom["Position"])
+
+    coords = np.asarray(positions).reshape(-1, 3)
+    is_direct = atominfo["CoordinateType"] == "Direct"
+    return Structure(lattice, elements, coords, coords_are_cartesian=(not is_direct))
+
+
+def from_dspaw_atominfos(aimd_dir: str) -> List[Structure]:
+    structures = []
+    if isinstance(aimd_dir, list) or isinstance(aimd_dir, np.ndarray):
+        for atominfo in aimd_dir:
+            structures.append(from_dspaw_atominfo(atominfo))
+    elif aimd_dir.endswith(".h5"):
+        aimd = h5py.File(aimd_dir)
+        steps = int(np.array(aimd.get("/Structures/FinalStep"))[0])
+        for i in range(steps):
+            structures.append(from_dspaw_atominfo(aimd_dir, i + 1))
+    elif aimd_dir.endswith(".json"):
+        with open(aimd_dir, "r") as fin:
+            aimd = json.load(fin)
+        for atominfo in aimd["Structures"]:
+            structures.append(from_dspaw_atominfo_json(atominfo))
+    else:
+        print("file - " + aimd_dir + " :  Unsupported format!")
+        return
+
+    return structures
+
+
+def to_dspaw_as(structure: Structure, filename: str, coords_are_cartesian=True):
+    with open(filename, "w", encoding="utf-8") as file:
+        file.write("Total number of atoms\n")
+        file.write("%d\n" % len(structure))
+
+        file.write("Lattice\n")
+        for v in structure.lattice.matrix:
+            file.write("%.6f %.6f %.6f\n" % (v[0], v[1], v[2]))
+
+        i = 0
+        for site in structure:
+            if i == 0:
+                if 'line6s' in site.properties:
+                    file.write("%s\n" % site.properties["line6s"])
+                else:
+                    if coords_are_cartesian:
+                        file.write("Cartesian\n")
+                    else:
+                        file.write("Direct\n")
+            i += 1
+
+            coords = site.coords if coords_are_cartesian else site.frac_coords
+            if 'others' in site.properties:
+                sp = " ".join(site.properties["others"])  # flatten str list
+                file.write(
+                    "%s %.6f %.6f %.6f %s\n"
+                    % (site.species_string, coords[0], coords[1], coords[2], sp)
+                )
+            else:
+                file.write(
+                    "%s %.6f %.6f %.6f\n"
+                    % (site.species_string, coords[0], coords[1], coords[2])
+                )
+
+
+def to_hzw(structure: Structure, filename: str):
+    with open(filename, "w", encoding="utf-8") as file:
+        file.write("% The number of probes \n")
+        file.write("0\n")
+        file.write("% Uni-cell vector\n")
+
+        for v in structure.lattice.matrix:
+            file.write("%.6f %.6f %.6f\n" % (v[0], v[1], v[2]))
+
+        file.write("% Total number of device_structure\n")
+        file.write("%d\n" % len(structure))
+        file.write("% Atom site\n")
+
+        for site in structure:
+            file.write(
+                "%s %.6f %.6f %.6f\n"
+                % (site.species_string, site.coords[0], site.coords[1], site.coords[2])
+            )
+
+
+def to_dspaw_dict(structure: Structure, coords_are_cartesian=True) -> Dict:
+    lattice = structure.lattice.matrix.flatten().tolist()
+    atoms = []
+    for site in structure:
+        coords = site.coords if coords_are_cartesian else site.frac_coords
+        atoms.append({"Element": site.species_string, "Position": coords.tolist()})
+
+    coordinate_type = "Cartesian" if coords_are_cartesian else "Direct"
+    return {"Lattice": lattice, "CoordinateType": coordinate_type, "Atoms": atoms}
+
+
+def to_dspaw_json(structure: Structure, filename: str, coords_are_cartesian=True):
+    d = to_dspaw_dict(structure, coords_are_cartesian)
+    with open(filename, "w", encoding="utf-8") as file:
+        json.dump(d, file, indent=4)
+
+
+def to_pdb(structures: List[Structure], pdb_filename: str):
+    with open(pdb_filename, "w", encoding="utf-8") as file:
+        for i, s in enumerate(structures):
+            file.write("MODEL         %d\n" % (i + 1))
+            file.write("REMARK   Converted from Structures\n")
+            file.write("REMARK   Converted using dspawpy\n")
+            lengths = s.lattice.lengths
+            angles = s.lattice.angles
+            file.write(
+                "CRYST1{0:9.3f}{1:9.3f}{2:9.3f}{3:7.2f}{4:7.2f}{5:7.2f}\n".format(
+                    lengths[0], lengths[1], lengths[2], angles[0], angles[1], angles[2]
+                )
+            )
+            for j, site in enumerate(s):
+                file.write(
+                    "%4s%7d%4s%5s%6d%4s%8.3f%8.3f%8.3f%6.2f%6.2f%12s\n"
+                    % (
+                        "ATOM",
+                        j + 1,
+                        site.species_string,
+                        "MOL",
+                        1,
+                        "    ",
+                        site.coords[0],
+                        site.coords[1],
+                        site.coords[2],
+                        1.0,
+                        0.0,
+                        site.species_string,
+                    )
+                )
+            file.write("TER\n")
+            file.write("ENDMDL\n")
```

## dspawpy/io/utils.py

```diff
@@ -1,327 +1,326 @@
-from scipy import integrate
-import re
-from typing import List
-import numpy as np
-import os
-
-
-def d_band(spin, dos_data):  # 定义函数，括号里给出函数的两个变量
-    """计算d带中心
-
-    Parameters
-    ----------
-    spin : Spin.up或Spin.down
-        自旋类型，
-    dos_data : pymatgen.electronic_structure.dos.CompleteDos
-        dos数据
-
-    Returns
-    -------
-    db1 : float
-        d带中心数值
-
-    Examples
-    --------
-    >>> from dspawpy.io.utils import d_band
-    >>> from dspawpy.io.read import get_dos_data
-    >>> dd = get_dos_data("dos.h5")  # 从dos.h5中读取数据
-    >>> db1 = d_band(spin=Spin.up, dos_data=dd)
-    >>> print(db1)
-    """
-
-    dos_d = dos_data.get_spd_dos()[2]
-    Efermi = dos_data.efermi
-    epsilon = dos_d.energies - Efermi  # shift d-band center
-
-    N1 = dos_d.densities[spin]
-    M1 = epsilon * N1
-    SummaM1 = integrate.simps(M1, epsilon)
-    SummaN1 = integrate.simps(N1, epsilon)
-
-    return SummaM1 / SummaN1
-
-
-def get_ele_from_h5(hpath: str = "aimd.h5"):
-    """从h5文件中读取元素列表
-
-    Parameters
-    ----------
-    hpath : str
-        h5文件路径
-
-    Returns
-    -------
-    ele : list
-        元素列表, Natom x 1
-
-    Examples
-    --------
-    >>> from dspawpy.io.utils import get_ele_from_h5
-    >>> ele = get_ele_from_h5(hpath='aimd.h5')
-    ['H', 'H', 'O']
-    """
-    import h5py
-
-    data = h5py.File(hpath)
-    Elements_bytes = np.array(data.get("/AtomInfo/Elements"))
-    tempdata = np.array([i.decode() for i in Elements_bytes])
-    ele = "".join(tempdata).split(";")
-
-    return ele
-
-
-def get_pos_ele_lat(spath: str):
-    """从DSPAW的as结构文件中读取坐标、元素列表，和晶胞信息
-
-    Parameters
-    ----------
-    path : str
-        结构文件路径
-
-    Returns
-    -------
-    pos : np.ndarray
-        坐标分量数组，Natom x 3
-    ele : list
-        元素列表, Natom x 1
-    latv : np.ndarray
-        晶胞矢量数组，3 x 3
-
-    Examples
-    --------
-    >>> from dspawpy.io.utils import get_pos_ele_lat
-    >>> pos, ele, latv = get_pos_ele_lat(spath='structure.as')
-    >>> pos
-    array([[ 0.        ,  0.        ,  9.06355632],
-           [ 1.59203323,  0.91916082, 10.62711265],
-           [ 1.59203323,  0.91916082,  7.5       ]])
-    >>> ele
-    ['Mo', 'S', 'S']
-    >>> latv
-    array([[ 3.18406646,  0.        ,  0.        ],
-           [-1.59203323,  2.75748245,  0.        ],
-           [ 0.        ,  0.        , 30.        ]])
-    """
-
-    with open(spath, "r") as f:
-        lines = f.readlines()
-        Natom = int(lines[1])  # 原子总数
-        ele = [line.split()[0] for line in lines[7 : 7 + Natom]]  # 元素列表
-
-        # 晶格矢量
-        latv = np.array([line.split()[0:3] for line in lines[3:6]], dtype=float)
-        # xyz坐标分量
-        coord = np.array(
-            [line.split()[1:4] for line in lines[7 : 7 + Natom]], dtype=float
-        )
-        if lines[6].startswith("C"):
-            pos = coord
-        elif lines[6].startswith("D"):  # 分数 --> 笛卡尔
-            pos = np.dot(coord, latv)
-        else:
-            raise ValueError(f"{spath}中的坐标类型未知！")
-
-    return pos, ele, latv
-
-
-def get_spo_ele_lat(spath: str):
-    """从DSPAW的as结构文件中读取分数坐标、元素列表，和晶胞信息
-
-    Parameters
-    ----------
-    spath : str
-        结构文件路径
-
-    Returns
-    -------
-    spos : np.ndarray
-        分数坐标分量数组，Natom x 3
-    ele : list
-        元素列表, Natom x 1
-    latv : np.ndarray
-        晶胞矢量数组，3 x 3
-
-    Examples
-    --------
-    >>> from dspawpy.io.utils import get_spo_ele_lat
-    >>> spos, ele, latv = get_spo_ele_lat(spath='structure.as')
-    >>> spos
-    array([[0.        , 0.        , 0.30211854],
-           [0.66666667, 0.33333333, 0.35423709],
-           [0.66666667, 0.33333333, 0.25      ]])
-    >>> ele
-    ['Mo', 'S', 'S']
-    >>> latv
-    array([[ 3.18406646,  0.        ,  0.        ],
-           [-1.59203323,  2.75748245,  0.        ],
-           [ 0.        ,  0.        , 30.        ]])
-    """
-
-    with open(spath, "r") as f:
-        lines = f.readlines()
-        Natom = int(lines[1])  # 原子总数
-        ele = [line.split()[0] for line in lines[7 : 7 + Natom]]  # 元素列表
-
-        # 晶格矢量
-        latv = np.array([line.split() for line in lines[3:6]], dtype=float)
-        # xyz坐标分量
-        coord = np.array(
-            [line.split()[1:4] for line in lines[7 : 7 + Natom]], dtype=float
-        )
-        if lines[6].startswith("C"):  # 笛卡尔 --> 分数坐标
-            spos = np.linalg.solve(latv.T, np.transpose(coord)).T
-        elif lines[6].startswith("D"):
-            spos = coord
-        else:
-            raise ValueError(f"{spath}中的坐标类型未知！")
-
-    return spos, ele, latv
-
-
-def thermo_correction(fretxt: str = "frequency.txt", T: float = 298.15):
-    """从fretext中读取数据，计算ZPE和TS
-
-    将另外保存结果到 ZPE_TS.dat 中
-
-    Parameters
-    ----------
-    fretxt : str
-        记录频率信息的文件所在路径, 默认当前路径下的'frequency.txt'
-    T : float
-        温度，单位K, 默认298.15
-
-    Returns
-    -------
-    ZPE: float
-        零点能
-    TS: float
-        熵校正
-
-    Examples
-    --------
-    >>> from dspawpy.io.utils import thermo_correction
-    >>> ZPE, TS = thermo_correction(fretxt='frequency.txt', T=298.15)
-    >>> ZPE
-    0.8842567390000002
-    >>> TS
-    0.18362317157111566
-    """
-
-    # 1. read data
-    data_get_ZPE = []
-    data_get_TS = []
-
-    with open(fretxt, "r") as f:
-        for line in f.readlines():
-            data_line = line.strip().split()
-            if len(data_line) != 6:
-                continue
-            if data_line[1] == "f":
-                data_get_ZPE.append(float(data_line[5]))
-                data_get_TS.append(float(data_line[2]))
-
-    data_get_ZPE = np.array(data_get_ZPE)
-    data_get_TS = np.array(data_get_TS)
-
-    # 2. printout to check
-    print(f"=== 从{fretxt}中读取到的相关如下 ===")
-    print("    ZPE(meV) \t TS(THz)：")
-    for i in range(len(data_get_ZPE)):
-        print(" ", data_get_ZPE[i], "\t", data_get_TS[i])
-    print("-" * 40)
-
-    if len(data_get_ZPE) == 0:
-        raise ValueError("全是虚频，请考虑重新优化结构...")
-    else:
-        print("正在写入ZPE_TS.dat文件...")
-        np.savetxt(
-            "ZPE_TS.dat",
-            np.array([data_get_ZPE, data_get_TS]).T,
-            fmt="%.6f",
-            header="ZPE(meV) \t TS(THz)",
-            comments=f"Data read from {os.path.abspath(fretxt)}\n",
-        )
-
-    # 3. calculate
-    ZPE = 0
-    for data in data_get_ZPE:
-        ZPE += data / 2000.0
-    print("\n--> 零点振动能为：", ZPE)
-
-    # T = 298.15 #温度 单位：K
-    # S = 0
-    Na = 6.02214179e23  # 阿伏伽德罗常数 单位 /mol
-    h = 6.6260696e-34  # 普朗克常数 单位J*s
-    kB = 1.3806503e-23  # 玻尔兹曼常数 J/K
-    R = Na * kB  # 理想气体常数 J/(K*mol)
-    # THz = 1e+12 # 1 Hz = 1e+12 THz
-    # e = 1.60217653E-19 #单位 C
-
-    sum_S = 0
-    import math  # 因为要使用 e的多少次方，ln（）对数
-
-    for vi_THz in data_get_TS:
-        vi_Hz = vi_THz * 1e12
-        m1 = h * Na * vi_Hz
-        m2 = h * vi_Hz / (kB * T)
-        m3 = math.exp(m2) - 1
-        m4 = T * m3
-        m5 = 1 - math.exp(-m2)  # math.exp(3) 就是e的3次方
-        m6 = math.log(m5, math.e)  # m6= ln(m5)   math.e在python中=e ，以右边为底的对数
-        m7 = R * m6
-        m8 = m1 / m4 - m7  # S 单位J/(mol*K)
-        m9 = (T * m8 / 1000) / 96.49  # T*S,将单位化为KJ/mol, 96.49 kJ/mol = 1 eV 单位eV
-        sum_S += m9
-
-    print("--> TS为：", sum_S)
-
-    with open("ZPE_TS.dat", "a") as f:
-        f.write(f"\n--> ZPE: {ZPE}")
-        f.write(f"\n--> TS: {sum_S}\n")
-
-    return ZPE, sum_S
-
-
-def get_lines_without_comment(filename: str, comment: str = "#") -> List[str]:
-    lines = []
-    with open(filename) as file:
-        while True:
-            line = file.readline()
-            if line:
-                line = re.sub(comment + r".*$", "", line)  # remove comment
-                line = line.strip()
-                if line:
-                    lines.append(line)
-            else:
-                break
-
-    return lines
-
-
-def _get_coordinateType_from_h5(hpath: str = "aimd.h5"):
-    """从h5文件中读取坐标类型
-
-    Parameters
-    ----------
-    hpath : str
-        h5文件路径
-
-    Returns
-    -------
-    coordinateType : list
-        坐标类型
-
-    Examples
-    --------
-    >>> from dspawpy.io.utils import get_coordinateType_from_h5
-    >>> coordinateType = get_coordinateType_from_h5(hpath='scf.h5')
-    ['Cartesian']
-    """
-    import h5py
-
-    data = h5py.File(hpath)
-    CoordinateType = np.array(data.get("/AtomInfo/CoordinateType"))
-    tempdata = np.array([i.decode() for i in CoordinateType])
-    coordinateType = "".join(tempdata).split(";")[0]
-
-    return coordinateType
+from scipy import integrate
+import re
+from typing import List
+import numpy as np
+import os
+import pandas as pd
+
+
+def d_band(spin, dos_data):  # 定义函数，括号里给出函数的两个变量
+    """计算d带中心
+
+    Parameters
+    ----------
+    spin : Spin.up或Spin.down
+        自旋类型，
+    dos_data : pymatgen.electronic_structure.dos.CompleteDos
+        dos数据
+
+    Returns
+    -------
+    db1 : float
+        d带中心数值
+
+    Examples
+    --------
+    >>> from dspawpy.io.utils import d_band
+    >>> from dspawpy.io.read import get_dos_data
+    >>> dd = get_dos_data("dos.h5")  # 从dos.h5中读取数据
+    >>> db1 = d_band(spin=Spin.up, dos_data=dd)
+    >>> print(db1)
+    """
+
+    dos_d = dos_data.get_spd_dos()[2]
+    Efermi = dos_data.efermi
+    epsilon = dos_d.energies - Efermi  # shift d-band center
+
+    N1 = dos_d.densities[spin]
+    M1 = epsilon * N1
+    SummaM1 = integrate.simps(M1, epsilon)
+    SummaN1 = integrate.simps(N1, epsilon)
+
+    return SummaM1 / SummaN1
+
+
+def get_ele_from_h5(hpath: str = "aimd.h5"):
+    """从h5文件中读取元素列表
+
+    Parameters
+    ----------
+    hpath : str
+        h5文件路径
+
+    Returns
+    -------
+    ele : list
+        元素列表, Natom x 1
+
+    Examples
+    --------
+    >>> from dspawpy.io.utils import get_ele_from_h5
+    >>> ele = get_ele_from_h5(hpath='aimd.h5')
+    ['H', 'H', 'O']
+    """
+    import h5py
+
+    data = h5py.File(hpath)
+    Elements_bytes = np.array(data.get("/AtomInfo/Elements"))
+    tempdata = np.array([i.decode() for i in Elements_bytes])
+    ele = "".join(tempdata).split(";")
+
+    return ele
+
+
+def get_pos_ele_lat(spath: str):
+    """从DSPAW的as结构文件中读取坐标、元素列表，和晶胞信息
+
+    Parameters
+    ----------
+    path : str
+        结构文件路径
+
+    Returns
+    -------
+    pos : np.ndarray
+        坐标分量数组，Natom x 3
+    ele : list
+        元素列表, Natom x 1
+    latv : np.ndarray
+        晶胞矢量数组，3 x 3
+
+    Examples
+    --------
+    >>> from dspawpy.io.utils import get_pos_ele_lat
+    >>> pos, ele, latv = get_pos_ele_lat(spath='structure.as')
+    >>> pos
+    array([[ 0.        ,  0.        ,  9.06355632],
+           [ 1.59203323,  0.91916082, 10.62711265],
+           [ 1.59203323,  0.91916082,  7.5       ]])
+    >>> ele
+    ['Mo', 'S', 'S']
+    >>> latv
+    array([[ 3.18406646,  0.        ,  0.        ],
+           [-1.59203323,  2.75748245,  0.        ],
+           [ 0.        ,  0.        , 30.        ]])
+    """
+
+    with open(spath, "r") as f:
+        lines = f.readlines()
+        Natom = int(lines[1])  # 原子总数
+        ele = [line.split()[0] for line in lines[7 : 7 + Natom]]  # 元素列表
+
+        # 晶格矢量
+        latv = np.array([line.split()[0:3] for line in lines[3:6]], dtype=float)
+        # xyz坐标分量
+        coord = np.array(
+            [line.split()[1:4] for line in lines[7 : 7 + Natom]], dtype=float
+        )
+        if lines[6].startswith("C"):
+            pos = coord
+        elif lines[6].startswith("D"):  # 分数 --> 笛卡尔
+            pos = np.dot(coord, latv)
+        else:
+            raise ValueError(f"{spath}中的坐标类型未知！")
+
+    return pos, ele, latv
+
+
+def get_spo_ele_lat(spath: str):
+    """从DSPAW的as结构文件中读取分数坐标、元素列表，和晶胞信息
+
+    Parameters
+    ----------
+    spath : str
+        结构文件路径
+
+    Returns
+    -------
+    spos : np.ndarray
+        分数坐标分量数组，Natom x 3
+    ele : list
+        元素列表, Natom x 1
+    latv : np.ndarray
+        晶胞矢量数组，3 x 3
+
+    Examples
+    --------
+    >>> from dspawpy.io.utils import get_spo_ele_lat
+    >>> spos, ele, latv = get_spo_ele_lat(spath='structure.as')
+    >>> spos
+    array([[0.        , 0.        , 0.30211854],
+           [0.66666667, 0.33333333, 0.35423709],
+           [0.66666667, 0.33333333, 0.25      ]])
+    >>> ele
+    ['Mo', 'S', 'S']
+    >>> latv
+    array([[ 3.18406646,  0.        ,  0.        ],
+           [-1.59203323,  2.75748245,  0.        ],
+           [ 0.        ,  0.        , 30.        ]])
+    """
+
+    with open(spath, "r") as f:
+        lines = f.readlines()
+        Natom = int(lines[1])  # 原子总数
+        ele = [line.split()[0] for line in lines[7 : 7 + Natom]]  # 元素列表
+
+        # 晶格矢量
+        latv = np.array([line.split() for line in lines[3:6]], dtype=float)
+        # xyz坐标分量
+        coord = np.array(
+            [line.split()[1:4] for line in lines[7 : 7 + Natom]], dtype=float
+        )
+        if lines[6].startswith("C"):  # 笛卡尔 --> 分数坐标
+            spos = np.linalg.solve(latv.T, np.transpose(coord)).T
+        elif lines[6].startswith("D"):
+            spos = coord
+        else:
+            raise ValueError(f"{spath}中的坐标类型未知！")
+
+    return spos, ele, latv
+
+
+def thermo_correction(fretxt: str = "frequency.txt", T: float = 298.15):
+    """从fretext中读取数据，计算ZPE和TS
+
+    将另外保存结果到 ZPE_TS.dat 中
+
+    Parameters
+    ----------
+    fretxt : str
+        记录频率信息的文件所在路径, 默认当前路径下的'frequency.txt'
+    T : float
+        温度，单位K, 默认298.15
+
+    Returns
+    -------
+    ZPE: float
+        零点能
+    TS: float
+        熵校正
+
+    Examples
+    --------
+    >>> from dspawpy.io.utils import thermo_correction
+    >>> ZPE, TS = thermo_correction(fretxt='frequency.txt', T=298.15)
+    >>> ZPE
+    0.8842567390000002
+    >>> TS
+    0.18362317157111566
+    """
+
+    # 1. read data
+    data_get_ZPE = []
+    data_get_TS = []
+
+    with open(fretxt, "r") as f:
+        for line in f.readlines():
+            data_line = line.strip().split()
+            if len(data_line) != 6:
+                continue
+            if data_line[1] == "f":
+                data_get_ZPE.append(float(data_line[5]))
+                data_get_TS.append(float(data_line[2]))
+
+    data_get_ZPE = np.array(data_get_ZPE)
+    data_get_TS = np.array(data_get_TS)
+
+    # 2. printout to check
+    print(f"=== 从{fretxt}中读取到的相关如下 ===")
+    dt = pd.DataFrame({'Frequency (meV)':data_get_ZPE, 'Frequency (THz)':data_get_TS}, index=None)
+    print(dt)
+
+    if len(data_get_ZPE) == 0:
+        raise ValueError("全是虚频，请考虑重新优化结构...")
+    else:
+        print("\n正在写入ZPE_TS.dat文件...")
+        np.savetxt(
+            "ZPE_TS.dat",
+            np.array([data_get_ZPE, data_get_TS]).T,
+            fmt="%.6f",
+            header="ZPE(meV) \t TS(THz)",
+            comments=f"Data read from {os.path.abspath(fretxt)}\n",
+        )
+
+    # 3. calculate
+    ZPE = 0
+    for data in data_get_ZPE:
+        ZPE += data / 2000.0
+    print("\n--> Zero-point energy,  ZPE (eV):", ZPE)
+
+    # T = 298.15 #温度 单位：K
+    # S = 0
+    Na = 6.02214179e23  # 阿伏伽德罗常数 单位 /mol
+    h = 6.6260696e-34  # 普朗克常数 单位J*s
+    kB = 1.3806503e-23  # 玻尔兹曼常数 J/K
+    R = Na * kB  # 理想气体常数 J/(K*mol)
+    # THz = 1e+12 # 1 Hz = 1e+12 THz
+    # e = 1.60217653E-19 #单位 C
+
+    sum_S = 0
+    import math  # 因为要使用 e的多少次方，ln（）对数
+
+    for vi_THz in data_get_TS:
+        vi_Hz = vi_THz * 1e12
+        m1 = h * Na * vi_Hz
+        m2 = h * vi_Hz / (kB * T)
+        m3 = math.exp(m2) - 1
+        m4 = T * m3
+        m5 = 1 - math.exp(-m2)  # math.exp(3) 就是e的3次方
+        m6 = math.log(m5, math.e)  # m6= ln(m5)   math.e在python中=e ，以右边为底的对数
+        m7 = R * m6
+        m8 = m1 / m4 - m7  # S 单位J/(mol*K)
+        m9 = (T * m8 / 1000) / 96.49  # T*S,将单位化为KJ/mol, 96.49 kJ/mol = 1 eV 单位eV
+        sum_S += m9
+
+    print("--> Entropy contribution, T*S (eV)：", sum_S)
+
+    with open("ZPE_TS.dat", "a") as f:
+        f.write(f"\n--> Zero-point energy,  ZPE (eV): {ZPE}")
+        f.write(f"\n--> Entropy contribution, T*S (eV): {sum_S}\n")
+
+    return ZPE, sum_S
+
+
+def get_lines_without_comment(filename: str, comment: str = "#") -> List[str]:
+    lines = []
+    with open(filename) as file:
+        while True:
+            line = file.readline()
+            if line:
+                line = re.sub(comment + r".*$", "", line)  # remove comment
+                line = line.strip()
+                if line:
+                    lines.append(line)
+            else:
+                break
+
+    return lines
+
+
+def _get_coordinateType_from_h5(hpath: str = "aimd.h5"):
+    """从h5文件中读取坐标类型
+
+    Parameters
+    ----------
+    hpath : str
+        h5文件路径
+
+    Returns
+    -------
+    coordinateType : list
+        坐标类型
+
+    Examples
+    --------
+    >>> from dspawpy.io.utils import get_coordinateType_from_h5
+    >>> coordinateType = get_coordinateType_from_h5(hpath='scf.h5')
+    ['Cartesian']
+    """
+    import h5py
+
+    data = h5py.File(hpath)
+    CoordinateType = np.array(data.get("/AtomInfo/CoordinateType"))
+    tempdata = np.array([i.decode() for i in CoordinateType])
+    coordinateType = "".join(tempdata).split(";")[0]
+
+    return coordinateType
```

## dspawpy/io/write.py

 * *Ordering differences only*

```diff
@@ -1,308 +1,308 @@
-# -*- coding: utf-8 -*-
-import json
-import numpy as np
-from typing import Dict, List
-from dspawpy.io.read import load_h5
-
-
-def write_VESTA(in_filename: str, data_type, out_filename="DS-PAW.vesta"):
-    """从包含电子体系信息的json或h5文件中读取数据并写入VESTA格式的文件中
-
-    Parameters
-    ----------
-    in_filename : str
-        包含电子体系信息的json或h5文件路径
-    data_type: str
-        数据类型，支持 "rho", "potential", "elf", "pcharge", "boundcharge"
-    out_filename : str
-        输出文件路径, 默认 "DS-PAW.vesta"
-
-    Returns
-    --------
-    out_filename : file
-        VESTA格式的文件
-
-    Examples
-    --------
-    >>> from dspawpy.io.write import write_VESTA
-    >>> write_VESTA("DS-PAW.json", "rho")
-    """
-    if in_filename.endswith(".h5"):
-        data = load_h5(in_filename)
-        if data_type.lower() == "rho" or data_type.lower() == "boundcharge":
-            write_VESTA_format(data, ["/Rho/TotalCharge"], out_filename)
-        elif data_type.lower() == "potential":
-            write_VESTA_format(
-                data,
-                [
-                    "/Potential/TotalElectrostaticPotential",
-                ],
-                out_filename,
-            )
-        elif data_type.lower() == "elf":
-            write_VESTA_format(data, ["/ELF/TotalELF"], out_filename)
-        elif data_type.lower() == "pcharge":
-            write_VESTA_format(data, ["/Pcharge/1/TotalCharge"], out_filename)
-        else:
-            raise NotImplementedError("仅支持rho/potential/elf/pcharge/boundcharge")
-
-    elif in_filename.endswith(".json"):
-        with open(in_filename, "r") as fin:
-            data = json.load(fin)
-        if data_type.lower() == "rho" or data_type.lower() == "boundcharge":
-            write_VESTA_format_json(
-                data["AtomInfo"], [data["Rho"]["TotalCharge"]], out_filename
-            )
-        elif data_type.lower() == "potential":
-            write_VESTA_format_json(
-                data["AtomInfo"],
-                [
-                    data["Potential"]["TotalElectrostaticPotential"],
-                ],
-                out_filename,
-            )
-        elif data_type.lower() == "elf":
-            write_VESTA_format_json(
-                data["AtomInfo"], [data["ELF"]["TotalELF"]], out_filename
-            )
-        elif data_type.lower() == "pcharge":
-            write_VESTA_format_json(
-                data["AtomInfo"], [data["Pcharge"][0]["TotalCharge"]], out_filename
-            )
-        else:
-            raise NotImplementedError("仅支持rho/potential/elf/pcharge/boundcharge")
-
-    else:
-        raise NotImplementedError("仅支持json或h5格式文件")
-
-def write_delta_rho_vesta(AB, A, B, output="delta_rho.vesta"):
-    """电荷密度差分可视化
-
-    DeviceStudio暂不支持大文件，临时写成可以用VESTA打开的格式
-
-    Parameters
-    ----------
-    AB : str
-        AB的电荷密度文件路径，可以是h5或json格式
-    A : str
-        A的电荷密度文件路径，可以是h5或json格式
-    B : str
-        B的电荷密度文件路径，可以是h5或json格式
-    output : str
-        输出文件路径，默认 "delta_rho.vesta"
-
-    Returns
-    -------
-    output : file
-        电荷差分（AB-A-B）后的电荷密度文件，
-
-    Examples
-    --------
-    >>> from dspawpy.io.write import write_delta_rho_vesta
-    >>> write_delta_rho_vesta('AB.h5', 'A.h5', 'B.h5', 'delta_rho.vesta')
-    >>> write_delta_rho_vesta('AB.json', 'A.json', 'B.json', 'delta_rho.vesta')
-    # 甚至可以混写
-    >>> write_delta_rho_vesta('AB.h5', 'A.json', 'B.json', 'delta_rho.vesta')
-    """
-    print(f'读取{AB}...')
-    if AB.endswith(".h5"):
-        dataAB = load_h5(AB)
-        rhoAB = np.array(dataAB["/Rho/TotalCharge"])
-        nGrids = dataAB["/AtomInfo/Grid"]
-        atom_symbol = dataAB["/AtomInfo/Elements"]
-        atom_pos = dataAB["/AtomInfo/Position"]
-        latticeConstantMatrix = dataAB["/AtomInfo/Lattice"]
-        atom_pos = np.array(atom_pos).reshape(-1, 3)
-    elif AB.endswith(".json"):
-        atom_symbol = []
-        atom_pos = []
-        with open(AB, "r") as f1:
-            dataAB = json.load(f1)
-            rhoAB = np.array(dataAB["Rho"]["TotalCharge"])
-            nGrids = dataAB["AtomInfo"]["Grid"]
-        for i in range(len(dataAB["AtomInfo"]["Atoms"])):
-            atom_symbol.append(dataAB["AtomInfo"]["Atoms"][i]["Element"])
-            atom_pos.append(dataAB["AtomInfo"]["Atoms"][i]["Position"])
-            atom_pos = np.array(atom_pos)
-
-        latticeConstantMatrix = dataAB["AtomInfo"]["Lattice"]
-    else:
-        raise ValueError(f"file format must be either h5 or json: {AB}")
-
-    print(f'读取{A}...')
-    if A.endswith(".h5"):
-        dataA = load_h5(A)
-        rhoA = np.array(dataA["/Rho/TotalCharge"])
-    elif A.endswith(".json"):
-        with open(A, "r") as f2:
-            dataA = json.load(f2)
-            rhoA = np.array(dataA["Rho"]["TotalCharge"])
-    else:
-        raise ValueError(f"file format must be either h5 or json: {A}")
-
-    print(f'读取{B}...')
-    if B.endswith(".h5"):
-        dataB = load_h5(B)
-        rhoB = np.array(dataB["/Rho/TotalCharge"])
-    elif B.endswith(".json"):
-        with open(B, "r") as f3:
-            dataB = json.load(f3)
-            rhoB = np.array(dataB["Rho"]["TotalCharge"])
-    else:
-        raise ValueError(f"file format must be either h5 or json: {B}")
-
-    print(f'计算电荷差分...')
-    rho = rhoAB - rhoA - rhoB
-    rho = np.array(rho).reshape(nGrids[0], nGrids[1], nGrids[2])
-
-    element = list(set(atom_symbol))
-    element = sorted(set(atom_symbol), key=atom_symbol.index)
-    element_num = np.zeros(len(element))
-    for i in range(len(element)):
-        element_num[i] = atom_symbol.count(element[i])
-
-    latticeConstantMatrix = np.array(latticeConstantMatrix)
-    latticeConstantMatrix = latticeConstantMatrix.reshape(3, 3)
-
-    print(f'写入文件{output}...')
-    with open(output, "w") as out:
-        out.write("DS-PAW_rho\n")
-        out.write("    1.000000\n")
-        for i in range(3):
-            for j in range(3):
-                out.write("    " + str(latticeConstantMatrix[i, j]) + "    ")
-            out.write("\n")
-        for i in range(len(element)):
-            out.write("    " + element[i] + "    ")
-        out.write("\n")
-
-        for i in range(len(element_num)):
-            out.write("    " + str(int(element_num[i])) + "    ")
-        out.write("\n")
-        out.write("Direct\n")
-        for i in range(len(atom_pos)):
-            for j in range(3):
-                out.write("    " + str(atom_pos[i, j]) + "    ")
-            out.write("\n")
-        out.write("\n")
-
-        for i in range(3):
-            out.write("  " + str(nGrids[i]) + "  ")
-        out.write("\n")
-
-        ind = 0
-        for i in range(nGrids[0]):
-            for j in range(nGrids[1]):
-                for k in range(nGrids[2]):
-                    out.write("  " + str(rho[i, j, k]) + "  ")
-                    ind = ind + 1
-                    if ind % 5 == 0:
-                        out.write("\n")
-
-    print(f"成功写入 {output}")
-
-
-def write_atoms(fileobj, hdf5):
-    fileobj.write("DS-PAW Structure\n")
-    fileobj.write("  1.00\n")
-    lattice = np.asarray(hdf5["/AtomInfo/Lattice"]).reshape(-1, 1)  # 将列表lattice下的多个列表整合
-    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[0][0], lattice[1][0], lattice[2][0]))
-    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[3][0], lattice[4][0], lattice[5][0]))
-    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[6][0], lattice[7][0], lattice[8][0]))
-
-    elements = hdf5["/AtomInfo/Elements"]
-    elements_set = []
-    elements_number = {}
-    for e in elements:
-        if e in elements_set:
-            elements_number[e] = elements_number[e] + 1
-        else:
-            elements_set.append(e)
-            elements_number[e] = 1
-
-    for e in elements_set:
-        fileobj.write("  " + e)
-    fileobj.write("\n")
-
-    for e in elements_set:
-        fileobj.write("%5d" % (elements_number[e]))
-    fileobj.write("\n")
-    if hdf5["/AtomInfo/CoordinateType"][0] == "Direct":
-        fileobj.write("Direct\n")
-    else:
-        fileobj.write("Cartesian\n")
-    for i, p in enumerate(hdf5["/AtomInfo/Position"]):
-        fileobj.write("%10.6f" % p)
-        if (i + 1) % 3 == 0:
-            fileobj.write("\n")
-    fileobj.write("\n")
-
-
-def write_VESTA_format(hdf5: Dict, datakeys: list, filename):
-    with open(filename, "w") as file:
-        write_atoms(file, hdf5)
-        for key in datakeys:
-            d = np.asarray(hdf5[key]).reshape(-1, 1)  # 将列表hdf5[key]下的多个列表整合
-            file.write("%5d %5d %5d\n" % tuple(hdf5["/AtomInfo/Grid"]))
-            i = 0
-            while i < len(d):
-                for j in range(10):
-                    file.write("%10.5f " % d[i])
-                    i += 1
-                    if i >= len(d):
-                        break
-                file.write("\n")
-
-            file.write("\n")
-
-
-def write_atoms_json(fileobj, atom_info):
-    fileobj.write("DS-PAW Structure\n")
-    fileobj.write("  1.00\n")
-    lattice = atom_info["Lattice"]
-
-    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[0], lattice[1], lattice[2]))
-    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[3], lattice[4], lattice[5]))
-    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[6], lattice[7], lattice[8]))
-
-    elements = [atom["Element"] for atom in atom_info["Atoms"]]
-    elements_set = []
-    elements_number = {}
-    for e in elements:
-        if e in elements_set:
-            elements_number[e] = elements_number[e] + 1
-        else:
-            elements_set.append(e)
-            elements_number[e] = 1
-
-    for e in elements_set:
-        fileobj.write("  " + e)
-    fileobj.write("\n")
-
-    for e in elements_set:
-        fileobj.write("%5d" % (elements_number[e]))
-    fileobj.write("\n")
-    if atom_info["CoordinateType"] == "Direct":
-        fileobj.write("Direct\n")
-    else:
-        fileobj.write("Cartesian\n")
-    for atom in atom_info["Atoms"]:
-        fileobj.write("%10.6f %10.6f %10.6f\n" % tuple(atom["Position"]))
-    fileobj.write("\n")
-
-
-def write_VESTA_format_json(atom_info: Dict, data: List, filename):
-    with open(filename, "w") as file:
-        write_atoms_json(file, atom_info)
-        for d in data:
-            file.write("%5d %5d %5d\n" % tuple(atom_info["Grid"]))
-            i = 0
-            while i < len(d):
-                for j in range(10):
-                    file.write("%10.5f " % d[i])
-                    i += 1
-                    if i >= len(d):
-                        break
-                file.write("\n")
-
-            file.write("\n")
+# -*- coding: utf-8 -*-
+import json
+import numpy as np
+from typing import Dict, List
+from dspawpy.io.read import load_h5
+
+
+def write_VESTA(in_filename: str, data_type, out_filename="DS-PAW.vesta"):
+    """从包含电子体系信息的json或h5文件中读取数据并写入VESTA格式的文件中
+
+    Parameters
+    ----------
+    in_filename : str
+        包含电子体系信息的json或h5文件路径
+    data_type: str
+        数据类型，支持 "rho", "potential", "elf", "pcharge", "boundcharge"
+    out_filename : str
+        输出文件路径, 默认 "DS-PAW.vesta"
+
+    Returns
+    --------
+    out_filename : file
+        VESTA格式的文件
+
+    Examples
+    --------
+    >>> from dspawpy.io.write import write_VESTA
+    >>> write_VESTA("DS-PAW.json", "rho")
+    """
+    if in_filename.endswith(".h5"):
+        data = load_h5(in_filename)
+        if data_type.lower() == "rho" or data_type.lower() == "boundcharge":
+            write_VESTA_format(data, ["/Rho/TotalCharge"], out_filename)
+        elif data_type.lower() == "potential":
+            write_VESTA_format(
+                data,
+                [
+                    "/Potential/TotalElectrostaticPotential",
+                ],
+                out_filename,
+            )
+        elif data_type.lower() == "elf":
+            write_VESTA_format(data, ["/ELF/TotalELF"], out_filename)
+        elif data_type.lower() == "pcharge":
+            write_VESTA_format(data, ["/Pcharge/1/TotalCharge"], out_filename)
+        else:
+            raise NotImplementedError("仅支持rho/potential/elf/pcharge/boundcharge")
+
+    elif in_filename.endswith(".json"):
+        with open(in_filename, "r") as fin:
+            data = json.load(fin)
+        if data_type.lower() == "rho" or data_type.lower() == "boundcharge":
+            write_VESTA_format_json(
+                data["AtomInfo"], [data["Rho"]["TotalCharge"]], out_filename
+            )
+        elif data_type.lower() == "potential":
+            write_VESTA_format_json(
+                data["AtomInfo"],
+                [
+                    data["Potential"]["TotalElectrostaticPotential"],
+                ],
+                out_filename,
+            )
+        elif data_type.lower() == "elf":
+            write_VESTA_format_json(
+                data["AtomInfo"], [data["ELF"]["TotalELF"]], out_filename
+            )
+        elif data_type.lower() == "pcharge":
+            write_VESTA_format_json(
+                data["AtomInfo"], [data["Pcharge"][0]["TotalCharge"]], out_filename
+            )
+        else:
+            raise NotImplementedError("仅支持rho/potential/elf/pcharge/boundcharge")
+
+    else:
+        raise NotImplementedError("仅支持json或h5格式文件")
+
+def write_delta_rho_vesta(AB, A, B, output="delta_rho.vesta"):
+    """电荷密度差分可视化
+
+    DeviceStudio暂不支持大文件，临时写成可以用VESTA打开的格式
+
+    Parameters
+    ----------
+    AB : str
+        AB的电荷密度文件路径，可以是h5或json格式
+    A : str
+        A的电荷密度文件路径，可以是h5或json格式
+    B : str
+        B的电荷密度文件路径，可以是h5或json格式
+    output : str
+        输出文件路径，默认 "delta_rho.vesta"
+
+    Returns
+    -------
+    output : file
+        电荷差分（AB-A-B）后的电荷密度文件，
+
+    Examples
+    --------
+    >>> from dspawpy.io.write import write_delta_rho_vesta
+    >>> write_delta_rho_vesta('AB.h5', 'A.h5', 'B.h5', 'delta_rho.vesta')
+    >>> write_delta_rho_vesta('AB.json', 'A.json', 'B.json', 'delta_rho.vesta')
+    # 甚至可以混写
+    >>> write_delta_rho_vesta('AB.h5', 'A.json', 'B.json', 'delta_rho.vesta')
+    """
+    print(f'读取{AB}...')
+    if AB.endswith(".h5"):
+        dataAB = load_h5(AB)
+        rhoAB = np.array(dataAB["/Rho/TotalCharge"])
+        nGrids = dataAB["/AtomInfo/Grid"]
+        atom_symbol = dataAB["/AtomInfo/Elements"]
+        atom_pos = dataAB["/AtomInfo/Position"]
+        latticeConstantMatrix = dataAB["/AtomInfo/Lattice"]
+        atom_pos = np.array(atom_pos).reshape(-1, 3)
+    elif AB.endswith(".json"):
+        atom_symbol = []
+        atom_pos = []
+        with open(AB, "r") as f1:
+            dataAB = json.load(f1)
+            rhoAB = np.array(dataAB["Rho"]["TotalCharge"])
+            nGrids = dataAB["AtomInfo"]["Grid"]
+        for i in range(len(dataAB["AtomInfo"]["Atoms"])):
+            atom_symbol.append(dataAB["AtomInfo"]["Atoms"][i]["Element"])
+            atom_pos.append(dataAB["AtomInfo"]["Atoms"][i]["Position"])
+            atom_pos = np.array(atom_pos)
+
+        latticeConstantMatrix = dataAB["AtomInfo"]["Lattice"]
+    else:
+        raise ValueError(f"file format must be either h5 or json: {AB}")
+
+    print(f'读取{A}...')
+    if A.endswith(".h5"):
+        dataA = load_h5(A)
+        rhoA = np.array(dataA["/Rho/TotalCharge"])
+    elif A.endswith(".json"):
+        with open(A, "r") as f2:
+            dataA = json.load(f2)
+            rhoA = np.array(dataA["Rho"]["TotalCharge"])
+    else:
+        raise ValueError(f"file format must be either h5 or json: {A}")
+
+    print(f'读取{B}...')
+    if B.endswith(".h5"):
+        dataB = load_h5(B)
+        rhoB = np.array(dataB["/Rho/TotalCharge"])
+    elif B.endswith(".json"):
+        with open(B, "r") as f3:
+            dataB = json.load(f3)
+            rhoB = np.array(dataB["Rho"]["TotalCharge"])
+    else:
+        raise ValueError(f"file format must be either h5 or json: {B}")
+
+    print(f'计算电荷差分...')
+    rho = rhoAB - rhoA - rhoB
+    rho = np.array(rho).reshape(nGrids[0], nGrids[1], nGrids[2])
+
+    element = list(set(atom_symbol))
+    element = sorted(set(atom_symbol), key=atom_symbol.index)
+    element_num = np.zeros(len(element))
+    for i in range(len(element)):
+        element_num[i] = atom_symbol.count(element[i])
+
+    latticeConstantMatrix = np.array(latticeConstantMatrix)
+    latticeConstantMatrix = latticeConstantMatrix.reshape(3, 3)
+
+    print(f'写入文件{output}...')
+    with open(output, "w") as out:
+        out.write("DS-PAW_rho\n")
+        out.write("    1.000000\n")
+        for i in range(3):
+            for j in range(3):
+                out.write("    " + str(latticeConstantMatrix[i, j]) + "    ")
+            out.write("\n")
+        for i in range(len(element)):
+            out.write("    " + element[i] + "    ")
+        out.write("\n")
+
+        for i in range(len(element_num)):
+            out.write("    " + str(int(element_num[i])) + "    ")
+        out.write("\n")
+        out.write("Direct\n")
+        for i in range(len(atom_pos)):
+            for j in range(3):
+                out.write("    " + str(atom_pos[i, j]) + "    ")
+            out.write("\n")
+        out.write("\n")
+
+        for i in range(3):
+            out.write("  " + str(nGrids[i]) + "  ")
+        out.write("\n")
+
+        ind = 0
+        for i in range(nGrids[0]):
+            for j in range(nGrids[1]):
+                for k in range(nGrids[2]):
+                    out.write("  " + str(rho[i, j, k]) + "  ")
+                    ind = ind + 1
+                    if ind % 5 == 0:
+                        out.write("\n")
+
+    print(f"成功写入 {output}")
+
+
+def write_atoms(fileobj, hdf5):
+    fileobj.write("DS-PAW Structure\n")
+    fileobj.write("  1.00\n")
+    lattice = np.asarray(hdf5["/AtomInfo/Lattice"]).reshape(-1, 1)  # 将列表lattice下的多个列表整合
+    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[0][0], lattice[1][0], lattice[2][0]))
+    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[3][0], lattice[4][0], lattice[5][0]))
+    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[6][0], lattice[7][0], lattice[8][0]))
+
+    elements = hdf5["/AtomInfo/Elements"]
+    elements_set = []
+    elements_number = {}
+    for e in elements:
+        if e in elements_set:
+            elements_number[e] = elements_number[e] + 1
+        else:
+            elements_set.append(e)
+            elements_number[e] = 1
+
+    for e in elements_set:
+        fileobj.write("  " + e)
+    fileobj.write("\n")
+
+    for e in elements_set:
+        fileobj.write("%5d" % (elements_number[e]))
+    fileobj.write("\n")
+    if hdf5["/AtomInfo/CoordinateType"][0] == "Direct":
+        fileobj.write("Direct\n")
+    else:
+        fileobj.write("Cartesian\n")
+    for i, p in enumerate(hdf5["/AtomInfo/Position"]):
+        fileobj.write("%10.6f" % p)
+        if (i + 1) % 3 == 0:
+            fileobj.write("\n")
+    fileobj.write("\n")
+
+
+def write_VESTA_format(hdf5: Dict, datakeys: list, filename):
+    with open(filename, "w") as file:
+        write_atoms(file, hdf5)
+        for key in datakeys:
+            d = np.asarray(hdf5[key]).reshape(-1, 1)  # 将列表hdf5[key]下的多个列表整合
+            file.write("%5d %5d %5d\n" % tuple(hdf5["/AtomInfo/Grid"]))
+            i = 0
+            while i < len(d):
+                for j in range(10):
+                    file.write("%10.5f " % d[i])
+                    i += 1
+                    if i >= len(d):
+                        break
+                file.write("\n")
+
+            file.write("\n")
+
+
+def write_atoms_json(fileobj, atom_info):
+    fileobj.write("DS-PAW Structure\n")
+    fileobj.write("  1.00\n")
+    lattice = atom_info["Lattice"]
+
+    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[0], lattice[1], lattice[2]))
+    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[3], lattice[4], lattice[5]))
+    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[6], lattice[7], lattice[8]))
+
+    elements = [atom["Element"] for atom in atom_info["Atoms"]]
+    elements_set = []
+    elements_number = {}
+    for e in elements:
+        if e in elements_set:
+            elements_number[e] = elements_number[e] + 1
+        else:
+            elements_set.append(e)
+            elements_number[e] = 1
+
+    for e in elements_set:
+        fileobj.write("  " + e)
+    fileobj.write("\n")
+
+    for e in elements_set:
+        fileobj.write("%5d" % (elements_number[e]))
+    fileobj.write("\n")
+    if atom_info["CoordinateType"] == "Direct":
+        fileobj.write("Direct\n")
+    else:
+        fileobj.write("Cartesian\n")
+    for atom in atom_info["Atoms"]:
+        fileobj.write("%10.6f %10.6f %10.6f\n" % tuple(atom["Position"]))
+    fileobj.write("\n")
+
+
+def write_VESTA_format_json(atom_info: Dict, data: List, filename):
+    with open(filename, "w") as file:
+        write_atoms_json(file, atom_info)
+        for d in data:
+            file.write("%5d %5d %5d\n" % tuple(atom_info["Grid"]))
+            i = 0
+            while i < len(d):
+                for j in range(10):
+                    file.write("%10.5f " % d[i])
+                    i += 1
+                    if i >= len(d):
+                        break
+                file.write("\n")
+
+            file.write("\n")
```

## dspawpy/io/write_json.py

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-# -*- coding: utf-8 -*-
-
-from typing import Dict, List
-
-
-def write_atoms(fileobj, atom_info):
-    fileobj.write("DS-PAW Structure\n")
-    fileobj.write("  1.00\n")
-    lattice = atom_info["Lattice"]
-
-    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[0], lattice[1], lattice[2]))
-    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[3], lattice[4], lattice[5]))
-    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[6], lattice[7], lattice[8]))
-
-    elements = [atom["Element"] for atom in atom_info["Atoms"]]
-    elements_set = []
-    elements_number = {}
-    for e in elements:
-        if e in elements_set:
-            elements_number[e] = elements_number[e] + 1
-        else:
-            elements_set.append(e)
-            elements_number[e] = 1
-
-    for e in elements_set:
-        fileobj.write("  " + e)
-    fileobj.write("\n")
-
-    for e in elements_set:
-        fileobj.write("%5d" % (elements_number[e]))
-    fileobj.write("\n")
-    if atom_info["CoordinateType"] == "Direct":
-        fileobj.write("Direct\n")
-    else:
-        fileobj.write("Cartesian\n")
-    for atom in atom_info["Atoms"]:
-        fileobj.write("%10.6f %10.6f %10.6f\n" % tuple(atom["Position"]))
-    fileobj.write("\n")
-
-
-def write_VESTA_format(atom_info: Dict, data: List, filename="DS-PAW.vesta"):
-    with open(filename, "w") as file:
-        write_atoms(file, atom_info)
-        for d in data:
-            file.write("%5d %5d %5d\n" % tuple(atom_info["Grid"]))
-            i = 0
-            while i < len(d):
-                for j in range(10):
-                    file.write("%10.5f " % d[i])
-                    i += 1
-                    if i >= len(d):
-                        break
-                file.write("\n")
-
-            file.write("\n")
+# -*- coding: utf-8 -*-
+
+from typing import Dict, List
+
+
+def write_atoms(fileobj, atom_info):
+    fileobj.write("DS-PAW Structure\n")
+    fileobj.write("  1.00\n")
+    lattice = atom_info["Lattice"]
+
+    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[0], lattice[1], lattice[2]))
+    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[3], lattice[4], lattice[5]))
+    fileobj.write("%10.6f %10.6f %10.6f\n" % (lattice[6], lattice[7], lattice[8]))
+
+    elements = [atom["Element"] for atom in atom_info["Atoms"]]
+    elements_set = []
+    elements_number = {}
+    for e in elements:
+        if e in elements_set:
+            elements_number[e] = elements_number[e] + 1
+        else:
+            elements_set.append(e)
+            elements_number[e] = 1
+
+    for e in elements_set:
+        fileobj.write("  " + e)
+    fileobj.write("\n")
+
+    for e in elements_set:
+        fileobj.write("%5d" % (elements_number[e]))
+    fileobj.write("\n")
+    if atom_info["CoordinateType"] == "Direct":
+        fileobj.write("Direct\n")
+    else:
+        fileobj.write("Cartesian\n")
+    for atom in atom_info["Atoms"]:
+        fileobj.write("%10.6f %10.6f %10.6f\n" % tuple(atom["Position"]))
+    fileobj.write("\n")
+
+
+def write_VESTA_format(atom_info: Dict, data: List, filename="DS-PAW.vesta"):
+    with open(filename, "w") as file:
+        write_atoms(file, atom_info)
+        for d in data:
+            file.write("%5d %5d %5d\n" % tuple(atom_info["Grid"]))
+            i = 0
+            while i < len(d):
+                for j in range(10):
+                    file.write("%10.5f " % d[i])
+                    i += 1
+                    if i >= len(d):
+                        break
+                file.write("\n")
+
+            file.write("\n")
```

