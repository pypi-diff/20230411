# Comparing `tmp/gradient_free_optimizers-1.2.6-py3-none-any.whl.zip` & `tmp/gradient_free_optimizers-1.3.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,58 +1,58 @@
-Zip file size: 103668 bytes, number of entries: 128
--rw-rw-r--  2.0 unx     1384 b- defN 23-Apr-11 05:49 gradient_free_optimizers/__init__.py
+Zip file size: 103078 bytes, number of entries: 129
+-rw-rw-r--  2.0 unx     1384 b- defN 23-Apr-11 08:41 gradient_free_optimizers/__init__.py
 -rw-rw-r--  2.0 unx     4393 b- defN 23-Apr-11 05:49 gradient_free_optimizers/high_lvl_api.py
 -rw-rw-r--  2.0 unx     1455 b- defN 23-Jan-19 10:42 gradient_free_optimizers/memory.py
 -rw-rw-r--  2.0 unx     2763 b- defN 22-Jul-26 13:24 gradient_free_optimizers/print_info.py
 -rw-rw-r--  2.0 unx     2900 b- defN 23-Jan-05 08:23 gradient_free_optimizers/progress_bar.py
 -rw-rw-r--  2.0 unx     1045 b- defN 23-Feb-28 10:35 gradient_free_optimizers/results_manager.py
--rw-rw-r--  2.0 unx     5867 b- defN 23-Apr-11 05:49 gradient_free_optimizers/search.py
+-rw-rw-r--  2.0 unx     5315 b- defN 23-Apr-11 08:41 gradient_free_optimizers/search.py
 -rw-rw-r--  2.0 unx      478 b- defN 23-Feb-28 10:35 gradient_free_optimizers/search_statistics.py
 -rw-rw-r--  2.0 unx     2263 b- defN 23-Apr-03 16:27 gradient_free_optimizers/stop_run.py
 -rw-rw-r--  2.0 unx      710 b- defN 21-Aug-23 09:01 gradient_free_optimizers/times_tracker.py
 -rw-rw-r--  2.0 unx      782 b- defN 23-Apr-03 14:58 gradient_free_optimizers/utils.py
 -rw-rw-r--  2.0 unx     1498 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/__init__.py
 -rw-rw-r--  2.0 unx      589 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/base_optimizer.py
 -rw-rw-r--  2.0 unx      207 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/core_optimizer/__init__.py
--rw-rw-r--  2.0 unx     5341 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/core_optimizer/converter.py
--rw-rw-r--  2.0 unx     2289 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py
--rw-rw-r--  2.0 unx     4142 b- defN 23-Apr-11 05:47 gradient_free_optimizers/optimizers/core_optimizer/init_positions.py
+-rw-rw-r--  2.0 unx     5728 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/core_optimizer/converter.py
+-rw-rw-r--  2.0 unx     2530 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py
+-rw-rw-r--  2.0 unx     5085 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/core_optimizer/init_positions.py
 -rw-rw-r--  2.0 unx     3758 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py
 -rw-rw-r--  2.0 unx      256 b- defN 22-Jul-08 16:20 gradient_free_optimizers/optimizers/exp_opt/__init__.py
 -rw-rw-r--  2.0 unx     2210 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/exp_opt/ensemble_optimizer.py
--rw-rw-r--  2.0 unx      967 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/exp_opt/random_annealing.py
+-rw-rw-r--  2.0 unx      966 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/exp_opt/random_annealing.py
 -rw-rw-r--  2.0 unx      567 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/global_opt/__init__.py
--rw-rw-r--  2.0 unx     4250 b- defN 23-Apr-11 05:47 gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py
+-rw-rw-r--  2.0 unx     4522 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py
 -rw-rw-r--  2.0 unx     2193 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/global_opt/lipschitz_optimization.py
--rw-rw-r--  2.0 unx     3101 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/global_opt/pattern_search.py
--rw-rw-r--  2.0 unx     3268 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/global_opt/powells_method.py
--rw-rw-r--  2.0 unx     1140 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py
+-rw-rw-r--  2.0 unx     3284 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/global_opt/pattern_search.py
+-rw-rw-r--  2.0 unx     3429 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/global_opt/powells_method.py
+-rw-rw-r--  2.0 unx     1121 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py
 -rw-rw-r--  2.0 unx      622 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/global_opt/random_search.py
 -rw-rw-r--  2.0 unx      167 b- defN 23-Feb-27 16:52 gradient_free_optimizers/optimizers/grid/__init__.py
--rw-rw-r--  2.0 unx     4023 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/grid/grid_search.py
+-rw-rw-r--  2.0 unx     4414 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/grid/grid_search.py
 -rw-rw-r--  2.0 unx      591 b- defN 21-Dec-07 17:30 gradient_free_optimizers/optimizers/local_opt/__init__.py
--rw-rw-r--  2.0 unx     5538 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py
--rw-rw-r--  2.0 unx     1944 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py
--rw-rw-r--  2.0 unx      940 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py
+-rw-rw-r--  2.0 unx     5694 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py
+-rw-rw-r--  2.0 unx     2081 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py
+-rw-rw-r--  2.0 unx      939 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py
 -rw-rw-r--  2.0 unx      971 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py
 -rw-rw-r--  2.0 unx     2097 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py
 -rw-rw-r--  2.0 unx      452 b- defN 22-Jul-26 13:24 gradient_free_optimizers/optimizers/pop_opt/__init__.py
 -rw-rw-r--  2.0 unx      309 b- defN 22-Jan-02 09:46 gradient_free_optimizers/optimizers/pop_opt/_individual.py
 -rw-rw-r--  2.0 unx     1478 b- defN 22-Oct-08 18:45 gradient_free_optimizers/optimizers/pop_opt/_particle.py
 -rw-rw-r--  2.0 unx     1546 b- defN 22-Oct-08 18:45 gradient_free_optimizers/optimizers/pop_opt/_spiral.py
--rw-rw-r--  2.0 unx     2590 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py
--rw-rw-r--  2.0 unx     2821 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py
--rw-rw-r--  2.0 unx     2468 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py
--rw-rw-r--  2.0 unx     1997 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py
--rw-rw-r--  2.0 unx     2209 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py
+-rw-rw-r--  2.0 unx     2644 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py
+-rw-rw-r--  2.0 unx     3013 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py
+-rw-rw-r--  2.0 unx     2401 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py
+-rw-rw-r--  2.0 unx     2149 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py
+-rw-rw-r--  2.0 unx     2377 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py
 -rw-rw-r--  2.0 unx      359 b- defN 21-Dec-07 17:30 gradient_free_optimizers/optimizers/smb_opt/__init__.py
 -rw-rw-r--  2.0 unx     1540 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/smb_opt/bayesian_optimization.py
 -rw-rw-r--  2.0 unx     2013 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/smb_opt/forest_optimizer.py
 -rw-rw-r--  2.0 unx     2614 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/smb_opt/sampling.py
--rw-rw-r--  2.0 unx     4784 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/smb_opt/smbo.py
+-rw-rw-r--  2.0 unx     5087 b- defN 23-Apr-11 08:41 gradient_free_optimizers/optimizers/smb_opt/smbo.py
 -rw-rw-r--  2.0 unx     3710 b- defN 22-Jan-31 13:47 gradient_free_optimizers/optimizers/smb_opt/surrogate_models.py
 -rw-rw-r--  2.0 unx     2201 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/smb_opt/tree_structured_parzen_estimators.py
 -rw-rw-r--  2.0 unx      177 b- defN 22-Jul-08 16:20 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/__init__.py
 -rw-rw-r--  2.0 unx     1206 b- defN 22-Jul-08 16:20 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/expected_improvement.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 11:38 tests/__init__.py
 -rw-rw-r--  2.0 unx     1716 b- defN 21-Dec-07 17:30 tests/_test_debug.py
 -rw-rw-r--  2.0 unx      686 b- defN 23-Apr-03 14:58 tests/_test_examples.py
@@ -67,29 +67,30 @@
 -rw-rw-r--  2.0 unx     1283 b- defN 21-Jan-14 11:38 tests/test_verbosity.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 11:38 tests/test_optimizers/__init__.py
 -rw-rw-r--  2.0 unx     5431 b- defN 23-Feb-28 10:46 tests/test_optimizers/_parametrize.py
 -rw-rw-r--  2.0 unx      938 b- defN 23-Apr-03 14:58 tests/test_optimizers/_test_max_time.py
 -rw-rw-r--  2.0 unx     1735 b- defN 23-Apr-03 14:58 tests/test_optimizers/_test_memory_warm_start.py
 -rw-rw-r--  2.0 unx     1475 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_backend_api.py
 -rw-rw-r--  2.0 unx     1881 b- defN 22-Jul-26 13:24 tests/test_optimizers/test_best_results.py
+-rw-rw-r--  2.0 unx     3195 b- defN 23-Apr-11 08:41 tests/test_optimizers/test_constr_opt.py
 -rw-rw-r--  2.0 unx     6818 b- defN 22-Oct-19 17:34 tests/test_optimizers/test_early_stop.py
 -rw-rw-r--  2.0 unx     1839 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_exploration.py
 -rw-rw-r--  2.0 unx     1775 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_inf_nan.py
 -rw-rw-r--  2.0 unx     3050 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_initializers.py
 -rw-rw-r--  2.0 unx     1547 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_large_search_space.py
 -rw-rw-r--  2.0 unx     1744 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_max_score.py
 -rw-rw-r--  2.0 unx     5610 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_multiple_searches.py
 -rw-rw-r--  2.0 unx      370 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_names.py
 -rw-rw-r--  2.0 unx     1624 b- defN 23-Feb-28 10:35 tests/test_optimizers/test_opt_algos_simple.py
 -rw-rw-r--  2.0 unx     1794 b- defN 23-Feb-28 10:35 tests/test_optimizers/test_random_seed.py
 -rw-rw-r--  2.0 unx     3918 b- defN 23-Feb-28 10:35 tests/test_optimizers/test_random_state.py
 -rw-rw-r--  2.0 unx     1511 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_results.py
 -rw-rw-r--  2.0 unx      632 b- defN 22-Oct-22 07:41 tests/test_optimizers/test_search_space_.py
 -rw-rw-r--  2.0 unx      704 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_search_step.py
--rw-rw-r--  2.0 unx     1560 b- defN 22-Oct-08 18:45 tests/test_optimizers/test_search_tracker.py
+-rw-rw-r--  2.0 unx     1560 b- defN 23-Apr-11 08:25 tests/test_optimizers/test_search_tracker.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/__init__.py
 -rw-rw-r--  2.0 unx      598 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/_base_para_test.py
 -rw-rw-r--  2.0 unx     3400 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_parameter/test_bayesian_optimizer_para_init.py
 -rw-rw-r--  2.0 unx     1397 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_evolution_strategy_para_init.py
 -rw-rw-r--  2.0 unx     2818 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_parameter/test_forest_optimizer_para_init.py
 -rw-rw-r--  2.0 unx      852 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_hill_climbing_para_init.py
 -rw-rw-r--  2.0 unx     1237 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_parallel_tempering_para_init.py
@@ -118,13 +119,13 @@
 -rw-rw-r--  2.0 unx     2757 b- defN 23-Jan-22 06:24 tests/test_parameters/test_parameter_init/test_tpe_para_init.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Nov-30 12:42 tests/test_performance/__init__.py
 -rw-rw-r--  2.0 unx     1101 b- defN 22-Nov-11 16:15 tests/test_performance/test_global_opt.py
 -rw-rw-r--  2.0 unx     2397 b- defN 22-Nov-11 16:14 tests/test_performance/test_grid_search.py
 -rw-rw-r--  2.0 unx     1155 b- defN 21-Dec-07 17:30 tests/test_performance/test_local_opt.py
 -rw-rw-r--  2.0 unx     1909 b- defN 23-Feb-28 10:46 tests/test_performance/test_pop_opt.py
 -rw-rw-r--  2.0 unx     1882 b- defN 23-Feb-28 10:46 tests/test_performance/test_smb_opt.py
--rw-rw-r--  2.0 unx     1069 b- defN 23-Apr-11 06:01 gradient_free_optimizers-1.2.6.dist-info/LICENSE
--rw-rw-r--  2.0 unx    34934 b- defN 23-Apr-11 06:01 gradient_free_optimizers-1.2.6.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-11 06:01 gradient_free_optimizers-1.2.6.dist-info/WHEEL
--rw-rw-r--  2.0 unx       31 b- defN 23-Apr-11 06:01 gradient_free_optimizers-1.2.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    14015 b- defN 23-Apr-11 06:01 gradient_free_optimizers-1.2.6.dist-info/RECORD
-128 files, 292468 bytes uncompressed, 80258 bytes compressed:  72.6%
+-rw-rw-r--  2.0 unx     1069 b- defN 23-Apr-11 08:46 gradient_free_optimizers-1.3.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx    26572 b- defN 23-Apr-11 08:46 gradient_free_optimizers-1.3.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-11 08:46 gradient_free_optimizers-1.3.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       31 b- defN 23-Apr-11 08:46 gradient_free_optimizers-1.3.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    14112 b- defN 23-Apr-11 08:46 gradient_free_optimizers-1.3.0.dist-info/RECORD
+129 files, 290498 bytes uncompressed, 79512 bytes compressed:  72.6%
```

## zipnote {}

```diff
@@ -210,14 +210,17 @@
 
 Filename: tests/test_optimizers/test_backend_api.py
 Comment: 
 
 Filename: tests/test_optimizers/test_best_results.py
 Comment: 
 
+Filename: tests/test_optimizers/test_constr_opt.py
+Comment: 
+
 Filename: tests/test_optimizers/test_early_stop.py
 Comment: 
 
 Filename: tests/test_optimizers/test_exploration.py
 Comment: 
 
 Filename: tests/test_optimizers/test_inf_nan.py
@@ -363,23 +366,23 @@
 
 Filename: tests/test_performance/test_pop_opt.py
 Comment: 
 
 Filename: tests/test_performance/test_smb_opt.py
 Comment: 
 
-Filename: gradient_free_optimizers-1.2.6.dist-info/LICENSE
+Filename: gradient_free_optimizers-1.3.0.dist-info/LICENSE
 Comment: 
 
-Filename: gradient_free_optimizers-1.2.6.dist-info/METADATA
+Filename: gradient_free_optimizers-1.3.0.dist-info/METADATA
 Comment: 
 
-Filename: gradient_free_optimizers-1.2.6.dist-info/WHEEL
+Filename: gradient_free_optimizers-1.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: gradient_free_optimizers-1.2.6.dist-info/top_level.txt
+Filename: gradient_free_optimizers-1.3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: gradient_free_optimizers-1.2.6.dist-info/RECORD
+Filename: gradient_free_optimizers-1.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## gradient_free_optimizers/__init__.py

```diff
@@ -1,12 +1,12 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
-__version__ = "1.2.6"
+__version__ = "1.3.0"
 __license__ = "MIT"
 
 from .high_lvl_api import (
     HillClimbingOptimizer,
     StochasticHillClimbingOptimizer,
     RepulsingHillClimbingOptimizer,
     SimulatedAnnealingOptimizer,
```

## gradient_free_optimizers/search.py

```diff
@@ -188,23 +188,7 @@
             self._initialization()
 
         if self.nth_iter == self.n_init_search:
             self.finish_initialization()
 
         if self.n_init_search <= self.nth_iter < self.n_iter:
             self._iteration()
-
-        """
-        # loop to initialize N positions
-        for nth_iter in range(self.n_inits_norm):
-            if self.stop.check(self.start_time, self.p_bar.score_best, self.score_l):
-                break
-            self._initialization(nth_iter)
-
-        self.finish_initialization()
-
-        # loop to do the iterations
-        for nth_iter in range(self.n_init_search, self.n_iter):
-            if self.stop.check(self.start_time, self.p_bar.score_best, self.score_l):
-                break
-            self._iteration(nth_iter)
-        """
```

## gradient_free_optimizers/optimizers/core_optimizer/converter.py

```diff
@@ -6,17 +6,22 @@
 import pandas as pd
 
 from functools import reduce
 from typing import Optional
 
 
 class Converter:
-    def __init__(self, search_space: dict) -> None:
+    def __init__(self, search_space: dict, constraints: list = None) -> None:
         self.n_dimensions = len(search_space)
         self.search_space = search_space
+        if constraints is None:
+            self.constraints = []
+        else:
+            self.constraints = constraints
+
         self.para_names = list(search_space.keys())
 
         dim_sizes_list = [len(array) for array in search_space.values()]
         self.dim_sizes = np.array(dim_sizes_list)
 
         # product of list
         self.search_space_size = reduce((lambda x, y: x * y), dim_sizes_list)
@@ -31,14 +36,22 @@
                 [np.arange(len(array)) for array in search_space.values()],
             )
         )
 
         self.max_positions = self.dim_sizes - 1
         self.search_space_values = list(search_space.values())
 
+    def not_in_constraint(self, position):
+        para = self.value2para(self.position2value(position))
+
+        for constraint in self.constraints:
+            if not constraint(para):
+                return False
+        return True
+
     def returnNoneIfArgNone(func_):
         def wrapper(self, *args):
             for arg in [*args]:
                 if arg is None:
                     return None
             else:
                 return func_(self, *args)
```

## gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py

```diff
@@ -9,33 +9,38 @@
 from ...utils import set_random_seed, move_random
 
 
 class CoreOptimizer(SearchTracker):
     def __init__(
         self,
         search_space,
-        initialize: dict = {"grid": 4, "random": 2, "vertices": 4},
+        initialize={"grid": 4, "random": 2, "vertices": 4},
+        constraints=None,
         random_state=None,
         rand_rest_p=0,
         nth_process=None,
         debug_log=False,
     ):
         super().__init__()
 
         self.random_seed = set_random_seed(nth_process, random_state)
 
-        self.conv = Converter(search_space)
+        self.conv = Converter(search_space, constraints)
         self.init = Initializer(self.conv, initialize)
 
         self.initialize = initialize
+        self.constraints = constraints
         self.random_state = random_state
         self.rand_rest_p = rand_rest_p
         self.nth_process = nth_process
         self.debug_log = debug_log
 
+        if self.constraints is None:
+            self.constraints = []
+
         self.nth_init = 0
         self.nth_trial = 0
         self.search_state = "init"
 
     def random_iteration(func):
         def wrapper(self, *args, **kwargs):
             if self.rand_rest_p > random.uniform(0, 1):
@@ -58,15 +63,18 @@
 
         if dist > threshold:
             return self.move_random()
 
         return pos
 
     def move_random(self):
-        return move_random(self.conv.search_space_positions)
+        while True:
+            pos = move_random(self.conv.search_space_positions)
+            if self.conv.not_in_constraint(pos):
+                return pos
 
     @SearchTracker.track_new_pos
     def init_pos(self):
         init_pos = self.init.init_positions_l[self.nth_init]
         return init_pos
 
     @SearchTracker.track_new_score
```

## gradient_free_optimizers/optimizers/core_optimizer/init_positions.py

```diff
@@ -9,14 +9,24 @@
 
 
 class Initializer:
     def __init__(self, conv, initialize):
         self.conv = conv
         self.initialize = initialize
 
+        self.n_inits = 0
+        if "random" in initialize:
+            self.n_inits += initialize["random"]
+        if "grid" in initialize:
+            self.n_inits += initialize["grid"]
+        if "vertices" in initialize:
+            self.n_inits += initialize["vertices"]
+        if "warm_start" in initialize:
+            self.n_inits += len(initialize["warm_start"])
+
         self.init_positions_l = None
 
         self.set_pos()
 
     def move_random(self):
         return move_random(self.conv.search_space_positions)
 
@@ -51,39 +61,47 @@
         if "warm_start" in self.initialize:
             positions = self._init_warm_start(self.initialize["warm_start"])
             init_positions_ll.append(positions)
 
         self.init_positions_l = [
             item for sublist in init_positions_ll for item in sublist
         ]
-        self.n_inits = len(self.init_positions_l)
+        self.init_positions_l = self._fill_rest_random(self.init_positions_l)
 
     def _init_warm_start(self, value_list):
         positions = []
 
         for value_ in value_list:
             pos = self.conv.value2position(list(value_.values()))
             positions.append(pos)
 
-        return positions
+        positions_constr = []
+        for pos in positions:
+            if self.conv.not_in_constraint(pos):
+                positions_constr.append(pos)
+
+        return positions_constr
 
     def _init_random_search(self, n_pos):
         positions = []
 
         if n_pos == 0:
             return positions
 
         for nth_pos in range(n_pos):
-            pos = move_random(self.conv.search_space_positions)
-            positions.append(pos)
+            while True:
+                pos = move_random(self.conv.search_space_positions)
+                if self.conv.not_in_constraint(pos):
+                    positions.append(pos)
+                    break
 
         return positions
 
-    def _fill_rest_random(self, n_pos, positions):
-        diff_pos = n_pos - len(positions)
+    def _fill_rest_random(self, positions):
+        diff_pos = self.n_inits - len(positions)
         if diff_pos > 0:
             pos_rnd = self._init_random_search(n_pos=diff_pos)
 
             return positions + pos_rnd
         else:
             return positions
 
@@ -104,16 +122,20 @@
                 n_points = [n * dim_dist for n in range(1, p_per_dim + 1)]
 
                 positions.append(n_points)
 
             pos_mesh = np.array(np.meshgrid(*positions))
             positions = list(pos_mesh.T.reshape(-1, n_dim))
 
-        positions = self._fill_rest_random(n_pos, positions)
-        return positions
+        positions_constr = []
+        for pos in positions:
+            if self.conv.not_in_constraint(pos):
+                positions_constr.append(pos)
+
+        return positions_constr
 
     def _get_random_vertex(self):
         vertex = []
         for dim_positions in self.conv.search_space_positions:
             rnd = random.randint(0, 1)
 
             if rnd == 0:
@@ -134,8 +156,13 @@
                 if not vert_in_list:
                     positions.append(vertex)
                     break
             else:
                 pos = move_random(self.conv.search_space_positions)
                 positions.append(pos)
 
-        return positions
+        positions_constr = []
+        for pos in positions:
+            if self.conv.not_in_constraint(pos):
+                positions_constr.append(pos)
+
+        return positions_constr
```

## gradient_free_optimizers/optimizers/exp_opt/random_annealing.py

```diff
@@ -27,11 +27,11 @@
         self.annealing_rate = annealing_rate
         self.start_temp = start_temp
         self.temp = start_temp
 
     @HillClimbingOptimizer.track_new_pos
     @HillClimbingOptimizer.random_iteration
     def iterate(self):
-        pos = self._move_climb(self.pos_current, epsilon_mod=self.temp)
+        pos = self.move_climb(self.pos_current, epsilon_mod=self.temp)
         self.temp = self.temp * self.annealing_rate
 
         return pos
```

## gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py

```diff
@@ -121,23 +121,30 @@
         subspace = SubSpace(self.conv.pos_space)
         self.subspace_l.append(subspace)
         self.search_state = "iter"
 
     @SMBO.track_new_pos
     @SMBO.track_X_sample
     def iterate(self):
-        self.current_subspace = self.select_next_subspace()
-        if self.current_subspace:
-            return self.current_subspace.center_pos
-
-        else:
-            self.current_subspace = self.select_subspace()
-            self.split_dim_into_n(self.current_subspace)
+        while True:
+            self.current_subspace = self.select_next_subspace()
+            if self.current_subspace:
+                pos = self.current_subspace.center_pos
+                if self.conv.not_in_constraint(pos):
+                    return pos
 
-            return self.subspace_l[-1].center_pos
+            else:
+                self.current_subspace = self.select_subspace()
+                self.split_dim_into_n(self.current_subspace)
+
+                pos = self.subspace_l[-1].center_pos
+                if self.conv.not_in_constraint(pos):
+                    return pos
+
+            return self.move_climb(pos, epsilon_mod=0.3)
 
     @SMBO.track_new_score
     def evaluate(self, score_new):
         if self.pos_best is None:
             self.pos_best = self.pos_new
             self.pos_current = self.pos_new
```

## gradient_free_optimizers/optimizers/global_opt/pattern_search.py

```diff
@@ -1,24 +1,24 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 import random
 import numpy as np
 
-from ..base_optimizer import BaseOptimizer
+from ..local_opt import HillClimbingOptimizer
 
 
 def max_list_idx(list_):
     max_item = max(list_)
     max_item_idx = [i for i, j in enumerate(list_) if j == max_item]
     return max_item_idx[-1:][0]
 
 
-class PatternSearch(BaseOptimizer):
+class PatternSearch(HillClimbingOptimizer):
     name = "Pattern Search"
     _name_ = "pattern_search"
     __name__ = "PatternSearch"
 
     optimizer_type = "global"
     computationally_expensive = False
 
@@ -61,29 +61,32 @@
             pos_pattern_n = self.conv2pos(pos_pattern_n)
 
             pattern_pos_l.append(pos_pattern_p)
             pattern_pos_l.append(pos_pattern_n)
 
         self.pattern_pos_l = list(random.sample(pattern_pos_l, self.n_positions_))
 
-    @BaseOptimizer.track_new_pos
-    @BaseOptimizer.random_iteration
+    @HillClimbingOptimizer.track_new_pos
+    @HillClimbingOptimizer.random_iteration
     def iterate(self):
-        pos_new = self.pattern_pos_l[0]
-        self.pattern_pos_l.pop(0)
-
-        return pos_new
+        while True:
+            pos_new = self.pattern_pos_l[0]
+            self.pattern_pos_l.pop(0)
+
+            if self.conv.not_in_constraint(pos_new):
+                return pos_new
+            return self.move_climb(pos_new)
 
     def finish_initialization(self):
         self.generate_pattern(self.pos_current)
         self.search_state = "iter"
 
-    @BaseOptimizer.track_new_score
+    @HillClimbingOptimizer.track_new_score
     def evaluate(self, score_new):
-        BaseOptimizer.evaluate(self, score_new)
+        super(HillClimbingOptimizer, self).evaluate(score_new)
         if len(self.scores_valid) == 0:
             return
 
         modZero = self.nth_trial % int(self.n_positions_ * 2) == 0
 
         if modZero or len(self.pattern_pos_l) == 0:
             if self.search_state == "iter":
```

## gradient_free_optimizers/optimizers/global_opt/powells_method.py

```diff
@@ -1,25 +1,24 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 import numpy as np
 from collections import OrderedDict
 
-from ..base_optimizer import BaseOptimizer
 from ..local_opt import HillClimbingOptimizer
 
 
 def sort_list_idx(list_):
     list_np = np.array(list_)
     idx_sorted = list(list_np.argsort()[::-1])
     return idx_sorted
 
 
-class PowellsMethod(BaseOptimizer):
+class PowellsMethod(HillClimbingOptimizer):
     name = "Powell's Method"
     _name_ = "powells_method"
     __name__ = "PowellsMethod"
 
     optimizer_type = "global"
     computationally_expensive = False
 
@@ -69,19 +68,20 @@
                 min_pos.append(self.powells_pos[idx])
                 max_pos.append(self.powells_pos[idx])
                 center_pos.append(self.powells_pos[idx])
 
         self.init_positions_ = [min_pos, center_pos, max_pos]
 
         self.hill_climb = HillClimbingOptimizer(
-            search_space=search_space_1D, initialize={"random": 5}
+            search_space=search_space_1D,
+            initialize={"random": 5},
         )
 
-    @BaseOptimizer.track_new_pos
-    @BaseOptimizer.random_iteration
+    @HillClimbingOptimizer.track_new_pos
+    @HillClimbingOptimizer.random_iteration
     def iterate(self):
         self.nth_iter_ += 1
         self.nth_iter_current_dim += 1
 
         modZero = self.nth_iter_ % self.iters_p_dim == 0
         # nonZero = self.nth_iter_ != 0
 
@@ -91,17 +91,20 @@
         if self.nth_iter_current_dim < 5:
             pos_new = self.hill_climb.init_pos()
             pos_new = self.hill_climb.conv.position2value(pos_new)
 
         else:
             pos_new = self.hill_climb.iterate()
             pos_new = self.hill_climb.conv.position2value(pos_new)
+        pos_new = np.array(pos_new)
 
-        return pos_new
+        if self.conv.not_in_constraint(pos_new):
+            return pos_new
+        return self.move_climb(pos_new)
 
-    @BaseOptimizer.track_new_score
+    @HillClimbingOptimizer.track_new_score
     def evaluate(self, score_new):
         if self.current_search_dim == -1:
-            BaseOptimizer.evaluate(self, score_new)
+            super(HillClimbingOptimizer, self).evaluate(score_new)
         else:
             self.hill_climb.evaluate(score_new)
-            BaseOptimizer.evaluate(self, score_new)
+            super(HillClimbingOptimizer, self).evaluate(score_new)
```

## gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py

```diff
@@ -32,12 +32,10 @@
     @HillClimbingOptimizer.track_new_pos
     @HillClimbingOptimizer.random_iteration
     def iterate(self):
         notZero = self.nth_trial != 0
         modZero = self.nth_trial % self.n_iter_restart == 0
 
         if notZero and modZero:
-            pos = self.move_random()
+            return self.move_random()
         else:
-            pos = self._move_climb(self.pos_current)
-
-        return pos
+            return self.move_climb(self.pos_current)
```

## gradient_free_optimizers/optimizers/grid/grid_search.py

```diff
@@ -63,46 +63,56 @@
         # The coordinate of our new position for each dimension is
         # the quotient of the pointer by the product of remaining dimensions
         # Describes a bijection from Z/search_space_size*Z -> (Z/dim_1*Z)x...x(Z/dim_n*Z)
         for dim in range(len(dim_sizes) - 1):
             new_pos.append(pointer // np.prod(dim_sizes[dim + 1 :]) % dim_sizes[dim])
             pointer = pointer % np.prod(dim_sizes[dim + 1 :])
         new_pos.append(pointer)
+
         return np.array(new_pos)
 
     @BaseOptimizer.track_new_pos
     def iterate(self):
-        # If this is the first iteration:
-        # Generate the direction and return initial_position
-        if self.direction is None:
-            self.direction = self.get_direction()
-            return self.initial_position
-
-        # If this is not the first iteration:
-        # Update high_dim_pointer by taking a step of size step_size * direction.
-
-        # Multiple passes are needed in order to observe the entire search space
-        # depending on the step_size parameter.
-        _, current_pass = self.nth_trial, self.high_dim_pointer % self.step_size
-        current_pass_finished = (
-            (self.nth_trial + 1) * self.step_size // self.conv.search_space_size
-            > self.nth_trial * self.step_size // self.conv.search_space_size
-        )
-        # Begin the next pass if current is finished.
-        if current_pass_finished:
-            self.high_dim_pointer = current_pass + 1
-        else:
-            # Otherwise update pointer in Z/(search_space_size*Z)
-            # using the prime step direction and step_size.
-            self.high_dim_pointer = (
-                self.high_dim_pointer + self.step_size * self.direction
-            ) % self.conv.search_space_size
-
-        # Compute corresponding position in our search space.
-
-        pos_new = self.grid_move()
-        pos_new = self.conv2pos(pos_new)
-        return pos_new
+        # while loop for constraint opt
+        while True:
+            # If this is the first iteration:
+            # Generate the direction and return initial_position
+            if self.direction is None:
+                self.direction = self.get_direction()
+
+                pos_new = self.initial_position
+                if self.conv.not_in_constraint(pos_new):
+                    return pos_new
+                else:
+                    return self.move_random()
+
+            # If this is not the first iteration:
+            # Update high_dim_pointer by taking a step of size step_size * direction.
+
+            # Multiple passes are needed in order to observe the entire search space
+            # depending on the step_size parameter.
+            _, current_pass = self.nth_trial, self.high_dim_pointer % self.step_size
+            current_pass_finished = (
+                (self.nth_trial + 1) * self.step_size // self.conv.search_space_size
+                > self.nth_trial * self.step_size // self.conv.search_space_size
+            )
+            # Begin the next pass if current is finished.
+            if current_pass_finished:
+                self.high_dim_pointer = current_pass + 1
+            else:
+                # Otherwise update pointer in Z/(search_space_size*Z)
+                # using the prime step direction and step_size.
+                self.high_dim_pointer = (
+                    self.high_dim_pointer + self.step_size * self.direction
+                ) % self.conv.search_space_size
+
+            # Compute corresponding position in our search space.
+
+            pos_new = self.grid_move()
+            pos_new = self.conv2pos(pos_new)
+
+            if self.conv.not_in_constraint(pos_new):
+                return pos_new
 
     @BaseOptimizer.track_new_score
     def evaluate(self, score_new):
         BaseOptimizer.evaluate(self, score_new)
```

## gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py

```diff
@@ -2,16 +2,15 @@
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
 import random
 import numpy as np
 
-from ..base_optimizer import BaseOptimizer
-from ...search import Search
+from .hill_climbing_optimizer import HillClimbingOptimizer
 
 
 def sort_list_idx(list_):
     list_np = np.array(list_)
     idx_sorted = list(list_np.argsort()[::-1])
     return idx_sorted
 
@@ -26,15 +25,15 @@
 
         center_dim_mean = np.array(center_dim_pos).mean()
         centeroid.append(center_dim_mean)
 
     return centeroid
 
 
-class DownhillSimplexOptimizer(BaseOptimizer, Search):
+class DownhillSimplexOptimizer(HillClimbingOptimizer):
     name = "Downhill Simplex"
     _name_ = "downhill_simplex"
     __name__ = "DownhillSimplexOptimizer"
 
     optimizer_type = "local"
     computationally_expensive = False
 
@@ -64,15 +63,15 @@
 
         self.i_x_0 = 0
         self.i_x_N_1 = -2
         self.i_x_N = -1
 
         self.search_state = "iter"
 
-    @BaseOptimizer.track_new_pos
+    @HillClimbingOptimizer.track_new_pos
     def iterate(self):
         simplex_stale = all(
             [np.array_equal(self.simplex_pos[0], array) for array in self.simplex_pos]
         )
 
         if simplex_stale:
             idx_sorted = sort_list_idx(self.scores_valid)
@@ -88,40 +87,45 @@
 
             self.center_array = centeroid(self.simplex_pos[:-1])
 
             r_pos = self.center_array + self.alpha * (
                 self.center_array - self.simplex_pos[-1]
             )
             self.r_pos = self.conv2pos(r_pos)
-            return self.r_pos
+            pos_new = self.r_pos
 
         elif self.simplex_step == 2:
             e_pos = self.center_array + self.gamma * (
                 self.center_array - self.simplex_pos[-1]
             )
             self.e_pos = self.conv2pos(e_pos)
             self.simplex_step = 1
 
-            return self.e_pos
+            pos_new = self.e_pos
 
         elif self.simplex_step == 3:
             # iter Contraction
             c_pos = self.h_pos + self.beta * (self.center_array - self.h_pos)
             c_pos = self.conv2pos(c_pos)
 
-            return c_pos
+            pos_new = c_pos
 
         elif self.simplex_step == 4:
             # iter Shrink
             pos = self.simplex_pos[self.compress_idx]
             pos = pos + self.sigma * (self.simplex_pos[0] - pos)
 
-            return self.conv2pos(pos)
+            pos_new = self.conv2pos(pos)
 
-    @BaseOptimizer.track_new_score
+        if self.conv.not_in_constraint(pos_new):
+            return pos_new
+
+        return HillClimbingOptimizer.move_climb(self, pos_new)
+
+    @HillClimbingOptimizer.track_new_score
     def evaluate(self, score_new):
         if self.simplex_step != 0:
             self.prev_pos = self.positions_valid[-1]
 
         if self.simplex_step == 1:
             # self.r_pos = self.prev_pos
             self.r_score = score_new
```

## gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py

```diff
@@ -34,24 +34,28 @@
         self, *args, epsilon=0.03, distribution="normal", n_neighbours=3, **kwargs
     ):
         super().__init__(*args, **kwargs)
         self.epsilon = epsilon
         self.distribution = distribution
         self.n_neighbours = n_neighbours
 
-    def _move_climb(self, pos, epsilon_mod=1):
-        sigma = self.conv.max_positions * self.epsilon * epsilon_mod
-        pos_normal = dist_dict[self.distribution](pos, sigma, pos.shape)
+    def move_climb(self, pos, epsilon_mod=1):
+        while True:
+            sigma = self.conv.max_positions * self.epsilon * epsilon_mod
+            pos_normal = dist_dict[self.distribution](pos, sigma, pos.shape)
+            pos = self.conv2pos(pos_normal)
 
-        return self.conv2pos(pos_normal)
+            if self.conv.not_in_constraint(pos):
+                return pos
+            epsilon_mod *= 1.01
 
     @BaseOptimizer.track_new_pos
     @BaseOptimizer.random_iteration
     def iterate(self):
-        return self._move_climb(self.pos_current)
+        return self.move_climb(self.pos_current)
 
     @BaseOptimizer.track_new_score
     def evaluate(self, score_new):
         BaseOptimizer.evaluate(self, score_new)
         if len(self.scores_valid) == 0:
             return
```

## gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py

```diff
@@ -19,15 +19,15 @@
 
         self.tabus = []
         self.repulsion_factor = repulsion_factor
         self.epsilon_mod = 1
 
     @HillClimbingOptimizer.track_new_pos
     def iterate(self):
-        return self._move_climb(self.pos_current, epsilon_mod=self.epsilon_mod)
+        return self.move_climb(self.pos_current, epsilon_mod=self.epsilon_mod)
 
     def evaluate(self, score_new):
         super().evaluate(score_new)
 
         if score_new <= self.score_current:
             self.epsilon_mod = self.repulsion_factor
         else:
```

## gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py

```diff
@@ -71,14 +71,15 @@
                 init_paras = self.conv.values2paras(init_values)
 
                 population.append(
                     Optimizer(
                         self.conv.search_space,
                         rand_rest_p=self.rand_rest_p,
                         initialize={"warm_start": init_paras},
+                        constraints=self.constraints,
                     )
                 )
         else:
             population = self.population
 
         return population
```

## gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py

```diff
@@ -38,33 +38,37 @@
             add_choice = np.random.randint(n_arrays, size=size - 2).astype(bool)
             choice += list(add_choice)
 
         cross_array = np.choose(choice, array_list)
         return cross_array
 
     def _cross(self):
-        if len(self.individuals) > 2:
-            rnd_int2 = random.choice(
-                [i for i in range(0, self.n_ind - 1) if i not in [self.rnd_int]]
-            )
-        else:
-            rnd_int2 = random.choice(
-                [i for i in range(0, self.n_ind) if i not in [self.rnd_int]]
-            )
-
-        p_sec = self.pop_sorted[rnd_int2]
-        p_worst = self.pop_sorted[-1]
+        while True:
+            if len(self.individuals) > 2:
+                rnd_int2 = random.choice(
+                    [i for i in range(0, self.n_ind - 1) if i not in [self.rnd_int]]
+                )
+            else:
+                rnd_int2 = random.choice(
+                    [i for i in range(0, self.n_ind) if i not in [self.rnd_int]]
+                )
+
+            p_sec = self.pop_sorted[rnd_int2]
+            p_worst = self.pop_sorted[-1]
+
+            two_best_pos = [self.p_current.pos_current, p_sec.pos_current]
+            pos_new = self._random_cross(two_best_pos)
 
-        two_best_pos = [self.p_current.pos_current, p_sec.pos_current]
-        pos_new = self._random_cross(two_best_pos)
+            self.p_current = p_worst
+            p_worst.pos_new = pos_new
 
-        self.p_current = p_worst
-        p_worst.pos_new = pos_new
+            if self.conv.not_in_constraint(pos_new):
+                return pos_new
 
-        return pos_new
+            return self.p_current.move_climb(pos_new)
 
     @BasePopulationOptimizer.track_new_pos
     def init_pos(self):
         nth_pop = self.nth_trial % len(self.individuals)
 
         self.p_current = self.individuals[nth_pop]
         return self.p_current.init_pos()
```

## gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py

```diff
@@ -55,26 +55,21 @@
 
             temp = (1 / _p1_.temp) - (1 / _p2_.temp)
             return np.exp(score_diff_norm * temp) * 100
 
     @BasePopulationOptimizer.track_new_pos
     def init_pos(self):
         nth_pop = self.nth_trial % len(self.systems)
-        print("\n nth_pop", nth_pop)
-
         self.p_current = self.systems[nth_pop]
 
         return self.p_current.init_pos()
 
     @BasePopulationOptimizer.track_new_pos
     def iterate(self):
-        print("  iterate")
-
         self.p_current = self.systems[self.nth_trial % len(self.systems)]
-
         return self.p_current.iterate()
 
     @BasePopulationOptimizer.track_new_score
     def evaluate(self, score_new):
         notZero = self.n_iter_swap != 0
         modZero = self.nth_trial % self.n_iter_swap == 0
```

## gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py

```diff
@@ -52,18 +52,22 @@
 
         self.p_current.velo = np.zeros(len(self.conv.max_positions))
 
         return self.p_current.init_pos()
 
     @BasePopulationOptimizer.track_new_pos
     def iterate(self):
-        self.p_current = self.particles[self.nth_trial % len(self.particles)]
+        while True:
+            self.p_current = self.particles[self.nth_trial % len(self.particles)]
 
-        self.sort_pop_best_score()
-        self.p_current.global_pos_best = self.pop_sorted[0].pos_best
+            self.sort_pop_best_score()
+            self.p_current.global_pos_best = self.pop_sorted[0].pos_best
 
-        pos_new = self.p_current.move_linear()
-        return pos_new
+            pos_new = self.p_current.move_linear()
+
+            if self.conv.not_in_constraint(pos_new):
+                return pos_new
+            return self.p_current.move_climb(pos_new)
 
     @BasePopulationOptimizer.track_new_score
     def evaluate(self, score_new):
         self.p_current.evaluate(score_new)
```

## gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py

```diff
@@ -52,20 +52,25 @@
         self.center_pos = self.pop_sorted[0].pos_current
         self.center_score = self.pop_sorted[0].score_current
 
         self.search_state = "iter"
 
     @BasePopulationOptimizer.track_new_pos
     def iterate(self):
-        self.p_current = self.particles[self.nth_trial % len(self.particles)]
+        while True:
+            self.p_current = self.particles[self.nth_trial % len(self.particles)]
 
-        self.sort_pop_best_score()
-        self.p_current.global_pos_best = self.pop_sorted[0].pos_current
+            self.sort_pop_best_score()
+            self.p_current.global_pos_best = self.pop_sorted[0].pos_current
 
-        return self.p_current.move_spiral(self.center_pos)
+            pos_new = self.p_current.move_spiral(self.center_pos)
+
+            if self.conv.not_in_constraint(pos_new):
+                return pos_new
+            return self.p_current.iterate()
 
     @BasePopulationOptimizer.track_new_score
     def evaluate(self, score_new):
         if self.search_state == "iter":
             if self.pop_sorted[0].score_current > self.center_score:
                 self.center_pos = self.pop_sorted[0].pos_current
                 self.center_score = self.pop_sorted[0].score_current
```

## gradient_free_optimizers/optimizers/smb_opt/smbo.py

```diff
@@ -1,21 +1,21 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
-from ..base_optimizer import BaseOptimizer
+from ..local_opt import HillClimbingOptimizer
 from .sampling import InitialSampler
 
 import numpy as np
 
 np.seterr(divide="ignore", invalid="ignore")
 
 
-class SMBO(BaseOptimizer):
+class SMBO(HillClimbingOptimizer):
     def __init__(
         self,
         *args,
         warm_start_smbo=None,
         max_sample_size=10000000,
         sampling={"random": 1000000},
         **kwargs
@@ -96,15 +96,23 @@
             _idx_sample = np.random.choice(n_pos_comb, n_samples, replace=False)
             pos_comb_sampled = pos_comb[_idx_sample, :]
             return pos_comb_sampled
 
     def _all_possible_pos(self):
         pos_space = self.sampler.get_pos_space()
         n_dim = len(pos_space)
-        return np.array(np.meshgrid(*pos_space)).T.reshape(-1, n_dim)
+        all_pos_comb = np.array(np.meshgrid(*pos_space)).T.reshape(-1, n_dim)
+
+        all_pos_comb_constr = []
+        for pos in all_pos_comb:
+            if self.conv.not_in_constraint(pos):
+                all_pos_comb_constr.append(pos)
+
+        all_pos_comb_constr = np.array(all_pos_comb_constr)
+        return all_pos_comb_constr
 
     def memory_warning(self, max_sample_size):
         if (
             self.conv.search_space_size > self.warnings
             and max_sample_size > self.warnings
         ):
             warning_message0 = "\n Warning:"
@@ -116,26 +124,26 @@
             warning_message3 = "\n Reduce search space size for better performance."
             print(warning_message0 + warning_message1 + warning_message3)
 
     @track_X_sample
     def init_pos(self):
         return super().init_pos()
 
-    @BaseOptimizer.track_new_pos
+    @HillClimbingOptimizer.track_new_pos
     @track_X_sample
     def iterate(self):
         return self._propose_location()
 
-    @BaseOptimizer.track_new_score
+    @HillClimbingOptimizer.track_new_score
     @track_y_sample
     def evaluate(self, score_new):
         self._evaluate_new2current(score_new)
         self._evaluate_current2best()
 
-    @BaseOptimizer.track_new_score
+    @HillClimbingOptimizer.track_new_score
     @track_y_sample
     def evaluate_init(self, score_new):
         self._evaluate_new2current(score_new)
         self._evaluate_current2best()
 
     def _propose_location(self):
         try:
```

## Comparing `gradient_free_optimizers-1.2.6.dist-info/LICENSE` & `gradient_free_optimizers-1.3.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `gradient_free_optimizers-1.2.6.dist-info/METADATA` & `gradient_free_optimizers-1.3.0.dist-info/METADATA`

 * *Files 24% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: gradient-free-optimizers
-Version: 1.2.6
+Version: 1.3.0
 Summary: 
 Home-page: https://github.com/SimonBlanke/Gradient-Free-Optimizers
 Author: Simon Blanke
 Author-email: simon.blanke@yahoo.com
 License: MIT
 Keywords: optimization
 Classifier: Programming Language :: Python :: 3
@@ -115,15 +115,15 @@
 ---
 
 <div align="center"><a name="menu"></a>
   <h3>
     <a href="https://github.com/SimonBlanke/Gradient-Free-Optimizers#optimization-algorithms">Optimization algorithms</a> •
     <a href="https://github.com/SimonBlanke/Gradient-Free-Optimizers#installation">Installation</a> •
     <a href="https://github.com/SimonBlanke/Gradient-Free-Optimizers#examples">Examples</a> •
-    <a href="https://github.com/SimonBlanke/Gradient-Free-Optimizers#basic-api-reference">API reference</a> •
+    <a href="https://simonblanke.github.io/gradient-free-optimizers-documentation">API reference</a> •
     <a href="https://github.com/SimonBlanke/Gradient-Free-Optimizers#roadmap">Roadmap</a>
   </h3>
 </div>
 
 ---
 
 
@@ -848,265 +848,14 @@
 ```
 
 </details>
 
 
 <br>
 
-## Basic API reference
-
-The API reference can also be found in the [official documentation](https://simonblanke.github.io/gradient-free-optimizers-documentation).
-
-
-### General optimization arguments
-
-The following (general) arguments can be passed to any optimization class:
-
-- search_space
-  - Pass the search_space to the optimizer class to define the space were the optimization algorithm can search for the best parameters for the given objective function.
-
-    example:
-    ```python
-    ...
-    
-    search_space = {
-      "x1": numpy.arange(-10, 31, 0.3),
-      "x2": numpy.arange(-10, 31, 0.3),
-    }
-    
-    opt = HillClimbingOptimizer(search_space)
-    
-    ...
-    ```
-
-- initialize={"grid": 8, "vertices": 8, "random": 4, "warm_start": []}
-  - (dict, None)
-
-  - The initialization dictionary automatically determines a number of parameters that will be evaluated in the first n iterations (n is the sum of the values in initialize). The initialize keywords are the following:
-    - grid
-      - Initializes positions in a grid like pattern. Positions that cannot be put into a grid are randomly positioned.
-    - vertices
-      - Initializes positions at the vertices of the search space. Positions that cannot be put into a vertices are randomly positioned.
-
-    - random
-      - Number of random initialized positions
-
-    - warm_start
-      - List of parameter dictionaries that marks additional start points for the optimization run.
-
-
-- random_state=None
-    - (int, None)
-    - Random state for random processes in the random, numpy and scipy module.
-
-
-
-### Optimizer Classes
-
-Each optimization class needs the "search_space" as an input argument. Optionally "initialize" and optimizer-specific parameters can be passed as well. You can read more about each optimization-algorithm and its parameters in the [Optimization Tutorial](https://github.com/SimonBlanke/optimization-tutorial).
-
-- HillClimbingOptimizer
-- StochasticHillClimbingOptimizer
-- RepulsingHillClimbingOptimizer
-- SimulatedAnnealingOptimizer
-- DownhillSimplexOptimizer
-- RandomSearchOptimizer
-- GridSearchOptimizer
-- RandomRestartHillClimbingOptimizer
-- RandomAnnealingOptimizer
-- PowellsMethod
-- PatternSearch
-- ParallelTemperingOptimizer
-- ParticleSwarmOptimizer
-- SpiralOptimization
-- EvolutionStrategyOptimizer
-- LipschitzOptimizer
-- DirectAlgorithm
-- BayesianOptimizer
-- TreeStructuredParzenEstimators
-- ForestOptimizer
-
-
-
-<br>
-
-<details>
-<summary><b> .search(...)</b></summary>
-
-- objective_function
-  - (callable)
-
-  - The objective function defines the optimization problem. The optimization algorithm will try to maximize the numerical value that is returned by the objective function by trying out different parameters from the search space.
-
-    example:
-    ```python
-    def objective_function(para):
-        score = -(para["x1"] * para["x1"] + para["x2"] * para["x2"])
-        return score
-    ```
-
-- n_iter 
-  - (int)
-
-  - The number of iterations that will be performed during the optimiation run. The entire iteration consists of the optimization-step, which decides the next parameter that will be evaluated and the evaluation-step, which will run the objective function with the chosen parameter and return the score.
-
-- max_time=None
-  - (float, None)
-  - Maximum number of seconds until the optimization stops. The time will be checked after each completed iteration.
-
-- max_score=None
-  - (float, None)
-  - Maximum score until the optimization stops. The score will be checked after each completed iteration.
-
-
-- early_stopping=None
-  - (dict, None)
-  - Stops the optimization run early if it did not achive any score-improvement within the last iterations. The early_stopping-parameter enables to set three parameters:
-    - `n_iter_no_change`: Non-optional int-parameter. This marks the last n iterations to look for an improvement over the iterations that came before n. If the best score of the entire run is within those last n iterations the run will continue (until other stopping criteria are met), otherwise the run will stop.
-    - `tol_abs`: Optional float-paramter. The score must have improved at least this absolute tolerance in the last n iterations over the best score in the iterations before n. This is an absolute value, so 0.1 means an imporvement of 0.8 -> 0.9 is acceptable but 0.81 -> 0.9 would stop the run.
-    - `tol_rel`: Optional float-paramter. The score must have imporved at least this relative tolerance (in percentage) in the last n iterations over the best score in the iterations before n. This is a relative value, so 10 means an imporvement of 0.8 -> 0.88 is acceptable but 0.8 -> 0.87 would stop the run.
-
-  
-
-- memory=True
-  - (bool)
-  - Whether or not to use the "memory"-feature. The memory is a dictionary, which gets filled with parameters and scores during the optimization run. If the optimizer encounters a parameter that is already in the dictionary it just extracts the score instead of reevaluating the objective function (which can take a long time).
-
-
-- memory_warm_start=None
-  - (pandas dataframe, None)
-  - Pandas dataframe that contains score and paramter information that will be automatically loaded into the memory-dictionary.
-
-      example:
-
-      <table class="table">
-        <thead class="table-head">
-          <tr class="row">
-            <td class="cell">score</td>
-            <td class="cell">x1</td>
-            <td class="cell">x2</td>
-            <td class="cell">x...</td>
-          </tr>
-        </thead>
-        <tbody class="table-body">
-          <tr class="row">
-            <td class="cell">0.756</td>
-            <td class="cell">0.1</td>
-            <td class="cell">0.2</td>
-            <td class="cell">...</td>
-          </tr>
-          <tr class="row">
-            <td class="cell">0.823</td>
-            <td class="cell">0.3</td>
-            <td class="cell">0.1</td>
-            <td class="cell">...</td>
-          </tr>
-          <tr class="row">
-            <td class="cell">...</td>
-            <td class="cell">...</td>
-            <td class="cell">...</td>
-            <td class="cell">...</td>
-          </tr>
-          <tr class="row">
-            <td class="cell">...</td>
-            <td class="cell">...</td>
-            <td class="cell">...</td>
-            <td class="cell">...</td>
-          </tr>
-        </tbody>
-      </table>
-
-
-
-- verbosity=[
-          "progress_bar",
-          "print_results",
-          "print_times"
-      ]
-  - (list, False)
-  - The verbosity list determines what part of the optimization information will be printed in the command line.
-
-
-</details>
-
-<br>
-
-<details>
-<summary><b> Results from attributes </b></summary>
-
-
-- .search_data
-  - Dataframe containing information about the score and the value of each parameter. Each row shows the information of one optimization iteration.
-
-    example:
-
-    <table class="table">
-      <thead class="table-head">
-        <tr class="row">
-          <td class="cell">score</td>
-          <td class="cell">x1</td>
-          <td class="cell">x2</td>
-          <td class="cell">x...</td>
-        </tr>
-      </thead>
-      <tbody class="table-body">
-        <tr class="row">
-          <td class="cell">0.756</td>
-          <td class="cell">0.1</td>
-          <td class="cell">0.2</td>
-          <td class="cell">...</td>
-        </tr>
-        <tr class="row">
-          <td class="cell">0.823</td>
-          <td class="cell">0.3</td>
-          <td class="cell">0.1</td>
-          <td class="cell">...</td>
-        </tr>
-        <tr class="row">
-          <td class="cell">...</td>
-          <td class="cell">...</td>
-          <td class="cell">...</td>
-          <td class="cell">...</td>
-        </tr>
-        <tr class="row">
-          <td class="cell">...</td>
-          <td class="cell">...</td>
-          <td class="cell">...</td>
-          <td class="cell">...</td>
-        </tr>
-      </tbody>
-    </table>
-
-- .best_score
-  - numerical value of the best score, that was found during the optimization run.
-
-- .best_para
-  - parameter dictionary of the best score, that was found during the optimization run.
-
-    example:
-    ```python
-    {
-      'x1': 0.2, 
-      'x2': 0.3,
-    }
-    ```
-      
-- .eval_times
-  - List of evaluation times (time of objective function evaluation) collected during the optimization run.
-
-- .iter_times
-  - List of iteration times (evaluation + optimization) collected during the optimization run.
-
-
-
-</details>
-
-
-<br>
-
 ## Roadmap
 
 
 <details>
 <summary><b>v0.3.0</b> :heavy_check_mark:</summary>
 
   - [x] add sampling parameter to Bayesian optimizer
@@ -1164,44 +913,44 @@
 
 </details>
 
 
 <details>
 <summary><b>v1.3.0</b> </summary>
 
-  - [ ] add API, testing and doc to (better) use GFO as backend-optimization package
+  - [ ] add support for constrained optimization
 
 </details>
 
 <details>
 <summary><b>v1.4.0</b> </summary>
 
-  - [ ] add support for constrained optimization
+  - [ ] add API, testing and doc to (better) use GFO as backend-optimization package
+  - [ ] add Grid search paraneter that changes direction of search
+  - [ ] add Random search parameter that enables to avoid replacement of the sampling
+  - [ ] add SMBO parameter that enables to avoid replacement of the sampling
 
 </details>
 
 <details>
 <summary><b>v1.5.0</b> </summary>
 
   - [ ] add Ant-colony optimization
-  - [ ] add Grid search paraneter that changes direction of search
-  - [ ] add Random search parameter that enables to avoid replacement of the sampling
 
 </details>
 
 <details>
 <summary><b>v2.0.0</b> </summary>
 
   - [ ] add other acquisition functions to smbo (Probability of improvement, Entropy search, ...)
   - [ ] ...
 
 </details>
 
 
-
 <br>
 
 ## Gradient Free Optimizers <=> Hyperactive
 
 Gradient-Free-Optimizers was created as the optimization backend of the [Hyperactive package](https://github.com/SimonBlanke/Hyperactive). Therefore the algorithms are exactly the same in both packages and deliver the same results. 
 However you can still use Gradient-Free-Optimizers as a standalone package.
 The separation of Gradient-Free-Optimizers from Hyperactive enables multiple advantages:
```

## Comparing `gradient_free_optimizers-1.2.6.dist-info/RECORD` & `gradient_free_optimizers-1.3.0.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,57 +1,57 @@
-gradient_free_optimizers/__init__.py,sha256=rdb93u4mPUh0t8zTLpl52aamq871a1m7RMGRMDEUz7Q,1384
+gradient_free_optimizers/__init__.py,sha256=1wuD77k1O2eOjewUOo1guDCmgr4piRONQxZ7NMeToaI,1384
 gradient_free_optimizers/high_lvl_api.py,sha256=GjuOoxUQtqA076XsPzvmr3mhtKhMuo4EgP4aNetPzp0,4393
 gradient_free_optimizers/memory.py,sha256=ep60wwNMSTbUioCV0MAajj69P68hqmOTzWwAN0uVJqA,1455
 gradient_free_optimizers/print_info.py,sha256=I2b6hg5sAmyLhfCXZYPt69tX_NTsbP6o-JA7FnGn2MA,2763
 gradient_free_optimizers/progress_bar.py,sha256=fDnZ5CyNoykn2qnCvgY-ySl08Pup0sOI_ID6gWsjBB4,2900
 gradient_free_optimizers/results_manager.py,sha256=vQNaVLLWpfGlrfkCoESW64ZdDUX8DvJUdhHek_YCaro,1045
-gradient_free_optimizers/search.py,sha256=uPy4NsAofByOekPq2EPRythUTUUX4ysuwF4kH-ZaolA,5867
+gradient_free_optimizers/search.py,sha256=3sDWgHIxfnYGaLETr_QC832MHEFr4HUqfBLlqhJM6YM,5315
 gradient_free_optimizers/search_statistics.py,sha256=AcwdPH1kFnD5-OtAmlbwaXfLV-fi6EzFhk3uz3mXlns,478
 gradient_free_optimizers/stop_run.py,sha256=4u4DA4ilpzGL9L6NN2ky0QxwrLw03EwHguGIvGFLao8,2263
 gradient_free_optimizers/times_tracker.py,sha256=jRB1xOgd2898Ty9OIGoG_a6oJ7gGW53s4GnmYZvp7as,710
 gradient_free_optimizers/utils.py,sha256=X2M6tJ-hJacQqAHcQstdI28jkB0hhUwyFA_w7h8j2pw,782
 gradient_free_optimizers/optimizers/__init__.py,sha256=lXsfj6shH7lY1DJSaJwiqdYg3chaC9OxOMI7DYXabzU,1498
 gradient_free_optimizers/optimizers/base_optimizer.py,sha256=7--DnGQeSCVJ71LNfFpbxOHobqd4VJqCi_5YtjfNwkY,589
 gradient_free_optimizers/optimizers/core_optimizer/__init__.py,sha256=FIDA8-LbTyFBE5hStaafJqpzK7cyrdvBRoJIbEFoe38,207
-gradient_free_optimizers/optimizers/core_optimizer/converter.py,sha256=SS62tFcfNtubbKwC3Iprd0Yku3bjz_spoye30gheNw0,5341
-gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py,sha256=-RaW3n5txxKNJXeGC8LTDzWGCCd3VBfTFe-g-OnoTxw,2289
-gradient_free_optimizers/optimizers/core_optimizer/init_positions.py,sha256=TM6Dw00ZKMvLC-g8SFZ47O9sycVDKsTiDrsw3zipqh0,4142
+gradient_free_optimizers/optimizers/core_optimizer/converter.py,sha256=0HbWniCRsGgBLYtZvXTIAE046DHnRWTNP_KyK59laAs,5728
+gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py,sha256=mXu0zBGF8yZJm-M2ju9eDJ5JdcbXaXMOb3XnMIMyzwg,2530
+gradient_free_optimizers/optimizers/core_optimizer/init_positions.py,sha256=emNFgNEt88ycxSMwKIpaBlVvnPW-dcuO8JfaRO9784g,5085
 gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py,sha256=aR_fy6J3jE2Ox29XNLfwmQ6dku8o36nPuFWZouYIEgU,3758
 gradient_free_optimizers/optimizers/exp_opt/__init__.py,sha256=LPWvfKVliMKw2_jQy1FHV-eyZM5MjbpKuL-q3JTuzQ0,256
 gradient_free_optimizers/optimizers/exp_opt/ensemble_optimizer.py,sha256=tRQJjUuumu-aYCMo_0Ulp2qI7Z-iWUq4jHh7T0HKT3Q,2210
-gradient_free_optimizers/optimizers/exp_opt/random_annealing.py,sha256=6GJ6cU9YfdOyHe2bNiuaeuzkFNRR_gl4SdPUrYEDE_M,967
+gradient_free_optimizers/optimizers/exp_opt/random_annealing.py,sha256=yruRTFVczw4pD0r5C5LiDbdm5wo25rdQW6evtw7c7fM,966
 gradient_free_optimizers/optimizers/global_opt/__init__.py,sha256=kkL9j8TfPGzBufPKcbPHAQ53ju6HTs3J8PZdsaeqFl0,567
-gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py,sha256=lLwgpDXc5e0feDfAhzD4FKnRJl4yhqC72J6RJL9QR94,4250
+gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py,sha256=SBb45uolwSQziOtMEBGbe8CebfM44twdyECbZGHZS0s,4522
 gradient_free_optimizers/optimizers/global_opt/lipschitz_optimization.py,sha256=6XbCEe5v47TXtzq4b1RPNLQPCpCRrM2ghSCle8ri7ag,2193
-gradient_free_optimizers/optimizers/global_opt/pattern_search.py,sha256=1aLwk4gdR8E5Gs5ZYMT-hiu723AewRgWOsFN93-fRy0,3101
-gradient_free_optimizers/optimizers/global_opt/powells_method.py,sha256=FyplDfdGMK3S6cxKLVDxeUfHNUXPHoc9VKdVSzqiu7s,3268
-gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py,sha256=JXaCmvwk8vNzrOnkEe1yODpp32br74baRFALDT8v4sA,1140
+gradient_free_optimizers/optimizers/global_opt/pattern_search.py,sha256=GqIyC5lmkwhxqhcCf3qO30DsQBu-6D5JSy8wrLv532k,3284
+gradient_free_optimizers/optimizers/global_opt/powells_method.py,sha256=5xBtf0XIKs5rWA8UoSbKJuk7ehZtvLcPLadiuEQcp5w,3429
+gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py,sha256=b9X11YdC7Ao-m2oAVFFy_9PYMgau_eFQqXdr7KtrhUg,1121
 gradient_free_optimizers/optimizers/global_opt/random_search.py,sha256=Ct5TBDGGVq7wuSN6ZWPOzrHnPssuP2Y-1NiqtUgZNP8,622
 gradient_free_optimizers/optimizers/grid/__init__.py,sha256=81XQ8K4ij1HYv3oH8egbwmbi1YMeH4Ej2hrDwkzeCRo,167
-gradient_free_optimizers/optimizers/grid/grid_search.py,sha256=07xC_Vl2ddvbBlvsE81GjWVwE8A_WoholtYPMwoem9w,4023
+gradient_free_optimizers/optimizers/grid/grid_search.py,sha256=k2xXIDNHMz1QoX22PEgwMrcuNeqYttMEV7JcE9hZksM,4414
 gradient_free_optimizers/optimizers/local_opt/__init__.py,sha256=mbXmx0NFu50CSQotpqcGvFcV2j0_tjdEExPFuoTBNGo,591
-gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py,sha256=JvxTpw0TId3lNmSGTp4lM4Msi3-GwQuwnPfm9hMm_Tc,5538
-gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py,sha256=TBGELaIrnOgnZIDjuebvp-RJKfHYwwaLVn1aM4c33GE,1944
-gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py,sha256=mOe7DgB04n_OkrL1EQcGjZWWep--sRZu7W_t_oINX7c,940
+gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py,sha256=HifSeTKbOs1NJt044ksTyzSFC7Vog7YqmE9EAsgc1fw,5694
+gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py,sha256=CXuED1WeEduyaOqsRBVIU1CNLegSIEbVuVtIpaaY1ts,2081
+gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py,sha256=VyXimCdmukVXccdX2t7XuD8TV-uVKWvYtT-Q3_n5fZc,939
 gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py,sha256=eGlObRWMag8jJeoMzw13Vp10u6UyOPRdoavR1Mr4DqM,971
 gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py,sha256=FqyLOscIRzJbqkS0mjg-6n8JQn36YNFySy_FVbDXzKI,2097
 gradient_free_optimizers/optimizers/pop_opt/__init__.py,sha256=bNJlXHMb0ziYQLrvHIwfWRYdc0mTN_d8WWJc2LjDhTs,452
 gradient_free_optimizers/optimizers/pop_opt/_individual.py,sha256=jNG11NWoDpHI4x7h5Y5_CV_D0EckqkyqoPjf9MvzD6I,309
 gradient_free_optimizers/optimizers/pop_opt/_particle.py,sha256=f5iMsAXZXHK41EDAHdAp87VDh42BFL55mPe_evqLeUo,1478
 gradient_free_optimizers/optimizers/pop_opt/_spiral.py,sha256=J7Wai0jXaUnuLpSE0AvW2xJztjtXMuAcD0pBs6D0pLk,1546
-gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py,sha256=3qTUyihylnzXfG2kb8rXnrc62t3dA4GM6asG6Zp25zM,2590
-gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py,sha256=DRBEU2Yi0jpO0HaTp_jtgDrt3VN2dcg9HmmaKaGxN7c,2821
-gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py,sha256=JkPKsArH5nc9cfw00TP_AhpfWcgJBEDtz51dxt83sls,2468
-gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py,sha256=aENEFW_GWDGrQlJcTPkHwSLn9bAYuqH2qe8jk76bNo8,1997
-gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py,sha256=b2LuFpVNW7inJmhKQX_zDBxJykY_4NqGOt4wti-FICw,2209
+gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py,sha256=y3FO6xQ7zsP27b7lRDwv8tVEYZoq_oSp0BH3j63TJ8I,2644
+gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py,sha256=qse7FUMcQT5R9XRr26BqkVD3BQfjMdagz0DeuVyavHY,3013
+gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py,sha256=p7b-gnyW8l62vGSn-NGlYlmNWxracfsv8GbiHq9_EJg,2401
+gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py,sha256=7enLGKk50dqAsGMQeF1E_Px_grwGYwVpP665_0EXzQo,2149
+gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py,sha256=8h8ISUPg4hGczbCrQIjjJqnh7I8yCnTSGgnvhtHEjDQ,2377
 gradient_free_optimizers/optimizers/smb_opt/__init__.py,sha256=ITLZ8DIKx0ZxLT_YpEKYO28TQYXcrUHQYzgDXUDbsgo,359
 gradient_free_optimizers/optimizers/smb_opt/bayesian_optimization.py,sha256=0yr0NODLJsm88gX2UA1hi0qHWwE-xJsrpuBm-Bl34BM,1540
 gradient_free_optimizers/optimizers/smb_opt/forest_optimizer.py,sha256=tz6bavnIROEUXRCWBFJcLuec3uJzULuf3Nm1476nsJ8,2013
 gradient_free_optimizers/optimizers/smb_opt/sampling.py,sha256=1VV5mHL8mnqJSYyBi-AffZZMiLJVGjUg_b-O3RL-9XE,2614
-gradient_free_optimizers/optimizers/smb_opt/smbo.py,sha256=El7x6jg0smS6y0jDXJXrypv3szOA_jwwTo0eKhjVCBw,4784
+gradient_free_optimizers/optimizers/smb_opt/smbo.py,sha256=rcIFCx4G7ZZJ-hxa0oHvybVILY4mm0SisXp8-NobdDI,5087
 gradient_free_optimizers/optimizers/smb_opt/surrogate_models.py,sha256=ynQE209wwP7kEGnUk5-Zio7zo4rnVXCfU1y62zf2t8I,3710
 gradient_free_optimizers/optimizers/smb_opt/tree_structured_parzen_estimators.py,sha256=jfeLSje8XpJX0v3xVPLXl0XCKMjSCcrr1N-5ze6CaW0,2201
 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/__init__.py,sha256=RfxV3A9aunpUJLu4AEnyE452OBaD1d5eRshKvTbxotI,177
 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/expected_improvement.py,sha256=Y6Zx8ZpfgDkwoYiHSntK5kv8_kQ5-HWOmKcn68EmhWs,1206
 tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/_test_debug.py,sha256=1485GY-WaPX7AVhqpS0eBst-yKvHiqU1asVP8Q2aPc0,1716
 tests/_test_examples.py,sha256=7QRuve6FWx7oiw39NU2Lzx1eAA3cN_Hn2POBDYpl2ig,686
@@ -66,14 +66,15 @@
 tests/test_verbosity.py,sha256=jVZ18KCqn6tnYjoV3IZoe25k3vLjKcvabgmLCozkpgM,1283
 tests/test_optimizers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/test_optimizers/_parametrize.py,sha256=ZvG0uQHSC_6DwWB52LwZea8l0336IKdorJEjfsbJGi0,5431
 tests/test_optimizers/_test_max_time.py,sha256=xoSOrHSosrI4iSpqYuLscrxJtx81-04OhEMlmFEffB8,938
 tests/test_optimizers/_test_memory_warm_start.py,sha256=IYo7b7sOhTTcOBx_6z9nAFcIicPVT2Wu4bImNmA6B7I,1735
 tests/test_optimizers/test_backend_api.py,sha256=sWTaLWIZ4Z3Db49A5NMH3q6MGocQYCbGwVmUDrdBXXk,1475
 tests/test_optimizers/test_best_results.py,sha256=T7I9j0onwWkIWDX6n38CXtL_RW8Qi2Sx0655SvnV-vU,1881
+tests/test_optimizers/test_constr_opt.py,sha256=efybU5uV2CgUxoLSS-AedQD3n5b-wjXtLqgDJuWi45U,3195
 tests/test_optimizers/test_early_stop.py,sha256=GFrLve1iJI-k5lrXSisjHlqFoEFtn3diArNsK4eeLTo,6818
 tests/test_optimizers/test_exploration.py,sha256=A69x23HH0LyySF0hW3BbiRlqnUObkgD7L5Jm2sRS2hY,1839
 tests/test_optimizers/test_inf_nan.py,sha256=QctRomsGDLaZGHvrA7hggbwiqYjDZsQ0PwiA_rCfJeQ,1775
 tests/test_optimizers/test_initializers.py,sha256=hMp_5PtVBtTTHfKm4CuxEy28QMjBnSxvaQZB6kNxgW0,3050
 tests/test_optimizers/test_large_search_space.py,sha256=H0I8aVQZ7_-wrZpLnVdO0E6sAId-qCqpnKKqGkPlImw,1547
 tests/test_optimizers/test_max_score.py,sha256=C-ZFQqCUKJuvq-7TkyG5UfvBGwZTm6mlTSBHDcwTTvw,1744
 tests/test_optimizers/test_multiple_searches.py,sha256=ao1oRqwvXVDXI7F6OfBcTkb_1Qw-grMe5XFQOkuTPwQ,5610
@@ -117,12 +118,12 @@
 tests/test_parameters/test_parameter_init/test_tpe_para_init.py,sha256=bGY4yVqdkBVNKh5LvWW_KANo3uGIvj7YbsHSTiQxKlw,2757
 tests/test_performance/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/test_performance/test_global_opt.py,sha256=AfZSULL_Xd6ujhdCJlE_kN0BwVa7OFCUzu0rIUkXMx4,1101
 tests/test_performance/test_grid_search.py,sha256=KanTgPTI7XKWY5tUFSFA0jO6vmWCD6UlLm2gjDkJMPM,2397
 tests/test_performance/test_local_opt.py,sha256=Ntob_sYLr5ibivJTPuvm5sw8neWTLK9iJYk3jICKHHg,1155
 tests/test_performance/test_pop_opt.py,sha256=humLFMMbvZ_mJJLN3rUyop07MTp65Cmm4BEvKCReVNY,1909
 tests/test_performance/test_smb_opt.py,sha256=nMeWlSXMT5kBL1rlxeBza0vlvWOOWj0loFyiYHm81GA,1882
-gradient_free_optimizers-1.2.6.dist-info/LICENSE,sha256=RsfzUEwfUDjkq2Uk3ATEXNtAOa2WqG-6zBP_S5ERJ5I,1069
-gradient_free_optimizers-1.2.6.dist-info/METADATA,sha256=TqQy0xCgmyp4iwdkamwsmKzQkBc5fvsAnA4DcpK4lY8,34934
-gradient_free_optimizers-1.2.6.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-gradient_free_optimizers-1.2.6.dist-info/top_level.txt,sha256=vvSJQaQ3s7mZx_5ZoYAS3sBswwsRh2xhcx1vmlwhjrQ,31
-gradient_free_optimizers-1.2.6.dist-info/RECORD,,
+gradient_free_optimizers-1.3.0.dist-info/LICENSE,sha256=RsfzUEwfUDjkq2Uk3ATEXNtAOa2WqG-6zBP_S5ERJ5I,1069
+gradient_free_optimizers-1.3.0.dist-info/METADATA,sha256=tlNEEMb5jSWqDiu2GsARMAwtzUGnq5xTSt4dkGOzKg4,26572
+gradient_free_optimizers-1.3.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+gradient_free_optimizers-1.3.0.dist-info/top_level.txt,sha256=vvSJQaQ3s7mZx_5ZoYAS3sBswwsRh2xhcx1vmlwhjrQ,31
+gradient_free_optimizers-1.3.0.dist-info/RECORD,,
```

