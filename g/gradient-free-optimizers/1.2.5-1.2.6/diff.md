# Comparing `tmp/gradient_free_optimizers-1.2.5-py3-none-any.whl.zip` & `tmp/gradient_free_optimizers-1.2.6-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,108 +1,109 @@
-Zip file size: 102501 bytes, number of entries: 127
--rw-rw-r--  2.0 unx     1382 b- defN 23-Mar-24 16:23 gradient_free_optimizers/__init__.py
+Zip file size: 103668 bytes, number of entries: 128
+-rw-rw-r--  2.0 unx     1384 b- defN 23-Apr-11 05:49 gradient_free_optimizers/__init__.py
+-rw-rw-r--  2.0 unx     4393 b- defN 23-Apr-11 05:49 gradient_free_optimizers/high_lvl_api.py
 -rw-rw-r--  2.0 unx     1455 b- defN 23-Jan-19 10:42 gradient_free_optimizers/memory.py
 -rw-rw-r--  2.0 unx     2763 b- defN 22-Jul-26 13:24 gradient_free_optimizers/print_info.py
 -rw-rw-r--  2.0 unx     2900 b- defN 23-Jan-05 08:23 gradient_free_optimizers/progress_bar.py
 -rw-rw-r--  2.0 unx     1045 b- defN 23-Feb-28 10:35 gradient_free_optimizers/results_manager.py
--rw-rw-r--  2.0 unx     5901 b- defN 23-Feb-28 10:46 gradient_free_optimizers/search.py
+-rw-rw-r--  2.0 unx     5867 b- defN 23-Apr-11 05:49 gradient_free_optimizers/search.py
 -rw-rw-r--  2.0 unx      478 b- defN 23-Feb-28 10:35 gradient_free_optimizers/search_statistics.py
--rw-rw-r--  2.0 unx     2263 b- defN 23-Feb-28 10:46 gradient_free_optimizers/stop_run.py
+-rw-rw-r--  2.0 unx     2263 b- defN 23-Apr-03 16:27 gradient_free_optimizers/stop_run.py
 -rw-rw-r--  2.0 unx      710 b- defN 21-Aug-23 09:01 gradient_free_optimizers/times_tracker.py
--rw-rw-r--  2.0 unx      782 b- defN 23-Mar-24 16:23 gradient_free_optimizers/utils.py
+-rw-rw-r--  2.0 unx      782 b- defN 23-Apr-03 14:58 gradient_free_optimizers/utils.py
 -rw-rw-r--  2.0 unx     1498 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/__init__.py
--rw-rw-r--  2.0 unx      618 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/base_optimizer.py
+-rw-rw-r--  2.0 unx      589 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/base_optimizer.py
 -rw-rw-r--  2.0 unx      207 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/core_optimizer/__init__.py
--rw-rw-r--  2.0 unx     5148 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/core_optimizer/converter.py
--rw-rw-r--  2.0 unx     2343 b- defN 23-Mar-24 15:43 gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py
--rw-rw-r--  2.0 unx     4142 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/core_optimizer/init_positions.py
--rw-rw-r--  2.0 unx     3695 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py
+-rw-rw-r--  2.0 unx     5341 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/core_optimizer/converter.py
+-rw-rw-r--  2.0 unx     2289 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py
+-rw-rw-r--  2.0 unx     4142 b- defN 23-Apr-11 05:47 gradient_free_optimizers/optimizers/core_optimizer/init_positions.py
+-rw-rw-r--  2.0 unx     3758 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py
 -rw-rw-r--  2.0 unx      256 b- defN 22-Jul-08 16:20 gradient_free_optimizers/optimizers/exp_opt/__init__.py
 -rw-rw-r--  2.0 unx     2210 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/exp_opt/ensemble_optimizer.py
--rw-rw-r--  2.0 unx     1004 b- defN 22-Oct-08 18:45 gradient_free_optimizers/optimizers/exp_opt/random_annealing.py
+-rw-rw-r--  2.0 unx      967 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/exp_opt/random_annealing.py
 -rw-rw-r--  2.0 unx      567 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/global_opt/__init__.py
--rw-rw-r--  2.0 unx     4250 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py
--rw-rw-r--  2.0 unx     2193 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/global_opt/lipschitz_optimization.py
--rw-rw-r--  2.0 unx     3138 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/global_opt/pattern_search.py
--rw-rw-r--  2.0 unx     3305 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/global_opt/powells_method.py
--rw-rw-r--  2.0 unx     1175 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py
--rw-rw-r--  2.0 unx      659 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/global_opt/random_search.py
+-rw-rw-r--  2.0 unx     4250 b- defN 23-Apr-11 05:47 gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py
+-rw-rw-r--  2.0 unx     2193 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/global_opt/lipschitz_optimization.py
+-rw-rw-r--  2.0 unx     3101 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/global_opt/pattern_search.py
+-rw-rw-r--  2.0 unx     3268 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/global_opt/powells_method.py
+-rw-rw-r--  2.0 unx     1140 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py
+-rw-rw-r--  2.0 unx      622 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/global_opt/random_search.py
 -rw-rw-r--  2.0 unx      167 b- defN 23-Feb-27 16:52 gradient_free_optimizers/optimizers/grid/__init__.py
--rw-rw-r--  2.0 unx     4057 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/grid/grid_search.py
+-rw-rw-r--  2.0 unx     4023 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/grid/grid_search.py
 -rw-rw-r--  2.0 unx      591 b- defN 21-Dec-07 17:30 gradient_free_optimizers/optimizers/local_opt/__init__.py
--rw-rw-r--  2.0 unx     5537 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py
--rw-rw-r--  2.0 unx     1980 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py
--rw-rw-r--  2.0 unx      977 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py
--rw-rw-r--  2.0 unx     1008 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py
--rw-rw-r--  2.0 unx     2134 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py
+-rw-rw-r--  2.0 unx     5538 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py
+-rw-rw-r--  2.0 unx     1944 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py
+-rw-rw-r--  2.0 unx      940 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py
+-rw-rw-r--  2.0 unx      971 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py
+-rw-rw-r--  2.0 unx     2097 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py
 -rw-rw-r--  2.0 unx      452 b- defN 22-Jul-26 13:24 gradient_free_optimizers/optimizers/pop_opt/__init__.py
 -rw-rw-r--  2.0 unx      309 b- defN 22-Jan-02 09:46 gradient_free_optimizers/optimizers/pop_opt/_individual.py
 -rw-rw-r--  2.0 unx     1478 b- defN 22-Oct-08 18:45 gradient_free_optimizers/optimizers/pop_opt/_particle.py
 -rw-rw-r--  2.0 unx     1546 b- defN 22-Oct-08 18:45 gradient_free_optimizers/optimizers/pop_opt/_spiral.py
--rw-rw-r--  2.0 unx     1991 b- defN 23-Feb-28 10:35 gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py
--rw-rw-r--  2.0 unx     2858 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py
--rw-rw-r--  2.0 unx     2439 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py
--rw-rw-r--  2.0 unx     2034 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py
--rw-rw-r--  2.0 unx     2288 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py
+-rw-rw-r--  2.0 unx     2590 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py
+-rw-rw-r--  2.0 unx     2821 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py
+-rw-rw-r--  2.0 unx     2468 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py
+-rw-rw-r--  2.0 unx     1997 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py
+-rw-rw-r--  2.0 unx     2209 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py
 -rw-rw-r--  2.0 unx      359 b- defN 21-Dec-07 17:30 gradient_free_optimizers/optimizers/smb_opt/__init__.py
--rw-rw-r--  2.0 unx     1540 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/smb_opt/bayesian_optimization.py
--rw-rw-r--  2.0 unx     2013 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/smb_opt/forest_optimizer.py
+-rw-rw-r--  2.0 unx     1540 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/smb_opt/bayesian_optimization.py
+-rw-rw-r--  2.0 unx     2013 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/smb_opt/forest_optimizer.py
 -rw-rw-r--  2.0 unx     2614 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/smb_opt/sampling.py
--rw-rw-r--  2.0 unx     4821 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/smb_opt/smbo.py
+-rw-rw-r--  2.0 unx     4784 b- defN 23-Apr-11 05:49 gradient_free_optimizers/optimizers/smb_opt/smbo.py
 -rw-rw-r--  2.0 unx     3710 b- defN 22-Jan-31 13:47 gradient_free_optimizers/optimizers/smb_opt/surrogate_models.py
--rw-rw-r--  2.0 unx     2201 b- defN 23-Feb-28 10:46 gradient_free_optimizers/optimizers/smb_opt/tree_structured_parzen_estimators.py
+-rw-rw-r--  2.0 unx     2201 b- defN 23-Apr-03 14:58 gradient_free_optimizers/optimizers/smb_opt/tree_structured_parzen_estimators.py
 -rw-rw-r--  2.0 unx      177 b- defN 22-Jul-08 16:20 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/__init__.py
 -rw-rw-r--  2.0 unx     1206 b- defN 22-Jul-08 16:20 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/expected_improvement.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 11:38 tests/__init__.py
 -rw-rw-r--  2.0 unx     1716 b- defN 21-Dec-07 17:30 tests/_test_debug.py
--rw-rw-r--  2.0 unx      686 b- defN 23-Feb-28 10:46 tests/_test_examples.py
--rw-rw-r--  2.0 unx     5502 b- defN 23-Mar-24 16:23 tests/_test_memory.py
+-rw-rw-r--  2.0 unx      686 b- defN 23-Apr-03 14:58 tests/_test_examples.py
+-rw-rw-r--  2.0 unx     5502 b- defN 23-Apr-03 14:58 tests/_test_memory.py
 -rw-rw-r--  2.0 unx     1799 b- defN 21-Jan-14 11:38 tests/test_attributes.py
 -rw-rw-r--  2.0 unx    13523 b- defN 23-Feb-28 10:35 tests/test_converter.py
 -rw-rw-r--  2.0 unx     6593 b- defN 21-Dec-07 17:30 tests/test_early_stop.py
 -rw-rw-r--  2.0 unx     1096 b- defN 21-Nov-30 09:18 tests/test_issue_15.py
 -rw-rw-r--  2.0 unx     1765 b- defN 21-Dec-07 17:30 tests/test_max_score.py
 -rw-rw-r--  2.0 unx     1748 b- defN 21-Dec-07 17:30 tests/test_objective_functions.py
 -rw-rw-r--  2.0 unx     3009 b- defN 21-Dec-07 17:30 tests/test_results.py
 -rw-rw-r--  2.0 unx     1283 b- defN 21-Jan-14 11:38 tests/test_verbosity.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 11:38 tests/test_optimizers/__init__.py
 -rw-rw-r--  2.0 unx     5431 b- defN 23-Feb-28 10:46 tests/test_optimizers/_parametrize.py
--rw-rw-r--  2.0 unx      938 b- defN 23-Mar-24 16:23 tests/test_optimizers/_test_max_time.py
--rw-rw-r--  2.0 unx     1735 b- defN 23-Mar-24 16:23 tests/test_optimizers/_test_memory_warm_start.py
+-rw-rw-r--  2.0 unx      938 b- defN 23-Apr-03 14:58 tests/test_optimizers/_test_max_time.py
+-rw-rw-r--  2.0 unx     1735 b- defN 23-Apr-03 14:58 tests/test_optimizers/_test_memory_warm_start.py
 -rw-rw-r--  2.0 unx     1475 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_backend_api.py
 -rw-rw-r--  2.0 unx     1881 b- defN 22-Jul-26 13:24 tests/test_optimizers/test_best_results.py
 -rw-rw-r--  2.0 unx     6818 b- defN 22-Oct-19 17:34 tests/test_optimizers/test_early_stop.py
 -rw-rw-r--  2.0 unx     1839 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_exploration.py
 -rw-rw-r--  2.0 unx     1775 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_inf_nan.py
 -rw-rw-r--  2.0 unx     3050 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_initializers.py
--rw-rw-r--  2.0 unx     1547 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_large_search_space.py
+-rw-rw-r--  2.0 unx     1547 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_large_search_space.py
 -rw-rw-r--  2.0 unx     1744 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_max_score.py
 -rw-rw-r--  2.0 unx     5610 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_multiple_searches.py
 -rw-rw-r--  2.0 unx      370 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_names.py
 -rw-rw-r--  2.0 unx     1624 b- defN 23-Feb-28 10:35 tests/test_optimizers/test_opt_algos_simple.py
 -rw-rw-r--  2.0 unx     1794 b- defN 23-Feb-28 10:35 tests/test_optimizers/test_random_seed.py
 -rw-rw-r--  2.0 unx     3918 b- defN 23-Feb-28 10:35 tests/test_optimizers/test_random_state.py
 -rw-rw-r--  2.0 unx     1511 b- defN 21-Dec-07 17:30 tests/test_optimizers/test_results.py
 -rw-rw-r--  2.0 unx      632 b- defN 22-Oct-22 07:41 tests/test_optimizers/test_search_space_.py
--rw-rw-r--  2.0 unx      704 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_search_step.py
+-rw-rw-r--  2.0 unx      704 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_search_step.py
 -rw-rw-r--  2.0 unx     1560 b- defN 22-Oct-08 18:45 tests/test_optimizers/test_search_tracker.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/__init__.py
 -rw-rw-r--  2.0 unx      598 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/_base_para_test.py
--rw-rw-r--  2.0 unx     3400 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_parameter/test_bayesian_optimizer_para_init.py
+-rw-rw-r--  2.0 unx     3400 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_parameter/test_bayesian_optimizer_para_init.py
 -rw-rw-r--  2.0 unx     1397 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_evolution_strategy_para_init.py
--rw-rw-r--  2.0 unx     2818 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_parameter/test_forest_optimizer_para_init.py
+-rw-rw-r--  2.0 unx     2818 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_parameter/test_forest_optimizer_para_init.py
 -rw-rw-r--  2.0 unx      852 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_hill_climbing_para_init.py
 -rw-rw-r--  2.0 unx     1237 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_parallel_tempering_para_init.py
 -rw-rw-r--  2.0 unx     1305 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_pso_para_init.py
 -rw-rw-r--  2.0 unx      795 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_rand_rest_hill_climbing_para_init.py
 -rw-rw-r--  2.0 unx      870 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_random_annealing_para_init.py
 -rw-rw-r--  2.0 unx      950 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_simulated_annealing_para_init.py
 -rw-rw-r--  2.0 unx      918 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_stochastic_hill_climbing_para_init.py
 -rw-rw-r--  2.0 unx      790 b- defN 23-Feb-27 16:52 tests/test_optimizers/test_parameter/test_tabu_search_init.py
--rw-rw-r--  2.0 unx     2757 b- defN 23-Feb-28 10:46 tests/test_optimizers/test_parameter/test_tpe_para_init.py
+-rw-rw-r--  2.0 unx     2757 b- defN 23-Apr-03 14:58 tests/test_optimizers/test_parameter/test_tpe_para_init.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 11:38 tests/test_parameters/__init__.py
 -rw-rw-r--  2.0 unx      650 b- defN 23-Feb-07 15:42 tests/test_parameters/test_hill_climbing_para.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Jan-14 11:38 tests/test_parameters/test_parameter_init/__init__.py
 -rw-rw-r--  2.0 unx      598 b- defN 21-Mar-11 04:59 tests/test_parameters/test_parameter_init/_base_para_test.py
 -rw-rw-r--  2.0 unx     3400 b- defN 23-Jan-22 06:24 tests/test_parameters/test_parameter_init/test_bayesian_optimizer_para_init.py
 -rw-rw-r--  2.0 unx     1397 b- defN 21-Jun-04 05:55 tests/test_parameters/test_parameter_init/test_evolution_strategy_para_init.py
 -rw-rw-r--  2.0 unx     2818 b- defN 23-Jan-22 06:24 tests/test_parameters/test_parameter_init/test_forest_optimizer_para_init.py
@@ -117,13 +118,13 @@
 -rw-rw-r--  2.0 unx     2757 b- defN 23-Jan-22 06:24 tests/test_parameters/test_parameter_init/test_tpe_para_init.py
 -rw-rw-r--  2.0 unx        0 b- defN 21-Nov-30 12:42 tests/test_performance/__init__.py
 -rw-rw-r--  2.0 unx     1101 b- defN 22-Nov-11 16:15 tests/test_performance/test_global_opt.py
 -rw-rw-r--  2.0 unx     2397 b- defN 22-Nov-11 16:14 tests/test_performance/test_grid_search.py
 -rw-rw-r--  2.0 unx     1155 b- defN 21-Dec-07 17:30 tests/test_performance/test_local_opt.py
 -rw-rw-r--  2.0 unx     1909 b- defN 23-Feb-28 10:46 tests/test_performance/test_pop_opt.py
 -rw-rw-r--  2.0 unx     1882 b- defN 23-Feb-28 10:46 tests/test_performance/test_smb_opt.py
--rw-rw-r--  2.0 unx     1069 b- defN 23-Mar-24 16:23 gradient_free_optimizers-1.2.5.dist-info/LICENSE
--rw-rw-r--  2.0 unx    33825 b- defN 23-Mar-24 16:23 gradient_free_optimizers-1.2.5.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Mar-24 16:23 gradient_free_optimizers-1.2.5.dist-info/WHEEL
--rw-rw-r--  2.0 unx       31 b- defN 23-Mar-24 16:23 gradient_free_optimizers-1.2.5.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    13920 b- defN 23-Mar-24 16:23 gradient_free_optimizers-1.2.5.dist-info/RECORD
-127 files, 286655 bytes uncompressed, 79247 bytes compressed:  72.4%
+-rw-rw-r--  2.0 unx     1069 b- defN 23-Apr-11 06:01 gradient_free_optimizers-1.2.6.dist-info/LICENSE
+-rw-rw-r--  2.0 unx    34934 b- defN 23-Apr-11 06:01 gradient_free_optimizers-1.2.6.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-11 06:01 gradient_free_optimizers-1.2.6.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       31 b- defN 23-Apr-11 06:01 gradient_free_optimizers-1.2.6.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    14015 b- defN 23-Apr-11 06:01 gradient_free_optimizers-1.2.6.dist-info/RECORD
+128 files, 292468 bytes uncompressed, 80258 bytes compressed:  72.6%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: gradient_free_optimizers/__init__.py
 Comment: 
 
+Filename: gradient_free_optimizers/high_lvl_api.py
+Comment: 
+
 Filename: gradient_free_optimizers/memory.py
 Comment: 
 
 Filename: gradient_free_optimizers/print_info.py
 Comment: 
 
 Filename: gradient_free_optimizers/progress_bar.py
@@ -360,23 +363,23 @@
 
 Filename: tests/test_performance/test_pop_opt.py
 Comment: 
 
 Filename: tests/test_performance/test_smb_opt.py
 Comment: 
 
-Filename: gradient_free_optimizers-1.2.5.dist-info/LICENSE
+Filename: gradient_free_optimizers-1.2.6.dist-info/LICENSE
 Comment: 
 
-Filename: gradient_free_optimizers-1.2.5.dist-info/METADATA
+Filename: gradient_free_optimizers-1.2.6.dist-info/METADATA
 Comment: 
 
-Filename: gradient_free_optimizers-1.2.5.dist-info/WHEEL
+Filename: gradient_free_optimizers-1.2.6.dist-info/WHEEL
 Comment: 
 
-Filename: gradient_free_optimizers-1.2.5.dist-info/top_level.txt
+Filename: gradient_free_optimizers-1.2.6.dist-info/top_level.txt
 Comment: 
 
-Filename: gradient_free_optimizers-1.2.5.dist-info/RECORD
+Filename: gradient_free_optimizers-1.2.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## gradient_free_optimizers/__init__.py

```diff
@@ -1,15 +1,15 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
-__version__ = "1.2.5"
+__version__ = "1.2.6"
 __license__ = "MIT"
 
-from .optimizers import (
+from .high_lvl_api import (
     HillClimbingOptimizer,
     StochasticHillClimbingOptimizer,
     RepulsingHillClimbingOptimizer,
     SimulatedAnnealingOptimizer,
     DownhillSimplexOptimizer,
     RandomSearchOptimizer,
     GridSearchOptimizer,
```

## gradient_free_optimizers/search.py

```diff
@@ -24,16 +24,14 @@
         self.new_results_list = []
         self.all_results_list = []
 
         self.score_l = []
         self.pos_l = []
         self.random_seed = None
 
-        self.search_state = "init"
-
         self.results_mang = ResultsManager()
 
     @TimesTracker.eval_time
     def _score(self, pos):
         return self.score(pos)
 
     @TimesTracker.iter_time
@@ -92,16 +90,16 @@
             max_score,
             early_stopping,
             memory,
             memory_warm_start,
             verbosity,
         )
 
-        for nth_iter in range(n_iter):
-            self.search_step(nth_iter)
+        for nth_trial in range(n_iter):
+            self.search_step(nth_trial)
             if self.stop.check():
                 break
 
         self.finish_search()
 
     @SearchStatistics.init_stats
     def init_search(
```

## gradient_free_optimizers/optimizers/base_optimizer.py

```diff
@@ -6,16 +6,14 @@
 from .core_optimizer import CoreOptimizer
 
 
 class BaseOptimizer(CoreOptimizer):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
-        self.state = "init"
-
         self.optimizers = [self]
 
     def finish_initialization(self):
         self.search_state = "iter"
 
     def evaluate(self, score_new):
         if self.pos_best is None:
```

## gradient_free_optimizers/optimizers/core_optimizer/converter.py

```diff
@@ -69,15 +69,14 @@
         for key, p_ in zip(self.para_names, value):
             para[key] = p_
 
         return para
 
     @returnNoneIfArgNone
     def para2value(self, para: Optional[dict]) -> Optional[list]:
-
         value = []
         for para_name in self.para_names:
             value.append(para[para_name])
 
         return value
 
     @returnNoneIfArgNone
@@ -107,14 +106,21 @@
             value_ = np.take(space_dim, pos_1d, axis=0)
             values.append(value_)
 
         values = [list(t) for t in zip(*values)]
         return values
 
     @returnNoneIfArgNone
+    def values2paras(self, values: list) -> list:
+        paras = []
+        for value in values:
+            paras.append(self.value2para(value))
+        return paras
+
+    @returnNoneIfArgNone
     def positions_scores2memory_dict(
         self, positions: Optional[list], scores: Optional[list]
     ) -> Optional[dict]:
         value_tuple_list = list(map(tuple, positions))
         memory_dict = dict(zip(value_tuple_list, scores))
 
         return memory_dict
```

## gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py

```diff
@@ -9,15 +9,15 @@
 from ...utils import set_random_seed, move_random
 
 
 class CoreOptimizer(SearchTracker):
     def __init__(
         self,
         search_space,
-        initialize={"grid": 4, "random": 2, "vertices": 4},
+        initialize: dict = {"grid": 4, "random": 2, "vertices": 4},
         random_state=None,
         rand_rest_p=0,
         nth_process=None,
         debug_log=False,
     ):
         super().__init__()
 
@@ -28,14 +28,18 @@
 
         self.initialize = initialize
         self.random_state = random_state
         self.rand_rest_p = rand_rest_p
         self.nth_process = nth_process
         self.debug_log = debug_log
 
+        self.nth_init = 0
+        self.nth_trial = 0
+        self.search_state = "init"
+
     def random_iteration(func):
         def wrapper(self, *args, **kwargs):
             if self.rand_rest_p > random.uniform(0, 1):
                 return self.move_random()
             else:
                 return func(self, *args, **kwargs)
 
@@ -58,23 +62,17 @@
         return pos
 
     def move_random(self):
         return move_random(self.conv.search_space_positions)
 
     @SearchTracker.track_new_pos
     def init_pos(self):
-        init_pos = self.init.init_positions_l[self.n_init_total]
+        init_pos = self.init.init_positions_l[self.nth_init]
         return init_pos
 
-    def finish_initialization(self):
-        raise NotImplementedError
-
-    def evaluate_iter(self, score_new):
-        raise NotImplementedError
-
     @SearchTracker.track_new_score
     def evaluate_init(self, score_new):
         if self.pos_best is None:
             self.pos_best = self.pos_new
             self.score_best = score_new
 
         if self.pos_current is None:
```

## gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py

```diff
@@ -35,23 +35,25 @@
         self.best_since_iter = 0
 
     ##################### track new #####################
 
     def track_new_pos(func):
         def wrapper(self, *args, **kwargs):
             self.pos_new = func(self, *args, **kwargs)
-            self.nth_trial += 1
+            self.nth_init += 1
             return self.pos_new
 
         return wrapper
 
     def track_new_score(func):
         def wrapper(self, score):
             self.score_new = score
-            return func(self, score)
+            _return_ = func(self, score)
+            self.nth_trial += 1
+            return _return_
 
         return wrapper
 
     ##################### evaluate #####################
 
     def _eval2current(self, pos, score):
         if score > self.score_current:
```

## gradient_free_optimizers/optimizers/exp_opt/random_annealing.py

```diff
@@ -1,17 +1,16 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
 from ..local_opt import HillClimbingOptimizer
-from ...search import Search
 
 
-class RandomAnnealingOptimizer(HillClimbingOptimizer, Search):
+class RandomAnnealingOptimizer(HillClimbingOptimizer):
     name = "Random Annealing"
     _name_ = "random_annealing"
 
     def __init__(
         self,
         *args,
         epsilon=0.03,
```

## gradient_free_optimizers/optimizers/global_opt/pattern_search.py

```diff
@@ -2,24 +2,23 @@
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 import random
 import numpy as np
 
 from ..base_optimizer import BaseOptimizer
-from ...search import Search
 
 
 def max_list_idx(list_):
     max_item = max(list_)
     max_item_idx = [i for i, j in enumerate(list_) if j == max_item]
     return max_item_idx[-1:][0]
 
 
-class PatternSearch(BaseOptimizer, Search):
+class PatternSearch(BaseOptimizer):
     name = "Pattern Search"
     _name_ = "pattern_search"
     __name__ = "PatternSearch"
 
     optimizer_type = "global"
     computationally_expensive = False
 
@@ -78,18 +77,17 @@
         self.generate_pattern(self.pos_current)
         self.search_state = "iter"
 
     @BaseOptimizer.track_new_score
     def evaluate(self, score_new):
         BaseOptimizer.evaluate(self, score_new)
         if len(self.scores_valid) == 0:
-
             return
 
-        modZero = self.nth_iter % int(self.n_positions_ * 2) == 0
+        modZero = self.nth_trial % int(self.n_positions_ * 2) == 0
 
         if modZero or len(self.pattern_pos_l) == 0:
             if self.search_state == "iter":
                 self.generate_pattern(self.pos_current)
 
             score_new_list_temp = self.scores_valid[-self.n_positions_ :]
             pos_new_list_temp = self.positions_valid[-self.n_positions_ :]
```

## gradient_free_optimizers/optimizers/global_opt/powells_method.py

```diff
@@ -2,25 +2,24 @@
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 import numpy as np
 from collections import OrderedDict
 
 from ..base_optimizer import BaseOptimizer
-from ...search import Search
 from ..local_opt import HillClimbingOptimizer
 
 
 def sort_list_idx(list_):
     list_np = np.array(list_)
     idx_sorted = list(list_np.argsort()[::-1])
     return idx_sorted
 
 
-class PowellsMethod(BaseOptimizer, Search):
+class PowellsMethod(BaseOptimizer):
     name = "Powell's Method"
     _name_ = "powells_method"
     __name__ = "PowellsMethod"
 
     optimizer_type = "global"
     computationally_expensive = False
```

## gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py

```diff
@@ -1,17 +1,16 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
 from ..local_opt import HillClimbingOptimizer
-from ...search import Search
 
 
-class RandomRestartHillClimbingOptimizer(HillClimbingOptimizer, Search):
+class RandomRestartHillClimbingOptimizer(HillClimbingOptimizer):
     name = "Random Restart Hill Climbing"
     _name_ = "random_restart_hill_climbing"
     __name__ = "RandomRestartHillClimbingOptimizer"
 
     optimizer_type = "global"
     computationally_expensive = False
 
@@ -29,16 +28,16 @@
         self.distribution = distribution
         self.n_neighbours = n_neighbours
         self.n_iter_restart = n_iter_restart
 
     @HillClimbingOptimizer.track_new_pos
     @HillClimbingOptimizer.random_iteration
     def iterate(self):
-        notZero = self.nth_iter != 0
-        modZero = self.nth_iter % self.n_iter_restart == 0
+        notZero = self.nth_trial != 0
+        modZero = self.nth_trial % self.n_iter_restart == 0
 
         if notZero and modZero:
             pos = self.move_random()
         else:
             pos = self._move_climb(self.pos_current)
 
         return pos
```

## gradient_free_optimizers/optimizers/global_opt/random_search.py

```diff
@@ -1,17 +1,16 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
 from ..base_optimizer import BaseOptimizer
-from ...search import Search
 
 
-class RandomSearchOptimizer(BaseOptimizer, Search):
+class RandomSearchOptimizer(BaseOptimizer):
     name = "Random Search"
     _name_ = "random_search"
     __name__ = "RandomSearchOptimizer"
 
     optimizer_type = "global"
     computationally_expensive = False
```

## gradient_free_optimizers/optimizers/grid/grid_search.py

```diff
@@ -6,18 +6,17 @@
 
 try:
     from fractions import gcd
 except:
     from math import gcd
 
 from ..base_optimizer import BaseOptimizer
-from ...search import Search
 
 
-class GridSearchOptimizer(BaseOptimizer, Search):
+class GridSearchOptimizer(BaseOptimizer):
     name = "Grid Search"
     _name_ = "grid_search"
     __name__ = "GridSearchOptimizer"
 
     optimizer_type = "global"
     computationally_expensive = False
 
@@ -79,18 +78,18 @@
             return self.initial_position
 
         # If this is not the first iteration:
         # Update high_dim_pointer by taking a step of size step_size * direction.
 
         # Multiple passes are needed in order to observe the entire search space
         # depending on the step_size parameter.
-        _, current_pass = self.nth_iter, self.high_dim_pointer % self.step_size
+        _, current_pass = self.nth_trial, self.high_dim_pointer % self.step_size
         current_pass_finished = (
-            (self.nth_iter + 1) * self.step_size // self.conv.search_space_size
-            > self.nth_iter * self.step_size // self.conv.search_space_size
+            (self.nth_trial + 1) * self.step_size // self.conv.search_space_size
+            > self.nth_trial * self.step_size // self.conv.search_space_size
         )
         # Begin the next pass if current is finished.
         if current_pass_finished:
             self.high_dim_pointer = current_pass + 1
         else:
             # Otherwise update pointer in Z/(search_space_size*Z)
             # using the prime step direction and step_size.
```

## gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py

```diff
@@ -14,14 +14,15 @@
     list_np = np.array(list_)
     idx_sorted = list(list_np.argsort()[::-1])
     return idx_sorted
 
 
 def centeroid(array_list):
     centeroid = []
+
     for idx in range(array_list[0].shape[0]):
         center_dim_pos = []
         for array in array_list:
             center_dim_pos.append(array[idx])
 
         center_dim_mean = np.array(center_dim_pos).mean()
         centeroid.append(center_dim_mean)
```

## gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py

```diff
@@ -1,15 +1,14 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 import numpy as np
 
 from ..base_optimizer import BaseOptimizer
-from ...search import Search
 
 from numpy.random import normal, laplace, logistic, gumbel
 
 dist_dict = {
     "normal": normal,
     "laplace": laplace,
     "logistic": logistic,
@@ -19,15 +18,15 @@
 
 def max_list_idx(list_):
     max_item = max(list_)
     max_item_idx = [i for i, j in enumerate(list_) if j == max_item]
     return max_item_idx[-1:][0]
 
 
-class HillClimbingOptimizer(BaseOptimizer, Search):
+class HillClimbingOptimizer(BaseOptimizer):
     name = "Hill Climbing"
     _name_ = "hill_climbing"
     __name__ = "HillClimbingOptimizer"
 
     optimizer_type = "local"
     computationally_expensive = False
 
@@ -52,15 +51,15 @@
 
     @BaseOptimizer.track_new_score
     def evaluate(self, score_new):
         BaseOptimizer.evaluate(self, score_new)
         if len(self.scores_valid) == 0:
             return
 
-        modZero = self.nth_iter % self.n_neighbours == 0
+        modZero = self.nth_trial % self.n_neighbours == 0
         if modZero:
             score_new_list_temp = self.scores_valid[-self.n_neighbours :]
             pos_new_list_temp = self.positions_valid[-self.n_neighbours :]
 
             idx = max_list_idx(score_new_list_temp)
             score = score_new_list_temp[idx]
             pos = pos_new_list_temp[idx]
```

## gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py

```diff
@@ -1,17 +1,16 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
 from . import HillClimbingOptimizer
-from ...search import Search
 
 
-class RepulsingHillClimbingOptimizer(HillClimbingOptimizer, Search):
+class RepulsingHillClimbingOptimizer(HillClimbingOptimizer):
     name = "Repulsing Hill Climbing"
     _name_ = "repulsing_hill_climbing"
     __name__ = "RepulsingHillClimbingOptimizer"
 
     optimizer_type = "local"
     computationally_expensive = False
```

## gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py

```diff
@@ -2,18 +2,17 @@
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
 import numpy as np
 
 from ..local_opt import StochasticHillClimbingOptimizer
-from ...search import Search
 
 
-class SimulatedAnnealingOptimizer(StochasticHillClimbingOptimizer, Search):
+class SimulatedAnnealingOptimizer(StochasticHillClimbingOptimizer):
     name = "Simulated Annealing"
     _name_ = "simulated_annealing"
     __name__ = "SimulatedAnnealingOptimizer"
 
     optimizer_type = "local"
     computationally_expensive = False
```

## gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py

```diff
@@ -2,18 +2,17 @@
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 import random
 import numpy as np
 
 from . import HillClimbingOptimizer
-from ...search import Search
 
 
-class StochasticHillClimbingOptimizer(HillClimbingOptimizer, Search):
+class StochasticHillClimbingOptimizer(HillClimbingOptimizer):
     name = "Stochastic Hill Climbing"
     _name_ = "stochastic_hill_climbing"
     __name__ = "StochasticHillClimbingOptimizer"
 
     optimizer_type = "local"
     computationally_expensive = False
```

## gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py

```diff
@@ -1,17 +1,33 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
-
+import math
 import numpy as np
 
 from ..core_optimizer import CoreOptimizer
 
 
+def split(positions_l, population):
+    div_int = math.ceil(len(positions_l) / population)
+    dist_init_positions = []
+
+    for nth_indiv in range(population):
+        indiv_pos = []
+        for nth_indiv_pos in range(div_int):
+            idx = nth_indiv + nth_indiv_pos * population
+            if idx < len(positions_l):
+                indiv_pos.append(positions_l[idx])
+
+        dist_init_positions.append(indiv_pos)
+
+    return dist_init_positions
+
+
 class BasePopulationOptimizer(CoreOptimizer):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
         self.eval_times = []
         self.iter_times = []
 
@@ -41,24 +57,28 @@
             pop_size = len(self.population)
         diff_init = pop_size - self.init.n_inits
 
         if diff_init > 0:
             self.init.add_n_random_init_pos(diff_init)
 
         if isinstance(self.population, int):
+            distributed_init_positions = split(
+                self.init.init_positions_l, self.population
+            )
+
             population = []
-            for init_position in self.init.init_positions_l:
-                init_value = self.conv.position2value(init_position)
-                init_para = self.conv.value2para(init_value)
+            for init_positions in distributed_init_positions:
+                init_values = self.conv.positions2values(init_positions)
+                init_paras = self.conv.values2paras(init_values)
 
                 population.append(
                     Optimizer(
                         self.conv.search_space,
                         rand_rest_p=self.rand_rest_p,
-                        initialize={"warm_start": [init_para]},
+                        initialize={"warm_start": init_paras},
                     )
                 )
         else:
             population = self.population
 
         return population
```

## gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py

```diff
@@ -2,19 +2,18 @@
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 import random
 import numpy as np
 
 from .base_population_optimizer import BasePopulationOptimizer
-from ...search import Search
 from ._individual import Individual
 
 
-class EvolutionStrategyOptimizer(BasePopulationOptimizer, Search):
+class EvolutionStrategyOptimizer(BasePopulationOptimizer):
     name = "Evolution Strategy"
     _name_ = "evolution_strategy"
     __name__ = "EvolutionStrategyOptimizer"
 
     optimizer_type = "population"
     computationally_expensive = False
```

## gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py

```diff
@@ -4,19 +4,18 @@
 
 import copy
 import random
 
 import numpy as np
 
 from .base_population_optimizer import BasePopulationOptimizer
-from ...search import Search
 from ..local_opt import SimulatedAnnealingOptimizer
 
 
-class ParallelTemperingOptimizer(BasePopulationOptimizer, Search):
+class ParallelTemperingOptimizer(BasePopulationOptimizer):
     name = "Parallel Tempering"
     _name_ = "parallel_tempering"
     __name__ = "ParallelTemperingOptimizer"
 
     optimizer_type = "population"
     computationally_expensive = False
 
@@ -56,20 +55,24 @@
 
             temp = (1 / _p1_.temp) - (1 / _p2_.temp)
             return np.exp(score_diff_norm * temp) * 100
 
     @BasePopulationOptimizer.track_new_pos
     def init_pos(self):
         nth_pop = self.nth_trial % len(self.systems)
+        print("\n nth_pop", nth_pop)
+
         self.p_current = self.systems[nth_pop]
 
         return self.p_current.init_pos()
 
     @BasePopulationOptimizer.track_new_pos
     def iterate(self):
+        print("  iterate")
+
         self.p_current = self.systems[self.nth_trial % len(self.systems)]
 
         return self.p_current.iterate()
 
     @BasePopulationOptimizer.track_new_score
     def evaluate(self, score_new):
         notZero = self.n_iter_swap != 0
```

## gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py

```diff
@@ -2,19 +2,18 @@
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
 import numpy as np
 
 from .base_population_optimizer import BasePopulationOptimizer
-from ...search import Search
 from ._particle import Particle
 
 
-class ParticleSwarmOptimizer(BasePopulationOptimizer, Search):
+class ParticleSwarmOptimizer(BasePopulationOptimizer):
     name = "Particle Swarm Optimization"
     _name_ = "particle_swarm_optimization"
     __name__ = "ParticleSwarmOptimizer"
 
     optimizer_type = "population"
     computationally_expensive = False
```

## gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py

```diff
@@ -1,15 +1,14 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 import numpy as np
 
 from .base_population_optimizer import BasePopulationOptimizer
-from ...search import Search
 from ._spiral import Spiral
 
 
 def centeroid(array_list):
     centeroid = []
     for idx in range(array_list[0].shape[0]):
         center_dim_pos = []
@@ -18,15 +17,15 @@
 
         center_dim_mean = np.array(center_dim_pos).mean()
         centeroid.append(center_dim_mean)
 
     return centeroid
 
 
-class SpiralOptimization(BasePopulationOptimizer, Search):
+class SpiralOptimization(BasePopulationOptimizer):
     name = "Spiral Optimization"
     _name_ = "spiral_optimization"
     __name__ = "SpiralOptimization"
 
     optimizer_type = "population"
     computationally_expensive = False
 
@@ -53,16 +52,15 @@
         self.center_pos = self.pop_sorted[0].pos_current
         self.center_score = self.pop_sorted[0].score_current
 
         self.search_state = "iter"
 
     @BasePopulationOptimizer.track_new_pos
     def iterate(self):
-        n_iter = self._iterations(self.particles)
-        self.p_current = self.particles[n_iter % len(self.particles)]
+        self.p_current = self.particles[self.nth_trial % len(self.particles)]
 
         self.sort_pop_best_score()
         self.p_current.global_pos_best = self.pop_sorted[0].pos_current
 
         return self.p_current.move_spiral(self.center_pos)
 
     @BasePopulationOptimizer.track_new_score
```

## gradient_free_optimizers/optimizers/smb_opt/smbo.py

```diff
@@ -1,22 +1,21 @@
 # Author: Simon Blanke
 # Email: simon.blanke@yahoo.com
 # License: MIT License
 
 
 from ..base_optimizer import BaseOptimizer
-from ...search import Search
 from .sampling import InitialSampler
 
 import numpy as np
 
 np.seterr(divide="ignore", invalid="ignore")
 
 
-class SMBO(BaseOptimizer, Search):
+class SMBO(BaseOptimizer):
     def __init__(
         self,
         *args,
         warm_start_smbo=None,
         max_sample_size=10000000,
         sampling={"random": 1000000},
         **kwargs
```

## Comparing `gradient_free_optimizers-1.2.5.dist-info/LICENSE` & `gradient_free_optimizers-1.2.6.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `gradient_free_optimizers-1.2.5.dist-info/METADATA` & `gradient_free_optimizers-1.2.6.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: gradient-free-optimizers
-Version: 1.2.5
+Version: 1.2.6
 Summary: 
 Home-page: https://github.com/SimonBlanke/Gradient-Free-Optimizers
 Author: Simon Blanke
 Author-email: simon.blanke@yahoo.com
 License: MIT
 Keywords: optimization
 Classifier: Programming Language :: Python :: 3
@@ -21,19 +21,19 @@
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Information Technology
 Classifier: Intended Audience :: Science/Research
 Requires-Python: >=3.5
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: scipy
-Requires-Dist: numpy (>=1.18.1)
-Requires-Dist: pandas
+Requires-Dist: scipy (<2.0.0)
+Requires-Dist: numpy (<2.0.0,>=1.18.1)
+Requires-Dist: pandas (<2.0.0)
 Requires-Dist: scikit-learn (!=0.23.*,>=0.21)
-Requires-Dist: tqdm
+Requires-Dist: tqdm (<5.0.0,>=4.48.0)
 
 <p align="center">
   <br>
   <a href="https://github.com/SimonBlanke/Gradient-Free-Optimizers"><img src="./docs/images/gradient_logo_ink.png" height="280"></a>
   <br>
 </p>
 
@@ -720,14 +720,29 @@
     <td> <img src="./docs/gifs/forest_optimization_ackley_function_.gif" width="100%"> </td>
   </tr>
 </table>
 
 </details>
 
 
+
+<br>
+
+## Sideprojects and Tools
+
+The following packages are designed to support Gradient-Free-Optimizers and expand its use cases. 
+
+| Package                                                                       | Description                                                                          |
+|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|
+| [Search-Data-Collector](https://github.com/SimonBlanke/search-data-collector) | Simple tool to save search-data during or after the optimization run into csv-files. |
+| [Search-Data-Explorer](https://github.com/SimonBlanke/search-data-explorer)   | Visualize search-data with plotly inside a streamlit dashboard.
+
+If you want news about Gradient-Free-Optimizers and related projects you can follow me on [twitter](https://twitter.com/blanke_simon).
+
+
 <br>
 
 ## Installation
 
 [![PyPI version](https://badge.fury.io/py/gradient-free-optimizers.svg)](https://badge.fury.io/py/gradient-free-optimizers)
 
 The most recent version of Gradient-Free-Optimizers is available on PyPi:
@@ -1164,15 +1179,16 @@
 
 </details>
 
 <details>
 <summary><b>v1.5.0</b> </summary>
 
   - [ ] add Ant-colony optimization
-  - [ ] ...
+  - [ ] add Grid search paraneter that changes direction of search
+  - [ ] add Random search parameter that enables to avoid replacement of the sampling
 
 </details>
 
 <details>
 <summary><b>v2.0.0</b> </summary>
 
   - [ ] add other acquisition functions to smbo (Probability of improvement, Entropy search, ...)
```

## Comparing `gradient_free_optimizers-1.2.5.dist-info/RECORD` & `gradient_free_optimizers-1.2.6.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,56 +1,57 @@
-gradient_free_optimizers/__init__.py,sha256=7rDe8AUN4vuAHb7hD_bMZYSGZjzdhdhwpt7nFifZnYg,1382
+gradient_free_optimizers/__init__.py,sha256=rdb93u4mPUh0t8zTLpl52aamq871a1m7RMGRMDEUz7Q,1384
+gradient_free_optimizers/high_lvl_api.py,sha256=GjuOoxUQtqA076XsPzvmr3mhtKhMuo4EgP4aNetPzp0,4393
 gradient_free_optimizers/memory.py,sha256=ep60wwNMSTbUioCV0MAajj69P68hqmOTzWwAN0uVJqA,1455
 gradient_free_optimizers/print_info.py,sha256=I2b6hg5sAmyLhfCXZYPt69tX_NTsbP6o-JA7FnGn2MA,2763
 gradient_free_optimizers/progress_bar.py,sha256=fDnZ5CyNoykn2qnCvgY-ySl08Pup0sOI_ID6gWsjBB4,2900
 gradient_free_optimizers/results_manager.py,sha256=vQNaVLLWpfGlrfkCoESW64ZdDUX8DvJUdhHek_YCaro,1045
-gradient_free_optimizers/search.py,sha256=FLHX8q5yBWNFMP_22NYbiQGUrhexaWmd7qm_lE_q2Xs,5901
+gradient_free_optimizers/search.py,sha256=uPy4NsAofByOekPq2EPRythUTUUX4ysuwF4kH-ZaolA,5867
 gradient_free_optimizers/search_statistics.py,sha256=AcwdPH1kFnD5-OtAmlbwaXfLV-fi6EzFhk3uz3mXlns,478
 gradient_free_optimizers/stop_run.py,sha256=4u4DA4ilpzGL9L6NN2ky0QxwrLw03EwHguGIvGFLao8,2263
 gradient_free_optimizers/times_tracker.py,sha256=jRB1xOgd2898Ty9OIGoG_a6oJ7gGW53s4GnmYZvp7as,710
 gradient_free_optimizers/utils.py,sha256=X2M6tJ-hJacQqAHcQstdI28jkB0hhUwyFA_w7h8j2pw,782
 gradient_free_optimizers/optimizers/__init__.py,sha256=lXsfj6shH7lY1DJSaJwiqdYg3chaC9OxOMI7DYXabzU,1498
-gradient_free_optimizers/optimizers/base_optimizer.py,sha256=1PU-Pl1TYHS7Ch0dhhm7kiP4LBNhGV479IJ1x3xf2nI,618
+gradient_free_optimizers/optimizers/base_optimizer.py,sha256=7--DnGQeSCVJ71LNfFpbxOHobqd4VJqCi_5YtjfNwkY,589
 gradient_free_optimizers/optimizers/core_optimizer/__init__.py,sha256=FIDA8-LbTyFBE5hStaafJqpzK7cyrdvBRoJIbEFoe38,207
-gradient_free_optimizers/optimizers/core_optimizer/converter.py,sha256=oFO6c-mpBi3z29XQ7fh2pY467wcjiZJGfI4C7vA4iIA,5148
-gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py,sha256=zfxQD0CBs8NscgJALhC6MGuUd1PTieEl9jz10VuZdLQ,2343
+gradient_free_optimizers/optimizers/core_optimizer/converter.py,sha256=SS62tFcfNtubbKwC3Iprd0Yku3bjz_spoye30gheNw0,5341
+gradient_free_optimizers/optimizers/core_optimizer/core_optimizer.py,sha256=-RaW3n5txxKNJXeGC8LTDzWGCCd3VBfTFe-g-OnoTxw,2289
 gradient_free_optimizers/optimizers/core_optimizer/init_positions.py,sha256=TM6Dw00ZKMvLC-g8SFZ47O9sycVDKsTiDrsw3zipqh0,4142
-gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py,sha256=d2FcFDCSwmv89EP4l9Vw7Wga78CI-_rof5jrWU8sea8,3695
+gradient_free_optimizers/optimizers/core_optimizer/search_tracker.py,sha256=aR_fy6J3jE2Ox29XNLfwmQ6dku8o36nPuFWZouYIEgU,3758
 gradient_free_optimizers/optimizers/exp_opt/__init__.py,sha256=LPWvfKVliMKw2_jQy1FHV-eyZM5MjbpKuL-q3JTuzQ0,256
 gradient_free_optimizers/optimizers/exp_opt/ensemble_optimizer.py,sha256=tRQJjUuumu-aYCMo_0Ulp2qI7Z-iWUq4jHh7T0HKT3Q,2210
-gradient_free_optimizers/optimizers/exp_opt/random_annealing.py,sha256=ZbZ1J_3SaMb0Q83KFp-54viQQCNkV5wXiiumOE1y_5Y,1004
+gradient_free_optimizers/optimizers/exp_opt/random_annealing.py,sha256=6GJ6cU9YfdOyHe2bNiuaeuzkFNRR_gl4SdPUrYEDE_M,967
 gradient_free_optimizers/optimizers/global_opt/__init__.py,sha256=kkL9j8TfPGzBufPKcbPHAQ53ju6HTs3J8PZdsaeqFl0,567
 gradient_free_optimizers/optimizers/global_opt/direct_algorithm.py,sha256=lLwgpDXc5e0feDfAhzD4FKnRJl4yhqC72J6RJL9QR94,4250
 gradient_free_optimizers/optimizers/global_opt/lipschitz_optimization.py,sha256=6XbCEe5v47TXtzq4b1RPNLQPCpCRrM2ghSCle8ri7ag,2193
-gradient_free_optimizers/optimizers/global_opt/pattern_search.py,sha256=c0t3N5c8M_eoGUmhga9Aboh20uEBF2JkSl9Wum6A8Jo,3138
-gradient_free_optimizers/optimizers/global_opt/powells_method.py,sha256=rc7Qc58MQzMzrffylFIfL0ltf3rxNwKZxZiuTiRihqs,3305
-gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py,sha256=QkcS5dlpLI8A1UT6GPbpq71KtfCfdsqUvvgTLR8Pmsw,1175
-gradient_free_optimizers/optimizers/global_opt/random_search.py,sha256=dXspXxs8sAmV9VBC4j428E51xeedmqjIs_0aO3fCE_w,659
+gradient_free_optimizers/optimizers/global_opt/pattern_search.py,sha256=1aLwk4gdR8E5Gs5ZYMT-hiu723AewRgWOsFN93-fRy0,3101
+gradient_free_optimizers/optimizers/global_opt/powells_method.py,sha256=FyplDfdGMK3S6cxKLVDxeUfHNUXPHoc9VKdVSzqiu7s,3268
+gradient_free_optimizers/optimizers/global_opt/random_restart_hill_climbing.py,sha256=JXaCmvwk8vNzrOnkEe1yODpp32br74baRFALDT8v4sA,1140
+gradient_free_optimizers/optimizers/global_opt/random_search.py,sha256=Ct5TBDGGVq7wuSN6ZWPOzrHnPssuP2Y-1NiqtUgZNP8,622
 gradient_free_optimizers/optimizers/grid/__init__.py,sha256=81XQ8K4ij1HYv3oH8egbwmbi1YMeH4Ej2hrDwkzeCRo,167
-gradient_free_optimizers/optimizers/grid/grid_search.py,sha256=hyI930OkLt0Xul-EjdgypvmSdQTLtaKdTOS8dT0_ym0,4057
+gradient_free_optimizers/optimizers/grid/grid_search.py,sha256=07xC_Vl2ddvbBlvsE81GjWVwE8A_WoholtYPMwoem9w,4023
 gradient_free_optimizers/optimizers/local_opt/__init__.py,sha256=mbXmx0NFu50CSQotpqcGvFcV2j0_tjdEExPFuoTBNGo,591
-gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py,sha256=1pTlk7Qk8z-Dw955lbGbKYGZipNTbqbEGLD0g49vOoY,5537
-gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py,sha256=23elnl4iDVy_OscCDlqAcf3xg16BemO9Dbb-g_b-cs0,1980
-gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py,sha256=eG_ba6BPh7DOZSPqARsS4az_tbiEKBts_HqKQEj46D8,977
-gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py,sha256=ESKMRMXlFbXU4frcaWaO5Ux28aa3dgfUCHFX1KraJOQ,1008
-gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py,sha256=GW-FJffdwLHv1IgwE_ZvYcNpja0MIaKVAhPEHY_jV-Q,2134
+gradient_free_optimizers/optimizers/local_opt/downhill_simplex.py,sha256=JvxTpw0TId3lNmSGTp4lM4Msi3-GwQuwnPfm9hMm_Tc,5538
+gradient_free_optimizers/optimizers/local_opt/hill_climbing_optimizer.py,sha256=TBGELaIrnOgnZIDjuebvp-RJKfHYwwaLVn1aM4c33GE,1944
+gradient_free_optimizers/optimizers/local_opt/repulsing_hill_climbing_optimizer.py,sha256=mOe7DgB04n_OkrL1EQcGjZWWep--sRZu7W_t_oINX7c,940
+gradient_free_optimizers/optimizers/local_opt/simulated_annealing.py,sha256=eGlObRWMag8jJeoMzw13Vp10u6UyOPRdoavR1Mr4DqM,971
+gradient_free_optimizers/optimizers/local_opt/stochastic_hill_climbing.py,sha256=FqyLOscIRzJbqkS0mjg-6n8JQn36YNFySy_FVbDXzKI,2097
 gradient_free_optimizers/optimizers/pop_opt/__init__.py,sha256=bNJlXHMb0ziYQLrvHIwfWRYdc0mTN_d8WWJc2LjDhTs,452
 gradient_free_optimizers/optimizers/pop_opt/_individual.py,sha256=jNG11NWoDpHI4x7h5Y5_CV_D0EckqkyqoPjf9MvzD6I,309
 gradient_free_optimizers/optimizers/pop_opt/_particle.py,sha256=f5iMsAXZXHK41EDAHdAp87VDh42BFL55mPe_evqLeUo,1478
 gradient_free_optimizers/optimizers/pop_opt/_spiral.py,sha256=J7Wai0jXaUnuLpSE0AvW2xJztjtXMuAcD0pBs6D0pLk,1546
-gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py,sha256=oV3yEbW3rV0bQSdpviomosKYGielQuP0IcE48YFNWnY,1991
-gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py,sha256=uagS3f1MpRwW20UsGSawckuDAXjyY4CNM5LzRgobqe8,2858
-gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py,sha256=HeR5Oex23xzjkWFysXLw-exLp50F_AaBMLCBvz9z_Rc,2439
-gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py,sha256=kMcX6I-YPa_p0-FYKndFlzLiP2Q038Rgl_gD3CGEKvg,2034
-gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py,sha256=E_hECtd3-w3os-O0MB5tsx9cE8VkFDKaqL8dVrU0su0,2288
+gradient_free_optimizers/optimizers/pop_opt/base_population_optimizer.py,sha256=3qTUyihylnzXfG2kb8rXnrc62t3dA4GM6asG6Zp25zM,2590
+gradient_free_optimizers/optimizers/pop_opt/evolution_strategy.py,sha256=DRBEU2Yi0jpO0HaTp_jtgDrt3VN2dcg9HmmaKaGxN7c,2821
+gradient_free_optimizers/optimizers/pop_opt/parallel_tempering.py,sha256=JkPKsArH5nc9cfw00TP_AhpfWcgJBEDtz51dxt83sls,2468
+gradient_free_optimizers/optimizers/pop_opt/particle_swarm_optimization.py,sha256=aENEFW_GWDGrQlJcTPkHwSLn9bAYuqH2qe8jk76bNo8,1997
+gradient_free_optimizers/optimizers/pop_opt/spiral_optimization.py,sha256=b2LuFpVNW7inJmhKQX_zDBxJykY_4NqGOt4wti-FICw,2209
 gradient_free_optimizers/optimizers/smb_opt/__init__.py,sha256=ITLZ8DIKx0ZxLT_YpEKYO28TQYXcrUHQYzgDXUDbsgo,359
 gradient_free_optimizers/optimizers/smb_opt/bayesian_optimization.py,sha256=0yr0NODLJsm88gX2UA1hi0qHWwE-xJsrpuBm-Bl34BM,1540
 gradient_free_optimizers/optimizers/smb_opt/forest_optimizer.py,sha256=tz6bavnIROEUXRCWBFJcLuec3uJzULuf3Nm1476nsJ8,2013
 gradient_free_optimizers/optimizers/smb_opt/sampling.py,sha256=1VV5mHL8mnqJSYyBi-AffZZMiLJVGjUg_b-O3RL-9XE,2614
-gradient_free_optimizers/optimizers/smb_opt/smbo.py,sha256=AjIlT3X1YKouVVtvTTuiDPXJasrkpjsNcuU0IBV7dy8,4821
+gradient_free_optimizers/optimizers/smb_opt/smbo.py,sha256=El7x6jg0smS6y0jDXJXrypv3szOA_jwwTo0eKhjVCBw,4784
 gradient_free_optimizers/optimizers/smb_opt/surrogate_models.py,sha256=ynQE209wwP7kEGnUk5-Zio7zo4rnVXCfU1y62zf2t8I,3710
 gradient_free_optimizers/optimizers/smb_opt/tree_structured_parzen_estimators.py,sha256=jfeLSje8XpJX0v3xVPLXl0XCKMjSCcrr1N-5ze6CaW0,2201
 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/__init__.py,sha256=RfxV3A9aunpUJLu4AEnyE452OBaD1d5eRshKvTbxotI,177
 gradient_free_optimizers/optimizers/smb_opt/acquisition_function/expected_improvement.py,sha256=Y6Zx8ZpfgDkwoYiHSntK5kv8_kQ5-HWOmKcn68EmhWs,1206
 tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/_test_debug.py,sha256=1485GY-WaPX7AVhqpS0eBst-yKvHiqU1asVP8Q2aPc0,1716
 tests/_test_examples.py,sha256=7QRuve6FWx7oiw39NU2Lzx1eAA3cN_Hn2POBDYpl2ig,686
@@ -116,12 +117,12 @@
 tests/test_parameters/test_parameter_init/test_tpe_para_init.py,sha256=bGY4yVqdkBVNKh5LvWW_KANo3uGIvj7YbsHSTiQxKlw,2757
 tests/test_performance/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/test_performance/test_global_opt.py,sha256=AfZSULL_Xd6ujhdCJlE_kN0BwVa7OFCUzu0rIUkXMx4,1101
 tests/test_performance/test_grid_search.py,sha256=KanTgPTI7XKWY5tUFSFA0jO6vmWCD6UlLm2gjDkJMPM,2397
 tests/test_performance/test_local_opt.py,sha256=Ntob_sYLr5ibivJTPuvm5sw8neWTLK9iJYk3jICKHHg,1155
 tests/test_performance/test_pop_opt.py,sha256=humLFMMbvZ_mJJLN3rUyop07MTp65Cmm4BEvKCReVNY,1909
 tests/test_performance/test_smb_opt.py,sha256=nMeWlSXMT5kBL1rlxeBza0vlvWOOWj0loFyiYHm81GA,1882
-gradient_free_optimizers-1.2.5.dist-info/LICENSE,sha256=RsfzUEwfUDjkq2Uk3ATEXNtAOa2WqG-6zBP_S5ERJ5I,1069
-gradient_free_optimizers-1.2.5.dist-info/METADATA,sha256=ZxVNqP6BC6Ij5tM8W5ZKT9msW-w2a9-vjX1XtucVTJI,33825
-gradient_free_optimizers-1.2.5.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-gradient_free_optimizers-1.2.5.dist-info/top_level.txt,sha256=vvSJQaQ3s7mZx_5ZoYAS3sBswwsRh2xhcx1vmlwhjrQ,31
-gradient_free_optimizers-1.2.5.dist-info/RECORD,,
+gradient_free_optimizers-1.2.6.dist-info/LICENSE,sha256=RsfzUEwfUDjkq2Uk3ATEXNtAOa2WqG-6zBP_S5ERJ5I,1069
+gradient_free_optimizers-1.2.6.dist-info/METADATA,sha256=TqQy0xCgmyp4iwdkamwsmKzQkBc5fvsAnA4DcpK4lY8,34934
+gradient_free_optimizers-1.2.6.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+gradient_free_optimizers-1.2.6.dist-info/top_level.txt,sha256=vvSJQaQ3s7mZx_5ZoYAS3sBswwsRh2xhcx1vmlwhjrQ,31
+gradient_free_optimizers-1.2.6.dist-info/RECORD,,
```

