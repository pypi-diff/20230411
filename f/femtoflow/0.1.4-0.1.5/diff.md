# Comparing `tmp/femtoflow-0.1.4-py3-none-any.whl.zip` & `tmp/femtoflow-0.1.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,26 +1,26 @@
-Zip file size: 21426 bytes, number of entries: 24
--rwxr-xr-x  2.0 unx     1349 b- defN 23-Mar-30 01:41 femtoflow/ENV_REQUIREMENTS.sh
--rw-r--r--  2.0 unx      101 b- defN 23-Mar-30 01:41 femtoflow/PY_REQUIREMENTS
--rw-r--r--  2.0 unx        5 b- defN 23-Mar-30 01:41 femtoflow/VERSION
--rw-r--r--  2.0 unx      162 b- defN 23-Mar-30 01:41 femtoflow/__init__.py
--rw-r--r--  2.0 unx      403 b- defN 23-Mar-30 01:41 femtoflow/_open_docs.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-30 01:41 femtoflow/quantization/__init__.py
--rw-r--r--  2.0 unx    10052 b- defN 23-Mar-30 01:41 femtoflow/quantization/quantize_tflite.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-30 01:41 femtoflow/sparsity/__init__.py
--rw-r--r--  2.0 unx    14202 b- defN 23-Mar-30 01:41 femtoflow/sparsity/prune.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-30 01:41 femtoflow/sparsity/pruning_schedulers/__init__.py
--rw-r--r--  2.0 unx     2326 b- defN 23-Mar-30 01:41 femtoflow/sparsity/pruning_schedulers/linear_scheduler.py
--rw-r--r--  2.0 unx     2451 b- defN 23-Mar-30 01:41 femtoflow/sparsity/pruning_schedulers/sine_scheduler.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-30 01:41 femtoflow/utils/__init__.py
--rw-r--r--  2.0 unx      946 b- defN 23-Mar-30 01:41 femtoflow/utils/metrics.py
--rw-r--r--  2.0 unx     4756 b- defN 23-Mar-30 01:41 femtoflow/utils/plot.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-30 01:41 test/__init__.py
--rw-r--r--  2.0 unx      312 b- defN 23-Mar-30 01:41 test/test_package.py
--rw-r--r--  2.0 unx    20054 b- defN 23-Mar-30 01:41 test/test_prune.py
--rw-r--r--  2.0 unx     2673 b- defN 23-Mar-30 01:41 test/test_quantize.py
--rw-r--r--  2.0 unx       36 b- defN 23-Mar-30 01:45 femtoflow-0.1.4.dist-info/LICENSE
--rw-r--r--  2.0 unx     1195 b- defN 23-Mar-30 01:45 femtoflow-0.1.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-30 01:45 femtoflow-0.1.4.dist-info/WHEEL
--rw-r--r--  2.0 unx       15 b- defN 23-Mar-30 01:45 femtoflow-0.1.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2016 b- defN 23-Mar-30 01:45 femtoflow-0.1.4.dist-info/RECORD
-24 files, 63146 bytes uncompressed, 18124 bytes compressed:  71.3%
+Zip file size: 21747 bytes, number of entries: 24
+-rwxr-xr-x  2.0 unx     1349 b- defN 23-Apr-11 16:24 femtoflow/ENV_REQUIREMENTS.sh
+-rw-r--r--  2.0 unx      101 b- defN 23-Apr-11 16:24 femtoflow/PY_REQUIREMENTS
+-rw-r--r--  2.0 unx        5 b- defN 23-Apr-11 16:24 femtoflow/VERSION
+-rw-r--r--  2.0 unx      162 b- defN 23-Apr-11 16:24 femtoflow/__init__.py
+-rw-r--r--  2.0 unx      403 b- defN 23-Apr-11 16:24 femtoflow/_open_docs.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 16:24 femtoflow/quantization/__init__.py
+-rw-r--r--  2.0 unx    15261 b- defN 23-Apr-11 16:24 femtoflow/quantization/quantize_tflite.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 16:24 femtoflow/sparsity/__init__.py
+-rw-r--r--  2.0 unx    11990 b- defN 23-Apr-11 16:24 femtoflow/sparsity/prune.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 16:24 femtoflow/sparsity/pruning_schedulers/__init__.py
+-rw-r--r--  2.0 unx     2326 b- defN 23-Apr-11 16:24 femtoflow/sparsity/pruning_schedulers/linear_scheduler.py
+-rw-r--r--  2.0 unx     2451 b- defN 23-Apr-11 16:24 femtoflow/sparsity/pruning_schedulers/sine_scheduler.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 16:24 femtoflow/utils/__init__.py
+-rw-r--r--  2.0 unx      946 b- defN 23-Apr-11 16:24 femtoflow/utils/metrics.py
+-rw-r--r--  2.0 unx     4756 b- defN 23-Apr-11 16:24 femtoflow/utils/plot.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 16:24 test/__init__.py
+-rw-r--r--  2.0 unx      312 b- defN 23-Apr-11 16:24 test/test_package.py
+-rw-r--r--  2.0 unx    20054 b- defN 23-Apr-11 16:24 test/test_prune.py
+-rw-r--r--  2.0 unx     2847 b- defN 23-Apr-11 16:24 test/test_quantize.py
+-rw-r--r--  2.0 unx       36 b- defN 23-Apr-11 16:28 femtoflow-0.1.5.dist-info/LICENSE
+-rw-r--r--  2.0 unx     2106 b- defN 23-Apr-11 16:28 femtoflow-0.1.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-11 16:28 femtoflow-0.1.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx       15 b- defN 23-Apr-11 16:28 femtoflow-0.1.5.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2016 b- defN 23-Apr-11 16:28 femtoflow-0.1.5.dist-info/RECORD
+24 files, 67228 bytes uncompressed, 18445 bytes compressed:  72.6%
```

## zipnote {}

```diff
@@ -51,23 +51,23 @@
 
 Filename: test/test_prune.py
 Comment: 
 
 Filename: test/test_quantize.py
 Comment: 
 
-Filename: femtoflow-0.1.4.dist-info/LICENSE
+Filename: femtoflow-0.1.5.dist-info/LICENSE
 Comment: 
 
-Filename: femtoflow-0.1.4.dist-info/METADATA
+Filename: femtoflow-0.1.5.dist-info/METADATA
 Comment: 
 
-Filename: femtoflow-0.1.4.dist-info/WHEEL
+Filename: femtoflow-0.1.5.dist-info/WHEEL
 Comment: 
 
-Filename: femtoflow-0.1.4.dist-info/top_level.txt
+Filename: femtoflow-0.1.5.dist-info/top_level.txt
 Comment: 
 
-Filename: femtoflow-0.1.4.dist-info/RECORD
+Filename: femtoflow-0.1.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## femtoflow/VERSION

```diff
@@ -1 +1 @@
-0.1.4
+0.1.5
```

## femtoflow/quantization/quantize_tflite.py

```diff
@@ -1,75 +1,79 @@
 """
 TFLite Utilities Related Files, for converting a Tensorflow to TFLite Quantized Model 
 """
 import tensorflow as tf
 import numpy as np
 import tempfile
 from typing import Dict, Union, Callable
+from abc import ABC, abstractmethod
 
 # Setup Logging
 import logging
 logging.basicConfig(level=logging.INFO)
 
 
 class TFLiteUtils:
     """
-    Quantization Related Utilities for TFLite Conversion.
+    Provides utility methods for quantization-related operations during TFLite model 
+    conversion and inference.
     """
 
     @staticmethod
     def quantize_input_tensor(x: Union[np.ndarray, tf.Tensor], runner, name: str) -> np.ndarray:
         """
-        Quantize an input tensor from float to int, using metadata from the TFLite
+        Quantizes an input tensor from float to int using metadata from the TFLite
         SignatureRunner.
 
         Args:
-            x (np.ndarray): Unquantized Tensor
-            runner: TFLite Signature Runner
-            name (str): The 'name' of the tensor corresponding to `x`
-        
+            x (Union[np.ndarray, tf.Tensor]): Unquantized tensor.
+            runner: TFLite SignatureRunner instance.
+            name (str): The name of the tensor corresponding to `x`.
+
         Returns:
-            (np.ndarray): The Quantized Tensor
+            np.ndarray: The quantized tensor.
         """
         details = runner.get_input_details()[name]
         scale, zero_point = details['quantization']
         return np.round(np.array(x, dtype=float) / scale) + zero_point
 
     @staticmethod
     def dequantize_output_tensor(x: Union[np.ndarray, tf.Tensor], runner, name: str) -> np.ndarray:
-        """Dequantize an output value back to a float, using metadata from the TFLite 
+        """
+        Dequantizes an output tensor back to a float using metadata from the TFLite
         SignatureRunner.
 
         Args:
-            x (tf.Tensor): Quantized Tensor`
-            runner: TFLite Signature Runner
-            name (str): The 'name' of the tensor corresponding to `x`
+            x (Union[np.ndarray, tf.Tensor]): Quantized tensor.
+            runner: TFLite SignatureRunner instance.
+            name (str): The name of the tensor corresponding to `x`.
 
         Returns:
-            (np.ndarray): The De-Quantized Tensor
+            np.ndarray: The dequantized tensor.
         """
         details = runner.get_output_details()[name]
         scale, zero_point = details['quantization']
         return scale * (np.array(x, dtype=float) - zero_point)
 
     @staticmethod
     def quantized_predict(interpreter: tf.lite.Interpreter, signature_name: str,
                            inputs:Dict[str, np.ndarray], predict_fn: Callable) -> Dict[str, np.ndarray]:
         """
-        Prediction function for quantized models. Can be used for TFLite inference, 
-        depending on 'predict_fn'. Passed to __process_sequences__ during inference.
+        Performs prediction for quantized models. Can be used for TFLite inference,
+        depending on the provided `predict_fn`. This method is used during inference
+        to process sequences.
 
         Args:
-            interpreter (tf.lite.Interpreter): TFLite Interpreter
-            signature_name (str): Signature Name
-            inputs (Dict[str, np.ndarray]): {'input_name': input_arr ...}
-            predict_fn (Callable): Lambda Function that does a forward pass on 
-                                     the Quantized Inputs
+            interpreter (tf.lite.Interpreter): TFLite Interpreter instance.
+            signature_name (str): Name of the signature to use for inference.
+            inputs (Dict[str, np.ndarray]): Dictionary of input tensor names and their corresponding arrays.
+            predict_fn (Callable): Lambda function that performs a forward pass on quantized inputs.
+
         Returns:
-            (Dict[str, np.ndarray]): The Predicted De-Quantized Output
+            Dict[str, np.ndarray]: Dictionary of the predicted dequantized output tensor names and their corresponding arrays.
         """
         # Quantize inputs
         np_dtype = interpreter.get_input_details()[0]['dtype']
         runner = interpreter.get_signature_runner(signature_name)
         quantized_inputs = {name: TFLiteUtils.quantize_input_tensor(value, runner, name).astype(np_dtype) for name, value in inputs.items()} 
         
         # Predict
@@ -78,119 +82,143 @@
                                                                     runner,
                                                                     key).squeeze()
                                                 for key, value in quantized_outputs.items()}
         # Dequantize outputs
         return dequantized_output
 
 
-class TFLiteInt8Model:
+class TFLiteModelBase(ABC):
     """
-            This class provides functionality to convert a Tensorflow model 
-            into its TFLite Equivalent, with INT-8 Weights and Activations Quantized.
+    Base class for generating quantized TFLite models from TensorFlow models and performing inference
+    using the quantized TFLite models. This is an abstract class that must be subclassed by concrete
+    classes for specific quantization types.
     """
 
     def __init__(self, model: Union[tf.keras.Model, 
                                     tf.keras.Sequential], 
                        representative_dataset: Callable, 
-                       tflite_save_path: str) -> None:
+                       tflite_save_path: str,
+                       signature_name: str='serving_default') -> None:
         """
-        Constructor for the `TFLiteInt8Model` class
+        Initializes the TFLiteModelBase instance.
 
         Args:
-            model (Union[tf.keras.Model, tf.keras.Sequential]): The Tensorflow Model
-            representative_dataset (Callable): Lambda Function used as the Calibration dataset
-                                                    for TFLite Conversion.
-            tflite_save_path (str): Path to save the TFlite model file.
+            model (Union[tf.keras.Model, tf.keras.Sequential]): The TensorFlow model to be converted.
+            representative_dataset (Callable): A callable that produces representative data samples
+                used for calibration during quantization.
+            tflite_save_path (str): Path where the converted TFLite model will be saved.
+            signature_name (str, optional): The name of the signature to be used for inference.
+                Defaults to 'serving_default'.
+
+        Note:
+            1) The inputs to the model are expected to be in dictionary format, i.e.
+                {'foo_ip': tf.Tensor(..), 'foo_ip_two': tf.Tensor(..)}
+               The outputs will also be a dictionary, of the format
+                {'foo_op_name': output_tensor_one, 'foo_op_name_two': output_tensor_two}
+
+            2) For models with multiple inputs or outputs, it is recommended to supply a model
+                of type `tf.keras.Model(inputs=inputs, outputs=outputs)` with the input/output
+                names and shapes explicitly defined. The model should also have `model.input_spec`
+                defined. There could be TFLite conversion errors if the inputs/outputs aren't
+                explicitly defined.
+        """
+        self.flatbuffer = self.generate_tflite_flatbuffer(model=model,
+                                                          representative_dataset=representative_dataset)
 
-        Example:
-    
-        .. code-block:: python
-        
-            from femtoflow.quantization.quantize_tflite import TFLiteInt8Model
-            import tensorflow as tf
+        # Save the TFLite Flatbuffer to a TFLite file
+        self.save_tflite_model(tflite_flatbuffer=self.flatbuffer, tflite_save_path=tflite_save_path)
 
-            # Model Definition
-            model_tf = tf.keras.Model(...) # The Tensorflow Model
 
-            # Input Name of Model (Assume Single-Input model).
-            model_input_name = 'foo_input_name'
+        self.interpreter = tf.lite.Interpreter(model_content=self.flatbuffer)
 
-            # Save Path of TFLite Object
-            tflite_save_path = 'save_path.tflite'
-            
-            # Calibration Data Setup
-            train_feats = ... # Input Features for Calibration
-            num_samples = 50
-            
-            def representative_data_gen():
-                for input_value in tf.data.Dataset.from_tensor_slices(train_feats).batch(batch_size).take(num_samples):
-                    # Model has only one input so each data point has one element.
-                    yield {model_input_name: tf.cast(input_value, dtype=tf.float32)}
-
-            representative_dataset = representative_data_gen
-
-            # Convert Tensorflow model to TFLite
-            model_tflite = TFLiteInt8(model= model_tf, representative_dataset=representative_dataset,             
-                                    tflite_save_path=tflite_save_path) 
-
-            # Perform TFLite Inference on `input_feats`
-            input_feats = ... # Input Features
-            model_tflite_out = model_tflite({model_input_name: input_feats})
-            print("Quantized inference output")
-            
-            # Check if TFLite file was actually generated
-            assert os.path.exists(tflite_save_path), f"ERR: TFLite file {tflite_save_path} not saved successfully."
-
-        
-
-        Note: 
-            For Models with Multiple Inputs/ Multiple Outputs, it is 
-            recommended to supply a model of type `tf.keras.Model(inputs=inputs, outputs=outputs)`
-            with the Input/Output Names/Shapes of the model explicitly defined.
-            The Model supplied should also have `model.input_spec` defined.
-            There could be TFLite Conversion Errors if the Inputs/Outputs 
-            aren't explicitly defined.
-        """
-        self.tflite_model = self.tflite_convert_int8(model=model,
-                                                     representative_dataset=representative_dataset,
-                                                     tflite_save_path=tflite_save_path)
-
-        self.interpreter = tf.lite.Interpreter(model_content=self.tflite_model)
-
-        self.default_sig_name = 'serving_default' 
-        self.runner = self.interpreter.get_signature_runner(self.default_sig_name)
+        self.signature_name = signature_name
+        self.runner = self.interpreter.get_signature_runner(self.signature_name)
 
         self.input_details = self.runner.get_input_details()
         self.output_details = self.runner.get_output_details()
 
         self.tflite_pred_fn = lambda inputs: TFLiteUtils.quantized_predict(interpreter=self.interpreter,
-                                                                            signature_name=self.default_sig_name,
+                                                                            signature_name=self.signature_name,
                                                                             inputs=inputs,
                                                                             predict_fn=lambda quantized_inputs: self.runner(**quantized_inputs))
 
-    def tflite_convert_int8(self, model: Union[tf.keras.Model, 
+    def save_tflite_model(self, tflite_flatbuffer: bytes, tflite_save_path: str):
+        """
+        Saves the TFLite model to the specified path.
+
+        Args:
+            tflite_flatbuffer (bytes): The TFLite binary flatbuffer to be saved.
+            tflite_save_path (str): Path where the TFLite model will be saved.
+        """
+        with open(tflite_save_path, 'wb') as f:
+            f.write(tflite_flatbuffer)
+
+    @abstractmethod
+    def generate_tflite_flatbuffer(self, model: Union[tf.keras.Model, 
                                                tf.keras.Sequential], 
-                                  representative_dataset: Callable, 
-                                  tflite_save_path: str) -> bytes:
+                                  representative_dataset: Callable) -> bytes:
         """
-        Converts TF model into its TFLite Equivalent, with INT-8 Weights and Activations Quantized.
-        Saves the TFLite File at the `tflite_save_path` specified, and returns the converted TFLite model buffer.
+        Generates the TFLite binary flatbuffer for the given TensorFlow model.
+        This method must be implemented by subclasses of TFLiteModelBase.
 
         Args:
-            model (Union[tf.keras.Model, tf.keras.Sequential]): The Tensorflow Model
-            representative_dataset (Callable): Lambda Function used as the Calibration dataset
-                                                    for TFLite Conversion.
-            tflite_save_path (str): Path to save the TFlite model file.
-        
+            model (Union[tf.keras.Model, tf.keras.Sequential]): The TensorFlow model to be converted.
+            representative_dataset (Callable): A callable that produces representative data samples
+                used for calibration during quantization.
+
         Returns:
-            (bytes): TFLite Binary Flatbuffer
+            bytes: The TFLite binary flatbuffer.
+
+        Raises:
+            Exception: If the method is not implemented by the subclass.
+        """
+        raise Exception(f"ERR: Method `generate_tflite_flatbuffer` needs to be implemented by Subclass")
+
+    def __call__(self, inputs: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
+        """
+        Performs a quantized forward pass using the TFLite model.
+
+        Args:
+            inputs (Dict[str, np.ndarray]): A dictionary of unquantized inputs.
+
+        Returns:
+            Dict[str, np.ndarray]: A dictionary of unquantized outputs.
+        """
+        output = self.tflite_pred_fn(inputs)
+        return output
+
+class TFLiteModelInt8(TFLiteModelBase):
+    """
+    Int 8 Activations, Int 8 Weights
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def generate_tflite_flatbuffer(self, model: Union[tf.keras.Model, 
+                                                tf.keras.Sequential], 
+                                    representative_dataset: Callable) -> bytes:
+        """
+        Converts a TensorFlow model into its TFLite equivalent with INT-8 quantization for both
+        weights and activations. Returns the converted TFLite model flatbuffer.
+
+        Args:
+            model (Union[tf.keras.Model, tf.keras.Sequential]): The TensorFlow model to be converted.
+            representative_dataset (Callable): A callable that produces representative data samples
+                used for calibration during quantization.
+
+        Returns:
+            bytes: The TFLite binary flatbuffer.
+
+        Raises:
+            AssertionError: If the model input specifications are not defined.
         """
         assert model.input_spec is not None, "model.input_spec() was None. Expected to be Defined. \
-                                              Please define explicit Input/Output Signatures to your model using \
-                                              tf.keras.Model(inputs=inputs, outputs=outputs)"
+                                                Please define explicit Input/Output Signatures to your model using \
+                                                tf.keras.Model(inputs=inputs, outputs=outputs)"
         ip_shapes = [tf.TensorSpec(shape=x.shape, name=x.name) for x in model.input_spec]
         with tempfile.TemporaryDirectory() as dirname: 
 
             run_model = tf.function(lambda x: model(x))
 
             concrete_func = run_model.get_concrete_function(ip_shapes)
             model.save(dirname, save_format="tf", signatures=concrete_func)
@@ -201,27 +229,107 @@
             converter.inference_output_type = tf_dtype
             converter.optimizations = [tf.lite.Optimize.DEFAULT]
             converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
 
             converter.representative_dataset = representative_dataset
 
             # Convert the inference model to TFLite
-            logging.info("Converting TF model to TFLite...")
+            logging.info("Converting TF model to TFLite 8x8...")
             tflite_model = converter.convert()
+        
+        return tflite_model
 
-            with open(tflite_save_path, 'wb') as f:
-                f.write(tflite_model)
+
+class TFLiteModelInt16(TFLiteModelBase):
+    """
+    Int 16 Activations, Int 8 Weights
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def generate_tflite_flatbuffer(self, model: Union[tf.keras.Model, 
+                                                tf.keras.Sequential], 
+                                    representative_dataset: Callable) -> bytes:
+        """
+        Converts a TensorFlow model into its TFLite equivalent with INT-16 quantization for activations
+        and INT-8 quantization for weights. Returns the converted TFLite model flatbuffer.
+
+        Args:
+            model (Union[tf.keras.Model, tf.keras.Sequential]): The TensorFlow model to be converted.
+            representative_dataset (Callable): A callable that produces representative data samples
+                used for calibration during quantization.
+
+        Returns:
+            bytes: The TFLite binary flatbuffer.
+
+        Raises:
+            AssertionError: If the model input specifications are not defined.
+        """
+        assert model.input_spec is not None, "model.input_spec() was None. Expected to be Defined. \
+                                              Please define explicit Input/Output Signatures to your model using \
+                                              tf.keras.Model(inputs=inputs, outputs=outputs)"
+        ip_shapes = [tf.TensorSpec(shape=x.shape, name=x.name) for x in model.input_spec]
+        with tempfile.TemporaryDirectory() as dirname: 
+
+            run_model = tf.function(lambda x: model(x))
+
+            concrete_func = run_model.get_concrete_function(ip_shapes)
+            model.save(dirname, save_format="tf", signatures=concrete_func)
+
+            tf_dtype = tf.int16
+            converter = tf.lite.TFLiteConverter.from_saved_model(dirname)
+            converter.inference_input_type = tf_dtype 
+            converter.inference_output_type = tf_dtype
+            converter.optimizations = [tf.lite.Optimize.DEFAULT]
+            converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
+
+            converter.representative_dataset = representative_dataset
+
+            # Convert the inference model to TFLite
+            logging.info("Converting TF model to TFLite 8x16...")
+            tflite_model = converter.convert()
         
         return tflite_model
 
+
+class TFLiteModelWrapper:
+
+    def __init__(self, quantize_mode: str, 
+                       model: Union[tf.keras.Model, 
+                                    tf.keras.Sequential], 
+                       representative_dataset: Callable, 
+                       tflite_save_path: str,
+                       signature_name: str='serving_default'):
+        """
+        Initializes the TFLiteModelWrapper instance.
+
+        Args:
+            quantize_mode (str): The quantization mode to use. Supported values are '8x16' and '8x8'.
+                8x16 quantizes the model to INT-8 Weights and INT-16 Activations
+                8x8 quantizes the model to INT-8 Weights and INT-8 Activations
+            model (Union[tf.keras.Model, tf.keras.Sequential]): The TensorFlow model to be converted.
+            representative_dataset (Callable): A callable that produces representative data samples
+                used for calibration during quantization.
+            tflite_save_path (str): Path where the converted TFLite model will be saved.
+            signature_name (str, optional): The name of the signature to be used for inference.
+                Defaults to 'serving_default'.
+
+        Raises:
+            AssertionError: If the specified quantization mode is not supported.
+        """
+        assert quantize_mode in ('8x16', '8x8'), f"ERR: Invalid Quant Mode. Supported: ('8x16', '8x8')"
+        self.model_cls_map = {'8x16': TFLiteModelInt16, '8x8': TFLiteModelInt8}
+        self.instance = self.model_cls_map[quantize_mode](model=model, representative_dataset=representative_dataset,
+                                                          tflite_save_path=tflite_save_path, signature_name=signature_name)
+
     def __call__(self, inputs: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
         """
-        Do Quantized Forward Pass for TFLite model
+        Performs a quantized forward pass using the TFLite model.
 
         Args:
-            inputs (Dict[str, np.ndarray]): Unquantized Dictionary Inputs
+            inputs (Dict[str, np.ndarray]): A dictionary of unquantized inputs.
 
-        Returns: 
-            output (Dict[str, np.ndarray]): Unquantized Dictionary Outputs
+        Returns:
+            Dict[str, np.ndarray]: A dictionary of unquantized outputs.
         """
-        output = self.tflite_pred_fn(inputs)
-        return output
+        return self.instance.__call__(inputs)
```

## femtoflow/sparsity/prune.py

```diff
@@ -1,109 +1,58 @@
 """
-Pruning Utilities, for Inducing Sparsity into Tensorflow Models 
+Pruning Utilities for Inducing Sparsity into Tensorflow Models.
 """
+
 import tensorflow as tf
 import tensorflow_model_optimization as tfmot
 from typing import Union, List, Callable, Any, Tuple
 from femtoflow.sparsity.pruning_schedulers.linear_scheduler import LinearPruningSchedule
 from femtoflow.sparsity.pruning_schedulers.sine_scheduler import QuadrantSinePruningSchedule
 
 # Setup Logging
 import logging
 logging.basicConfig(level=logging.INFO)
 
 
 class PruneHelper:
     """
-    Pruning related utilities for Tensorflow Models.
+    Pruning-related utilities for TensorFlow models.
     """
 
     def __init__(self, 
                  pencil_size: int=4,
                  pencil_pooling_type: str='AVG',
                  prune_scheduler: str='polynomial_decay',
                  min_parameter_thresh: int=200
                  ) -> None:
-        """This class adds Prune Wrappers (`tfmot.sparsity.keras.prune_low_magnitude`)
-        to the layers specified in `layers_to_prune` to the tensorflow model `model`.
-
-        We can perform Training, while inducing sparsity to `model-with-prune-wrappers()`
-        by calling instances of `tfmot.sparsity.keras.UpdatePruningStep()` at specific points 
-        in the training loop, to successfully induce sparsity.
-        See: https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md
-
-        We can also pass `tfmot.sparsity.keras.UpdatePruningStep()` to `model.fit()`
-        as an additional callback() instead to induce sparsity (See Example Below).
-
-        We can Prune model's with "Pencil Sparsity" to induce Sparsity Patterns 
-        that can take advantage of Femtosense's Sparse Accelerator.        
-        As an example, say we have a Dense Matrix of shape `(INPUT_CHANNEL, OUTPUT_CHANNEL)`
-        we force sparse blocks to be in chucks of shape `(Pencil_Size, 1)`
-
-        Note: 
-            `Conv1D` and `Conv2D` layers only support `pencil_size=1`.
-            However, pruning with `pencil_size=1` will not give performance boost
-            when using Femtosense's SPU. 
-            Hence, we say `pencil_size=1` is not supported, and raise an assertion above
-
-            Other Layers (Dense, LSTM, RNN, etc.) support pruning with `pencil_size > 1`
-
-        Example:
+        """
+        Initializes the PruneHelper class.
         
-        .. code-block:: python
-
-            # Define the PruneHelper Class
-            prune_helper = PruneHelper(pencil_size=4,
-                                        pencil_pooling_type='AVG',
-                                        prune_scheduler='poly_decay',
-                                        min_parameter_thresh=200)
-
-            model = ... # The Tensorflow Model
-            train_dataset = ...  # Optional - used to calculate end_step)
-            num_epochs = ... # Optional - used to calculate end_step
-
-            # Get the Model with `tfmot.sparsity.keras.prune_low_magnitude`
-            # Wrappers Applied to Layers we want to prune.
-            model_to_prune = prune_helper(model=model,
-                                          layers_to_prune=[tf.keras.layers.Dense, 
-                                                           tf.keras.layers.RNN],
-                                          initial_sparsity=0.2,
-                                          final_sparsity=0.6,
-                                          begin_step=0,
-                                          end_step=len(train_dataset) * num_epochs,
-                                          prune_frequency=100,
-                                          force_skip_layers=['dense_1/kernel:0', 'dense/kernel:0'])
-
-            model_to_prune.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)
-            model_to_prune.fit(train_dataset,
-                               epochs=epochs, 
-                               callbacks=[tfmot.sparsity.keras.UpdatePruningStep()])
-
-            # Perform Sparse Evaluation
-            model_to_prune.evaluate(...)
-            
-            # Remove Sparse Wrappers, and apply Prune Mask to obtain the Sparse Weights.
-            model_pruned_stripped = tfmot.sparsity.keras.strip_pruning(model_to_prune)
+        This class adds Prune Wrappers (`tfmot.sparsity.keras.prune_low_magnitude`)
+        to the layers specified in `layers_to_prune` to the TensorFlow model `model`.
+        The pruning can be performed during training to induce sparsity into the model.
+
+        We can Prune model’s with “Pencil Sparsity” to induce Sparsity Patterns that can 
+        take advantage of Femtosense’s Sparse Accelerator. As an example, say we have a 
+        Dense Matrix of shape (OUTPUT_CHANNEL, INPUT_CHANNEL) we force sparse blocks to be in chucks of shape (Pencil_Size, 1)
+
+        Args:
+            pencil_size (int, optional): The pencil size for inducing sparsity patterns.
+                The weight matrix is pruned in a pencil-sparse manner (along input dimension).
+                Default is 4.
+            pencil_pooling_type (str, optional): Computes pencil-wise (or block-wise) L1-Norm scores.
+                All elements in the pencil (or block) with lowest L1-Norm scores are set to 0.
+                Options are ['AVG', 'MAX']. Default is 'AVG'.
+            prune_scheduler (str, optional): Pruning scheduler to set the prune level after every step.
+                Options are ['poly_decay', 'constant', 'linear', 'sine']. Default is 'polynomial_decay'.
+            min_parameter_thresh (int, optional): A weight matrix tensor will be pruned only if the
+                weight has more than `min_parameter_thresh` parameters. Default is 200.
 
-            # Visualize Sparse Weights
-            sparse_weights = model_pruned_stripped.trainable_weights
-        
-        Args:
-            pencil_size (int): The Pencil Size 
-                Default: 1
-                The weight matrix is pruned in a pencil-sparse manner (along input dimension)
-            pencil_pooling_type (str): Computes Pencil-Wise (or block-wise) L1-Norm Scores.
-                All elements in the Pencil (or Block) with lowest L1-Norm Scores are set to 0 
-                Options: ['AVG', 'MAX']
-                Default: 'AVG'
-            prune_scheduler (str): Pruning Scheduler (to set the prune level after every step)
-                Options: ['poly_decay', 'constant', 'linear', 'sine']
-            min_parameter_thresh (int): A Weight Matix Tensor will be pruned only if the 
-                                        weight has more than `min_parameter_thresh` parameters. 
-                                        Default: 200
+        Raises:
+            AssertionError: If the pencil size is not 4 or 8.
         """
         assert pencil_size in (4, 8), f"Femtosense's SPU currently supports only Sparse Pencil Size of 4 or 8."
         self.pencil_size = pencil_size
         self.block_size  = (1, self.pencil_size) # We use Only Row-Major Blocks of Dim: (1, Pencil Size)
         self.pencil_pooling_type = pencil_pooling_type
 
         self.min_parameter_thresh = min_parameter_thresh
@@ -112,56 +61,61 @@
         self.prune_scheduler_map = {'poly_decay': tfmot.sparsity.keras.PolynomialDecay,
                                     'constant': tfmot.sparsity.keras.ConstantSparsity,
                                     'linear': LinearPruningSchedule,
                                     'sine': QuadrantSinePruningSchedule}
         
         
     def _is_block_pruning_supported(self, layer: tf.keras.layers.Layer) -> bool:
-        """Checks if Block Pruning (Pencil Sparsity) is supported for a given layer.
-        Currently, Conv1D and Conv2D do not support Block Pruning (Pencil Sparsity)
+        """
+        Checks if block pruning (pencil sparsity) is supported for a given layer.
+        Currently, Conv1D and Conv2D do not support block pruning (pencil sparsity).
 
         Args:
-            layer (tf.keras.layers.Layer)
-        
+            layer (tf.keras.layers.Layer): The layer to check for block pruning support.
+
         Returns:
-            (bool)
+            bool: True if block pruning is supported, False otherwise.
         """
         if layer in [tf.keras.layers.Conv1D, tf.keras.layers.Conv2D]:
             return False
         return True
 
     def _is_pruning_supported(self, layers_to_prune: List[tf.keras.layers.Layer]) -> bool:
         """
-        Checks if Pruning with specified `self.pencil_size` is supported 
+        Checks if pruning with the specified `self.pencil_size` is supported
         for the layers passed to `self.layers_to_prune`.
 
         Args:
-            layers_to_prune (List[tf.keras.layers.Layer])
-            
+            layers_to_prune (List[tf.keras.layers.Layer]): List of layers to check for pruning support.
+
         Returns:
-            (bool)
+            bool: True if pruning is supported, False otherwise.
         """
         bool_not_supported = any(not self._is_block_pruning_supported(layer=layer_type) for layer_type in layers_to_prune)
         return not bool_not_supported
 
     def _prune_selected_layers_fn(self, pruning_params: dict, 
                                         layers_to_prune: List[tf.keras.layers.Layer],
                                         force_skip_layers: List[str]=[]) -> Callable:
-        """Returns a helper function  `_prune_layer` to make prune layers included 
-        in `layers_to_prune` layers only.
+        """
+        Returns a helper function `_prune_layer` to prune only the layers included 
+        in `layers_to_prune`.
         
         Args:
-            pruning_params (dict)
-            layers_to_prune (List[tf.keras.layers.Layer])
-            force_skip_layers (List[str]) - Skips pruning layers included in `force_skip_layers`.
-                                                User should include the `layer.trainable_weights[index].name` in this list
-                                                if they explicitly want to skip pruning a given layer.
-         Returns:
-            _prune_layer (Callable)
+            pruning_params (dict): Pruning parameters for the pruning schedule.
+            layers_to_prune (List[tf.keras.layers.Layer]): List of layer types to prune.
+            force_skip_layers (List[str], optional): List of layer names to forcibly skip pruning.
+                User should include the `layer.trainable_weights[index].name` in this list
+                if they explicitly want to skip pruning a given layer. Default is an empty list.
+
+        Returns:
+            Callable: A function that, when applied to a layer, wraps it with a pruning
+                wrapper if it is included in `layers_to_prune`.
         """
+
         def _prune_layer(layer: tf.keras.layers.Layer) -> tfmot.sparsity.keras.prune_low_magnitude:
             """
             Add `tfmot.sparsity.keras.prune_low_magnitude` wrapper to layers
             that are intended to be pruned.
             """
             prune_layer_bool = any(isinstance(layer, layer_type) for layer_type in layers_to_prune)
             if prune_layer_bool is False:
@@ -191,19 +145,19 @@
 
         Args:
             begin_step (int): Step at which to begin pruning.
             end_step (int): Step at which to end pruning.  
             prune_frequency (int): Only apply pruning every `prune_frequency` steps.
             power (int): Exponent to be used in the sparsity function.
                              Applicable only for `prune_scheduler == 'poly_decay'` 
-            initial_sparsity (int): The Initial Sparsity Value to apply at `begin_step`
-            final_sparsity (int): The Expected Final Sparsity Value at `end_step`
+            initial_sparsity (float): The Initial Sparsity Value to apply at `begin_step`
+            final_sparsity (float): The Expected Final Sparsity Value at `end_step`
 
         Returns:
-            (Dict): The Parameter Dictionary.
+            dict: The parameter dictionary for the prune scheduler.
         """
         scheduler_params = {'begin_step': begin_step, 'end_step': end_step, 'frequency': prune_frequency}
         if self.prune_scheduler == 'poly_decay':
             scheduler_params.update({'power': power, 'initial_sparsity': initial_sparsity, 'final_sparsity': final_sparsity})
         elif  self.prune_scheduler == 'constant':
             scheduler_params.update({'target_sparsity': final_sparsity})
         elif self.prune_scheduler == 'linear' or self.prune_scheduler == 'sine':
@@ -218,41 +172,43 @@
                                         final_sparsity: float,
                                         begin_step: int,
                                         end_step: int,
                                         prune_frequency: int,
                                         power: int=3,
                                         force_skip_layers: List[str]=[],
                                         ) -> Union[tf.keras.Model, tf.keras.Sequential]:
-        """Adds `tfmot.sparsity.keras.prune_low_magnitude()` Pruning Wrappers to 
-        Layers of the Tensorflow Model specified in `self.layers_to_prune`.
+        """
+        Adds `tfmot.sparsity.keras.prune_low_magnitude()` pruning wrappers to
+        specified layers of the TensorFlow model to enable pruning during training.
 
         A layer wrapped with `tfmot.sparsity.keras.prune_low_magnitude()` can 
-        be pruned via the `tfmot` library by calling instances of the 
-        `tfmot.sparsity.keras.UpdatePruningStep()` callback appropriately in the 
-        training loop 
+        be pruned via the TensorFlow Model Optimization library by calling instances
+        of the `tfmot.sparsity.keras.UpdatePruningStep()` callback in the training loop.
 
         Args:
-            model (Union[tf.keras.Model, tf.keras.Sequential]) - The Tensorflow model.
-            layers_to_prune (List[tf.keras.layers.Layer]): List of Tensorflow Layers to Prune
+            model (Union[tf.keras.Model, tf.keras.Sequential]): The TensorFlow model to be pruned.
+            layers_to_prune (List[tf.keras.layers.Layer]): List of TensorFlow layers to be pruned.
                 Example: [tf.keras.layers.Dense, tf.keras.layers.Conv1D]
-            initial_sparsity (int): The Initial Sparsity Value to apply at `begin_step`
-            final_sparsity (int): The Expected Final Sparsity Value at `end_step`
+            initial_sparsity (float): The initial sparsity value to apply at `begin_step`.
+            final_sparsity (float): The expected final sparsity value at `end_step`.
             begin_step (int): Step at which to begin pruning.
-            end_step (int): Step at which to end pruning.  
+            end_step (int): Step at which to end pruning.
             prune_frequency (int): Only apply pruning every `prune_frequency` steps.
-            power (int): Exponent to be used in the sparsity function.
-                             Applicable only for `prune_scheduler == 'poly_decay'` 
-                             Default: 3
-            force_skip_layers (List[str]) - Don't Prune layers whose names are included 
-                                                in `force_skip_layers`
-                                                Default: []
+            power (int, optional): Exponent to be used in the sparsity function.
+                Applicable only for `prune_scheduler == 'poly_decay'`.
+                Default is 3.
+            force_skip_layers (List[str], optional): List of layer names to forcibly skip pruning.
+                Default is an empty list.
 
         Returns:
-            (Union[tf.keras.Model, tf.keras.Sequential]):  
-                model's prunable layers wrapped with `tfmot.sparsity.keras.prune_low_magnitude()`
+            Union[tf.keras.Model, tf.keras.Sequential]: Model with prunable layers wrapped
+                with `tfmot.sparsity.keras.prune_low_magnitude()`.
+
+        Raises:
+            AssertionError: If pruning is not supported for the given pencil size and layer types.
         """
         assert self._is_pruning_supported(layers_to_prune) is True, f"For Pencil Size \
                             {self.pencil_size}, Conv1D/ Conv2D layers are Unsupported.\
                                                 Supplied Layers: {layers_to_prune}"
 
         scheduler_params = self._get_scheduler_params(begin_step=begin_step, end_step=end_step, prune_frequency=prune_frequency, 
                                                       power=power, initial_sparsity=initial_sparsity, final_sparsity=final_sparsity)
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## test/test_quantize.py

```diff
@@ -1,19 +1,23 @@
 
 
 import tensorflow as tf
 import os
 import numpy as np
-from femtoflow.quantization.quantize_tflite import TFLiteInt8Model
+from parameterized import parameterized
+from femtoflow.quantization.quantize_tflite import TFLiteModelWrapper
 
-
-def test_TFLiteInt8Model():
+@parameterized.expand([
+    ('8x8'),
+    ('8x16'),
+])
+def test_TFLiteModelWrapper(quantize_mode):
     """
     Quantizes a Tensorflow Model with Dense/Conv2D Layers 
-    to INT-8 Activations and Weights using `TFLiteInt8Model`.
+    with quantize_mode=8x8 or quantize_mode=8x16.
     
     Further, confirm forward pass of the TF Model, TFLite Model
     gives roughly the same output on validation data.
     """
     batch_size = 256
     train_samples = 1024
     val_samples = 512
@@ -45,17 +49,18 @@
     output_name =  'output_0'
     def representative_data_gen(num_samples=5):
         for feats, labels in train_dataset.take(num_samples):
             # Model has only one input so each data point has one element.
             yield {input_name: tf.cast(feats, dtype=tf.float32)}
         
     tflite_save_path = 'tflite_unittest.tflite'
-    model_tflite = TFLiteInt8Model(model=model,
-                                    representative_dataset=representative_data_gen,
-                                    tflite_save_path=tflite_save_path)
+    model_tflite = TFLiteModelWrapper(quantize_mode=quantize_mode,
+                                      model=model,
+                                      representative_dataset=representative_data_gen,
+                                      tflite_save_path=tflite_save_path)
 
     assert os.path.exists(tflite_save_path), f"ERR: TFLite file not found at {tflite_save_path}"
 
     for feats, _ in val_dataset:
         input_dict = {input_name: feats}
         tf_fwd = model(input_dict)
         tflite_fwd = model_tflite(input_dict)[output_name]
```

## Comparing `femtoflow-0.1.4.dist-info/RECORD` & `femtoflow-0.1.5.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 femtoflow/ENV_REQUIREMENTS.sh,sha256=S1d5ucXZLYEcmqzW3hXXNa94KP-trbqP1qxcryfbeJE,1349
 femtoflow/PY_REQUIREMENTS,sha256=0GqXtGIxM2g9KL5SwBn6JNrK9o1751eRHWy4ZKfE2OY,101
-femtoflow/VERSION,sha256=RTMGWu18lgJhojG0A1zJvOyL_2HOazONQ-nFDRU6RUY,5
+femtoflow/VERSION,sha256=QXCYE8RmYkHVrHvVrvCJYMkGMqpOawbNrZOJ4WODczY,5
 femtoflow/__init__.py,sha256=h9UchNl62V-tKFPlf2uyeVqvUBFaSi9yhH15PDy-NaQ,162
 femtoflow/_open_docs.py,sha256=QVxUKe7I5PTujfMrdyPS6lWkjbg5qV8YbRw638RD4bM,403
 femtoflow/quantization/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-femtoflow/quantization/quantize_tflite.py,sha256=0vrd2ypzHjzZFYSmfznZreYG54MHeWQznHd07qjqRHk,10052
+femtoflow/quantization/quantize_tflite.py,sha256=j4LJHAp3Kap7J944RADEoRpFfH1XeTtCY4Q34swblvA,15261
 femtoflow/sparsity/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-femtoflow/sparsity/prune.py,sha256=mXM1n4ItL1abX_fKpvWOVfNZABKSdQuHseuHB1aE420,14202
+femtoflow/sparsity/prune.py,sha256=U8FwLwCoS2gWTesBgL45BQOqNLaNetMkn7uNTMcXxVI,11990
 femtoflow/sparsity/pruning_schedulers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 femtoflow/sparsity/pruning_schedulers/linear_scheduler.py,sha256=g0ouwqBu91e5ahJQMWk7MOgm_xoUj57C7Mxf6Y5fvHQ,2326
 femtoflow/sparsity/pruning_schedulers/sine_scheduler.py,sha256=Gn0CpBOIzwg5MGa4s-B__FKib_sUPIbMyDfkBYXgfqo,2451
 femtoflow/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 femtoflow/utils/metrics.py,sha256=R8MCPBiludHcSiZbKwXfmHMep5vFomJkwalTaWyP2Ro,946
 femtoflow/utils/plot.py,sha256=21fVRqDww2epFXoAMdl1VO_mzV8K3R-wPNYmBSvJC8I,4756
 test/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 test/test_package.py,sha256=UWGc6Ma_VEdJoVP3g18ahiJRqFL255uqsBn4fW92LOM,312
 test/test_prune.py,sha256=SEjXD6WFm1o3wkUMNJRa3-Y3ZMn2YQr7YADI4nNQbgA,20054
-test/test_quantize.py,sha256=If20ydk3SybqpnbDbM2xroc_JqiQmxYdvp_KDa9WJa4,2673
-femtoflow-0.1.4.dist-info/LICENSE,sha256=eN9ZI1xHjUmFvN3TEeop5kBGXRUBfbsl55KBNBYYFqI,36
-femtoflow-0.1.4.dist-info/METADATA,sha256=o6mgcdlvyi1vBuUR_t_m8scy2FITDAuCyuRGUG-Lk8k,1195
-femtoflow-0.1.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-femtoflow-0.1.4.dist-info/top_level.txt,sha256=c4pEf28SWHe3AfeltJNs1TfQ0amN3IU5PJm4zWkhS9Y,15
-femtoflow-0.1.4.dist-info/RECORD,,
+test/test_quantize.py,sha256=zj09e4RPKnU4EmlKAFW3J5XGcyy9RQFmGPvJiGa-b48,2847
+femtoflow-0.1.5.dist-info/LICENSE,sha256=eN9ZI1xHjUmFvN3TEeop5kBGXRUBfbsl55KBNBYYFqI,36
+femtoflow-0.1.5.dist-info/METADATA,sha256=2DmIagYnMpa-ZDCgb-p6vSJKkcrj4YJbG94BZGf7V44,2106
+femtoflow-0.1.5.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+femtoflow-0.1.5.dist-info/top_level.txt,sha256=c4pEf28SWHe3AfeltJNs1TfQ0amN3IU5PJm4zWkhS9Y,15
+femtoflow-0.1.5.dist-info/RECORD,,
```

